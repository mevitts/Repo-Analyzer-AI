{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de65a262",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install google-adk -q\n",
    "\n",
    "print(\"Installation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd412da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "\n",
    "import warnings\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bb6cc7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "import google.genai as genai\n",
    "\n",
    "load_dotenv()  \n",
    "\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"GOOGLE_API_KEY not found in environment variables.\")\n",
    "genai.Client(api_key=api_key)\n",
    "\n",
    "MODEL_GEMINI_2_0_FLASH = \"gemini-2.5-flash-lite-preview-06-17\"\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c685eb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jwt import decode\n",
    "from regex import E\n",
    "import requests\n",
    "import base64\n",
    "from arrow import get\n",
    "from bleach import clean\n",
    "from google.adk.agents import Agent, SequentialAgent, LoopAgent\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.runners import Runner\n",
    "from google.genai import types\n",
    "from google.adk.tools.tool_context import ToolContext\n",
    "\n",
    "def list_files(repo: str, owner: str) -> dict:\n",
    "    \"\"\"\n",
    "    Loads in the repository\n",
    "    \n",
    "    Args:\n",
    "        repo (str): The repository to load in.\n",
    "        owner (str): The owner of the repository.\n",
    "    Returns:\n",
    "        A dictionary with a 'status' key.\n",
    "        On success: {'status': 'success', 'files': ['file1.py', 'src/file2.js']}\n",
    "        On error: {'status': 'error', 'message': 'Error details...'}\n",
    "    \"\"\"\n",
    "    headers = {\"Authorization\": f\"token {GITHUB_TOKEN}\"}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(f\"https://api.github.com/repos/{owner}/{repo}/git/trees/main?recursive=1\", headers=headers)\n",
    "        response.raise_for_status()\n",
    "        tree = response.json().get('tree', [])\n",
    "        files = [item['path'] for item in tree if item['type'] == 'blob']\n",
    "        return {\"status\": \"success\", \"files\": files}\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "    \n",
    "def get_file_contents(repo: str, file_path: str, owner: str) -> dict:\n",
    "    \"\"\"\n",
    "    Fetches the contents of a file in the repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_url (str): The URL of the repository.\n",
    "        file_path (str): The path to the file within the repository.\n",
    "        owner (str): The owner of the repository.\n",
    "    Returns:\n",
    "        A dictionary with a 'status' key.\n",
    "        On success: {'status': 'success', 'content': 'file content...'}\n",
    "        On error: {'status': 'error', 'message': 'Error details...'}\n",
    "    \"\"\"\n",
    "    headers = {\"Authorization\": f\"token {GITHUB_TOKEN}\"}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(f\"https://api.github.com/repos/{owner}/{repo}/contents/{file_path}\", headers=headers)\n",
    "        response.raise_for_status()\n",
    "        content = response.json().get('content', '')\n",
    "        decoded_content = base64.b64decode(content).decode('utf-8')\n",
    "        return {\"status\": \"success\", \"content\": decoded_content}\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "    \n",
    "def save_selected_files(files: list[str], tool_context: ToolContext) -> dict:\n",
    "    \"\"\"Saves the provided list of selected file paths to the session state.\n",
    "    \n",
    "    Args:\n",
    "        files (ist[str]): list of files to be saved for later\n",
    "        tool_context (ToolContext): will assist in changing the state\n",
    "    Returns:\n",
    "        A dictionary with a 'status' key\n",
    "        On success: {'status': 'success', 'files_saved: int'}\n",
    "    \"\"\"\n",
    "    tool_context.state[\"selected_files_list\"] = files\n",
    "    tool_context.state[\"all_file_contents\"] = {}\n",
    "    return {\"status\": \"success\", \"files_saved\": len(files)}\n",
    "\n",
    "def fetch_all_content(tool_context: ToolContext) -> dict:\n",
    "    \"\"\"\n",
    "    Takes a list of file paths from the state, reads the content of each\n",
    "    one, and saves all content to the state in a dictionary.\n",
    "    \n",
    "    Args:\n",
    "        tool_context (ToolContext): will assist in changing the state\n",
    "    Returns:\n",
    "        A dictionary with a 'status' key\n",
    "        On success: {'status': 'success', 'files_fetched: int'}\n",
    "    \"\"\"\n",
    "    owner = tool_context.state.get(\"owner\")\n",
    "    repo = tool_context.state.get(\"repo\")\n",
    "    files_to_read = tool_context.state.get(\"selected_files_list\", [])\n",
    "    \n",
    "    all_content = {}\n",
    "    print(f\"Tool: Fetching content for {len(files_to_read)} files...\")\n",
    "    for path in files_to_read:\n",
    "        result = get_file_contents(repo=repo, file_path=path, owner=owner)\n",
    "        if result[\"status\"] == \"success\":\n",
    "            all_content[path] = result[\"content\"]\n",
    "        else:\n",
    "            all_content[path] = f\"Error fetching file: {result['message']}\"\n",
    "\n",
    "    tool_context.state[\"all_file_contents\"] = all_content\n",
    "    print(\"Tool: Finished fetching all file contents.\")\n",
    "    return {\"status\": \"success\", \"files_fetched\": len(all_content)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4bfda99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import PROMPT_FILE_SELECTOR, PROMPT_REPORT_SYNTHESIZER\n",
    "\n",
    "file_selector_agent = Agent(\n",
    "    model=MODEL_GEMINI_2_0_FLASH,\n",
    "    name=\"File_Selector\",\n",
    "    description=\"This agent selects the most relevant files from a repository.\",\n",
    "    instruction=PROMPT_FILE_SELECTOR,\n",
    "    tools=[list_files, get_file_contents, save_selected_files]\n",
    ")\n",
    "\n",
    "content_fetcher_agent = Agent(\n",
    "    model=MODEL_GEMINI_2_0_FLASH,\n",
    "    name=\"Content_Fetcher\",\n",
    "    description=\"Fetches the content of all selected files.\",\n",
    "    instruction=\"You must immediately call the `fetch_all_content` tool.\",\n",
    "    tools=[fetch_all_content],\n",
    ")\n",
    "\n",
    "report_synthesizer_agent = Agent(\n",
    "    model=MODEL_GEMINI_2_0_FLASH,\n",
    "    name=\"Report_Synthesizer\",\n",
    "    description=\"Synthesizes all file contents into a final report.\",\n",
    "    instruction=PROMPT_REPORT_SYNTHESIZER,\n",
    "    output_key=\"analysis_results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "adbf0ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef parse_json_response(response: str, tool_context: ToolContext) -> dict:\\n    \"\"\"\\n    Parses a JSON response string into a dictionary. Aims to get rid of formatting issues like outputting backticks.\\n    \\n    Args:\\n        response (str): The JSON response string.\\n        \\n    Returns:\\n        dict: The parsed JSON object.\\n    \"\"\"\\n    try:\\n        # Remove any leading/trailing whitespace and backticks\\n        response = response.strip().replace(\"```json\", \"\").replace(\"```\", \"\")\\n        data = json.loads(response)\\n        files = data.get(\"selected_files\", [])\\n        \\n        tool_context.state[\"validated_files_list\"] = files\\n        tool_context.state[\"individual_analyses\"] = \"\"\\n        return {\"status\": \"success\", \"files_validated\": len(files)} \\n    except Exception as e:\\n        return {\"status\": \"error\", \"message\": str(e)}\\n    \\n    \\nvalidator_agent = Agent(\\n    model=MODEL_GEMINI_2_0_FLASH,\\n    name=\"JSON_Validator\",\\n    description=\"This agent validates the JSON response and triggers a tool to parse the JSON response.\",\\n    instruction=\"\"\"\\n    You are a data validation trigger.\\n    You MUST immediately use the `parse_json_response` tool on the text from the `{selected_files}` context.\\n    \"\"\",\\n    tools=[parse_json_response]\\n    )\\n    \\n    '"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "'''\n",
    "def parse_json_response(response: str, tool_context: ToolContext) -> dict:\n",
    "    \"\"\"\n",
    "    Parses a JSON response string into a dictionary. Aims to get rid of formatting issues like outputting backticks.\n",
    "    \n",
    "    Args:\n",
    "        response (str): The JSON response string.\n",
    "        \n",
    "    Returns:\n",
    "        dict: The parsed JSON object.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Remove any leading/trailing whitespace and backticks\n",
    "        response = response.strip().replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "        data = json.loads(response)\n",
    "        files = data.get(\"selected_files\", [])\n",
    "        \n",
    "        tool_context.state[\"validated_files_list\"] = files\n",
    "        tool_context.state[\"individual_analyses\"] = \"\"\n",
    "        return {\"status\": \"success\", \"files_validated\": len(files)} \n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "    \n",
    "    \n",
    "validator_agent = Agent(\n",
    "    model=MODEL_GEMINI_2_0_FLASH,\n",
    "    name=\"JSON_Validator\",\n",
    "    description=\"This agent validates the JSON response and triggers a tool to parse the JSON response.\",\n",
    "    instruction=\"\"\"\n",
    "    You are a data validation trigger.\n",
    "    You MUST immediately use the `parse_json_response` tool on the text from the `{selected_files}` context.\n",
    "    \"\"\",\n",
    "    tools=[parse_json_response]\n",
    "    )\n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53709f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_pipeline = SequentialAgent(\n",
    "    name=\"Analysis_Pipeline\",\n",
    "    sub_agents=[file_selector_agent, content_fetcher_agent, report_synthesizer_agent]\n",
    ")\n",
    "\n",
    "root_agent = Agent(\n",
    "    model=MODEL_GEMINI_2_0_FLASH,\n",
    "    name=\"Root_Agent\",\n",
    "    description=\"Manages the repository analysis workflow.\",\n",
    "    instruction=\"\"\"Immediately start the Analysis_Pipeline to handle the request.\"\"\",\n",
    "    sub_agents=[analysis_pipeline],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00d2f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import uuid\n",
    "from google.genai.types import Content\n",
    "\n",
    "async def run_pipeline():\n",
    "    \"\"\"Sets up and runs the analysis pipeline with detailed logging and error handling.\"\"\"\n",
    "    \n",
    "    user_id = f\"user-{uuid.uuid4()}\"\n",
    "    session_id = f\"session-{uuid.uuid4()}\"\n",
    "    app_name = \"Repo_Analysis\"\n",
    "    \n",
    "    initial_state_data = {\n",
    "        \"owner\": \"mevitts\",\n",
    "        \"repo\": \"spanish_chat_bot\"\n",
    "    }\n",
    "\n",
    "    initial_message = Content(role=\"user\", parts=[types.Part(text=\"Please start the analysis of the repository.\")])\n",
    "\n",
    "    session_service = InMemorySessionService()\n",
    "    runner = Runner(\n",
    "        app_name=app_name,\n",
    "        session_service=session_service,\n",
    "        agent=root_agent\n",
    "    )\n",
    "    await session_service.create_session(\n",
    "        app_name=app_name, \n",
    "        user_id=user_id, \n",
    "        session_id=session_id, \n",
    "        state=initial_state_data\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        #execute the pipeline\n",
    "        print(\"Starting analysis pipeline...\")\n",
    "        events = runner.run_async(\n",
    "            user_id=user_id,\n",
    "            session_id=session_id,\n",
    "            new_message=initial_message\n",
    "        )\n",
    "\n",
    "        async for event in events:\n",
    "            author = event.author or \"System\"\n",
    "\n",
    "            if event.content and event.content.parts:\n",
    "                part = event.content.parts[0]\n",
    "                if part.function_call:\n",
    "                    tool_call = part.function_call\n",
    "                elif part.function_response:\n",
    "                    response = part.function_response\n",
    "                    print(f\"Got Tool Response from '{response.name}': {str(response.response)[:200]}...\")\n",
    "                elif part.text:\n",
    "                    print(f\"Agent Text Response: {part.text[:150]}...\")\n",
    "\n",
    "            if event.actions and event.actions.state_delta:\n",
    "                print(f\"State Change: {event.actions.state_delta}\")\n",
    "            \n",
    "            if event.is_final_response():\n",
    "                print(\"Final response from agent received.\")\n",
    "        \n",
    "        print(\"\\nPipeline execution complete.\")\n",
    "\n",
    "        final_session = await session_service.get_session(\n",
    "            app_name=app_name, user_id=user_id, session_id=session_id\n",
    "        )\n",
    "\n",
    "        final_analysis = final_session.state.get(\"analysis_results\") if final_session else None\n",
    "\n",
    "        if final_analysis:\n",
    "            print(\"\\n--- Final Analysis Results ---\")\n",
    "            print(str(final_analysis).strip().replace(\"```markdown\", \"\").replace(\"```\", \"\").strip())\n",
    "        else:\n",
    "            print(\"\\nCould not retrieve the final analysis from the session state.\")\n",
    "            if final_session:\n",
    "                print(\"Dumping final state for debugging:\")\n",
    "                print(final_session.state)\n",
    "\n",
    "    except (Exception) as e:\n",
    "        # --- ROBUST ERROR HANDLING ---\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"API ERROR: The model is currently overloaded or you have reached your quota.\")\n",
    "        print(\"This is a temporary issue with the Google API, not a bug in the pipeline.\")\n",
    "        print(\"Please wait a few minutes and try running the pipeline again.\")\n",
    "        print(f\"Details: {e}\")\n",
    "        print(\"=\"*50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "620ef16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting analysis pipeline...\n",
      "  ðŸ“¦ Got Tool Response from 'transfer_to_agent': {'result': None}...\n",
      "  ðŸ“¦ Got Tool Response from 'list_files': {'status': 'success', 'files': ['.gitignore', 'README.md', 'side_proj/.gitignore', 'side_proj/.ipynb_checkpoints/spanish_conversation_bot-checkpoint.ipynb', 'side_proj/README.md', 'side_proj/Rules/fro...\n",
      "  ðŸ“¦ Got Tool Response from 'get_file_contents': {'status': 'success', 'content': '# Contigo: Spanish Conversational Voice Bot (Whisper + LLM + TTS)\\n\\nContigo is a modular, voice-based Spanish conversational bot that enables **spoken conversations*...\n",
      "  ðŸ’¬ Agent Text Response: The README indicates this is a Spanish conversational voice bot using Whisper, an LLM, and TTS. The core pipeline appears to be operational, with a ba...\n",
      "  ðŸ“¦ Got Tool Response from 'save_selected_files': {'status': 'success', 'files_saved': 12}...\n",
      "  ðŸ’¾ State Change: {'selected_files_list': ['side_proj/requirements.txt', 'side_proj/src/spanish_chat_bot/conversation.py', 'side_proj/src/spanish_chat_bot/endpoint_handler.py', 'side_proj/frontend/src/App.jsx', 'side_proj/notebooks/inference_pipeline.ipynb', 'side_proj/spanish_conversation_bot.ipynb', 'side_proj/src/spanish_chat_bot/audio/recorder.py', 'side_proj/src/spanish_chat_bot/audio/tts.py', 'side_proj/src/spanish_chat_bot/models/response_generator.py', 'side_proj/src/spanish_chat_bot/models/transcriber.py', 'side_proj/frontend/package.json', 'side_proj/frontend/vite.config.js'], 'all_file_contents': {}}\n",
      "  ðŸ’¬ Agent Text Response: {\"files_saved\": 12, \"status\": \"success\"}...\n",
      "ðŸ Final response from agent received.\n",
      "Tool: Fetching content for 12 files...\n",
      "Tool: Finished fetching all file contents.\n",
      "  ðŸ“¦ Got Tool Response from 'fetch_all_content': {'status': 'success', 'files_fetched': 12}...\n",
      "  ðŸ’¾ State Change: {'all_file_contents': {'side_proj/requirements.txt': 'torch>=2.0.0\\ntransformers>=4.30.0\\nsounddevice>=0.4.6\\nnumpy>=1.24.0\\nscipy>=1.10.0\\npython-dotenv>=1.0.0\\ngoogle-generativeai>=0.3.0\\nTTS>=0.17.0\\ndatasets>=2.12.0\\nevaluate>=0.4.0\\nwandb>=0.15.0\\nfastapi>=0.104.0\\nuvicorn>=0.24.0 ', 'side_proj/src/spanish_chat_bot/conversation.py': '\"\"\"\\nMain conversation handler that orchestrates the Spanish chat bot.\\n\"\"\"\\nfrom typing import Optional\\nimport time\\nfrom .audio import record_audio, text_to_speech\\nfrom .models import transcribe_audio, generate_response\\n\\nclass Conversation:\\n    def __init__(self):\\n        \"\"\"Initialize the conversation handler.\"\"\"\\n        self.is_active = False\\n        self.conversation_history = []\\n        self.audio_files = []\\n\\n\\n    def process_audio(self, audio_data):\\n        text = transcribe_audio(audio_data)\\n        \\n        response = generate_response(text)\\n        \\n        audio_path = text_to_speech(response)\\n        \\n        #conversation history\\n        self.conversation_history.append({\\n            \\'user\\': text,\\n            \\'bot\\': response,\\n            \\'audio_path\\': audio_path,\\n            \\'timestamp\\': time.time()\\n        })\\n        \\n        return {\\n            \\'transcription\\': text,\\n            \\'response\\': response,\\n            \\'audio_path\\': audio_path\\n        }\\n    \\n        \\n        \\n    def start(self):\\n        \"\"\"Start the conversation loop.\"\"\"\\n        self.is_active = True\\n        print(\"Â¡Hola! Estoy listo para conversar. Presiona Ctrl+C para salir.\")\\n        \\n        while self.is_active:\\n            try:\\n                #record audio\\n                print(\"\\\\nEscuchando...\")\\n                audio = record_audio()\\n                \\n                #transcribe audio to text\\n                text = transcribe_audio(audio)\\n                #print(f\"Dijiste: {text}? Y/N\")\\n                if not text:\\n                    print(\"No pude entender eso. Â¿PodrÃ­as repetirlo?\")\\n                    continue\\n                \\n                #generate response\\n                response = generate_response(text)\\n                \\n                #Convert response to speech and play it\\n                text_to_speech(response)\\n                \\n                #conversation history\\n                self.conversation_history.append({\\n                    \\'user\\': text,\\n                    \\'bot\\': response,\\n                    \\'timestamp\\': time.time()\\n                })\\n                \\n            except KeyboardInterrupt:\\n                print(\"\\\\nÂ¡Hasta luego!\")\\n                self.is_active = False\\n            except Exception as e:\\n                print(f\"Lo siento, hubo un error: {str(e)}\")\\n                continue\\n            \\n    def stop(self):\\n        \"\"\"Stop the conversation.\"\"\"\\n        self.is_active = False\\n\\n\\ndef main():\\n    \"\"\"Main entry point for the conversation.\"\"\"\\n    conversation = Conversation()\\n    conversation.start()\\n\\n\\nif __name__ == \"__main__\":\\n    main()', 'side_proj/src/spanish_chat_bot/endpoint_handler.py': 'from dotenv import load_dotenv\\nload_dotenv(\"tokens.env\")\\n\\n\\nfrom fastapi import FastAPI, WebSocket, HTTPException\\nfrom pydantic import BaseModel\\nimport numpy as np\\nimport traceback\\nimport logging\\nfrom .audio import record_audio, text_to_speech\\nfrom .models import transcribe_audio, generate_response\\n\\n# Set up logging\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger(__name__)\\n\\napp = FastAPI()\\n\\nclass AudioData(BaseModel):\\n    audioData: list[float]\\n\\nclass TextData(BaseModel):\\n    text: str\\n\\n@app.post(\"/api/start-recording\")\\nasync def start_recording():\\n    try:\\n        audio = record_audio()\\n        return {\"audioData\": audio.tolist()}\\n    except Exception as e:\\n        logger.error(f\"Error in start_recording: {str(e)}\")\\n        logger.error(traceback.format_exc())\\n        raise HTTPException(status_code=500, detail=str(e))\\n\\n@app.post(\"/api/transcribe\")\\nasync def handle_transcription(audio: AudioData):\\n    try:\\n        audio_array = np.array(audio.audioData)\\n        # Get the recorded audio turn into an array and transcribe it\\n        text = transcribe_audio(audio=audio_array)\\n        return {\"transcription\": text}\\n    except Exception as e:\\n        logger.error(f\"Error in handle_transcription: {str(e)}\")\\n        logger.error(traceback.format_exc())\\n        raise HTTPException(status_code=500, detail=str(e))\\n\\n@app.post(\"/api/generate\")\\nasync def handle_generation(text_data: TextData): #since frontend sends in json form, need to introduce it as TextData to validate JSON\\n    try:\\n        logger.info(f\"[testing] Starting generation for text: {text_data.text}\")\\n        \\n        # Generate response and convert to speech\\n        # generate_response expects text and optional model_name and api_key\\n        logger.info(\"[testing] Calling generate_response...\")\\n        response = generate_response(text=text_data.text)\\n        logger.info(f\"[testing] Generated response: {response}\")\\n        \\n        logger.info(\"[testing] Calling text_to_speech...\")\\n        audio_url = text_to_speech(response)\\n        logger.info(f\"[testing] Audio URL: {audio_url}\")\\n        \\n        return {\\n            \"response\": response,\\n            \"audioUrl\": audio_url\\n        }\\n    except Exception as e:\\n        logger.error(f\"Error in handle_generation: {str(e)}\")\\n        logger.error(traceback.format_exc())\\n        raise HTTPException(status_code=500, detail=str(e))\\n\\n@app.websocket(\"/ws\")\\nasync def websocket_endpoint(websocket: WebSocket):\\n    await websocket.accept()\\n    # Handle real-time updates if needed\\n', 'side_proj/frontend/src/App.jsx': 'import { useState } from \\'react\\'\\nimport \\'./App.css\\'\\nimport ChatMessage from \\'./components/Message\\'\\nimport RecordingInterface from \\'./components/RecordingInterface\\'\\n\\nfunction App() {\\n  const [messages, setMessages] = useState([]);\\n\\n  // This function will be called by RecordingInterface\\n  const handleNewMessage = (newMessage) => {\\n    // Add user message to the chat\\n    setMessages(prev => [...prev, { text: newMessage.user, isBot: false }]);\\n    \\n    // Add bot response to the chat\\n    setMessages(prev => [...prev, { text: newMessage.bot, isBot: true }]);\\n  };\\n\\n  //main container that is whole screen, gray bg\\n  return (\\n    <div className=\"min-h-screen bg-gray-100\">\\n      {/*center container with padding, with extra bottom padding to avoid overlap with fixed recorder */}\\n      <div className=\"container mx-auto p-4 pb-40\">\\n        {/*white chat card, takes remaining space, max width*/}\\n        <div className=\"bg-white rounded-lg shadow-lg p-6 max-w-xl mx-auto\">\\n          <h1 className=\"text-2xl font-bold text-center mb-6 text-gray-800\">\\n          {/*Title*/}\\n            Contigo\\n          </h1>\\n          {/*Message area - scrollable, takes remaining space*/}\\n          <div className=\"space-y-4\">\\n            {/*Messages rendered here*/}\\n            {messages.map((msg, index) => (\\n              <ChatMessage \\n                key={index}\\n                message={msg.text}\\n                isBot={msg.isBot}\\n              />\\n            ))}\\n          </div>\\n          {/*recording interface (margin top for separaton)*/}\\n          <div className=\"mt-6\">\\n            <RecordingInterface onNewMessage={handleNewMessage} />\\n          </div>\\n        </div>\\n      </div>\\n    </div>\\n  )\\n}\\n\\nexport default App\\n', 'side_proj/notebooks/inference_pipeline.ipynb': '{\\n \"cells\": [\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"import torch\\\\n\",\\n    \"from transformers import WhisperProcessor, WhisperForConditionalGeneration, WhisperFeatureExtractor, WhisperTokenizer\\\\n\",\\n    \"import os\\\\n\",\\n    \"\\\\n\",\\n    \"import torch\\\\n\",\\n    \"if torch.cuda.is_available():\\\\n\",\\n    \"    device = \\'cuda\\'\\\\n\",\\n    \"    print(f\\'Using GPU: {torch.cuda.get_device_name(0)}\\')\\\\n\",\\n    \"else:\\\\n\",\\n    \"    device = \\'cpu\\'\\\\n\",\\n    \"    print(\\'GPU not available, using CPU.\\')\\\\n\",\\n    \"    \\\\n\",\\n    \"device = torch.device(\\\\\"cuda\\\\\" if torch.cuda.is_available() else \\\\\"cpu\\\\\")\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 3,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"   from dotenv import load_dotenv\\\\n\",\\n    \"   import os\\\\n\",\\n    \"\\\\n\",\\n    \"   load_dotenv(\\\\\"HF.config\\\\\")\\\\n\",\\n    \"   hf_token = os.getenv(\\\\\"HF_TOKEN\\\\\")\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 3,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"#load feature extractor from pre-trained checkpoint\\\\n\",\\n    \"feature_extractor = WhisperFeatureExtractor.from_pretrained(\\\\\"openai/whisper-small\\\\\")\\\\n\",\\n    \"tokenizer = WhisperTokenizer.from_pretrained(\\\\\"openai/whisper-small\\\\\", language=\\\\\"Spanish\\\\\", task=\\\\\"transcribe\\\\\")\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 124,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"processor = WhisperProcessor.from_pretrained(\\\\\"openai/whisper-small\\\\\", language=\\\\\"spanish\\\\\", task=\\\\\"transcribe\\\\\")\\\\n\",\\n    \"og_model = WhisperForConditionalGeneration.from_pretrained(\\\\\"openai/whisper-small\\\\\")\\\\n\",\\n    \"model = WhisperForConditionalGeneration.from_pretrained(\\\\\"mevitts/whisper-small-es\\\\\")\\\\n\",\\n    \"\\\\n\",\\n    \"og_model = og_model.to(device)\\\\n\",\\n    \"model = model.to(device)\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 125,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from transformers import GenerationConfig\\\\n\",\\n    \"\\\\n\",\\n    \"\\\\n\",\\n    \"# Set model configuration\\\\n\",\\n    \"og_model.generation_config = GenerationConfig.from_pretrained(\\\\\"openai/whisper-small\\\\\")\\\\n\",\\n    \"og_model.generation_config.language = \\\\\"spanish\\\\\"\\\\n\",\\n    \"og_model.generation_config.task = \\\\\"transcribe\\\\\"\\\\n\",\\n    \"og_model.config.pad_token_id = og_model.config.eos_token_id\\\\n\",\\n    \"\\\\n\",\\n    \"model.generation_config = GenerationConfig.from_pretrained(\\\\\"openai/whisper-small\\\\\")\\\\n\",\\n    \"model.generation_config.language = \\\\\"spanish\\\\\"\\\\n\",\\n    \"model.generation_config.task = \\\\\"transcribe\\\\\"\\\\n\",\\n    \"model.config.pad_token_id = model.config.eos_token_id\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 121,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"import sounddevice as sd\\\\n\",\\n    \"import numpy as np\\\\n\",\\n    \"from scipy.signal import butter, filtfilt\\\\n\",\\n    \"\\\\n\",\\n    \"\\\\n\",\\n    \"def record_audio(duration=5, sample_rate=16000):\\\\n\",\\n    \"    \\\\\"\\\\\"\\\\\"Record audio for set duration\\\\\"\\\\\"\\\\\"\\\\n\",\\n    \"    print(\\\\\"Say what you want\\\\\")\\\\n\",\\n    \"    myrecording = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1)\\\\n\",\\n    \"    sd.wait()\\\\n\",\\n    \"    \\\\n\",\\n    \"    #convert to single array, then normalize audio\\\\n\",\\n    \"    audio = myrecording.flatten()\\\\n\",\\n    \"    \\\\n\",\\n    \"    audio = audio / np.max(np.abs(audio))\\\\n\",\\n    \"    \\\\n\",\\n    \"    # apply filter to reduce noise\\\\n\",\\n    \"    nyquist = sample_rate / 2\\\\n\",\\n    \"    cutoff = 100 #Hz\\\\n\",\\n    \"    b, a = butter(4, cutoff/nyquist, btype=\\'high\\')\\\\n\",\\n    \"    audio = filtfilt(b, a, audio)\\\\n\",\\n    \"    \\\\n\",\\n    \"    audio = audio / np.max(np.abs(audio))\\\\n\",\\n    \"    return audio\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 127,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"def transcribe_audio(audio, sample_rate=16000, model=og_model):\\\\n\",\\n    \"    \\\\n\",\\n    \"    \\\\n\",\\n    \"    inputs = feature_extractor(\\\\n\",\\n    \"        audio,\\\\n\",\\n    \"        sampling_rate=sample_rate,\\\\n\",\\n    \"        return_tensors=\\\\\"pt\\\\\"\\\\n\",\\n    \"    )\\\\n\",\\n    \"    input_features = inputs.input_features.to(device)\\\\n\",\\n    \"    \\\\n\",\\n    \"    predicted_ids = model.generate(\\\\n\",\\n    \"        input_features,\\\\n\",\\n    \"        max_length=448,  # Increased from default\\\\n\",\\n    \"        num_beams=5,     # Increased from default\\\\n\",\\n    \"        temperature=0.7  # Slightly reduced from default\\\\n\",\\n    \"    )\\\\n\",\\n    \"    transcription = tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)[0]\\\\n\",\\n    \"    \\\\n\",\\n    \"    return transcription\\\\n\",\\n    \"    \"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"rec = record_audio()\\\\n\",\\n    \"msg = transcribe_audio(rec, model=og_model)\\\\n\",\\n    \"matt_msg = transcribe_audio(rec, model=model)\\\\n\",\\n    \"print(msg)\\\\n\",\\n    \"print(matt_msg)\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 157,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"\\\\n\",\\n    \"from dotenv import load_dotenv\\\\n\",\\n    \"import os\\\\n\",\\n    \"import google.generativeai as gem\\\\n\",\\n    \"\\\\n\",\\n    \"load_dotenv(\\\\\"HF.config\\\\\")\\\\n\",\\n    \"google_api_key = os.getenv(\\\\\"GOOGLE_API_KEY\\\\\")\\\\n\",\\n    \"if not google_api_key:\\\\n\",\\n    \"    raise ValueError(\\\\\"GOOGLE_API_KEY not found in environment variables\\\\\")\\\\n\",\\n    \"#genai.configure(api_key=google_api_key)\\\\n\",\\n    \"gem.configure(api_key=google_api_key)\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 163,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"gemini = gem.GenerativeModel(\\'gemini-2.0-flash\\')\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 166,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"def generate_response(text):\\\\n\",\\n    \"    response = gemini.generate_content(\\\\n\",\\n    \"            f\\\\\"Eres un asistente conversacional en espaÃ±ol. Adaptarse al contexto de la conversaciÃ³n. (Por ejemplo: si la persona te diga algo como si fuera tu amigo o un familiar, respondele con ese rol.) Mantenga la duraciÃ³n de tu respuesta corta. Responde de manera natural y conversacional a: {text}\\\\\"\\\\n\",\\n    \"        )\\\\n\",\\n    \"    return response\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"generate_response(\\\\\"Cual es tu comida favorita?\\\\\")\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 160,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"def test_full_pipeline():\\\\n\",\\n    \"    print(\\\\\"Recording audio...\\\\\")\\\\n\",\\n    \"    audio = record_audio()\\\\n\",\\n    \"    \\\\n\",\\n    \"    print(\\\\\"\\\\\\\\nTranscribing audio...\\\\\")\\\\n\",\\n    \"    transcription = transcribe_audio(audio, model=og_model)\\\\n\",\\n    \"    print(f\\\\\"Transcription: {transcription}\\\\\")\\\\n\",\\n    \"    \\\\n\",\\n    \"    print(\\\\\"\\\\\\\\nGenerating response...\\\\\")\\\\n\",\\n    \"    response = generate_response(transcription)\\\\n\",\\n    \"    print(f\\\\\"Response: {response}\\\\\")\\\\n\",\\n    \"    \\\\n\",\\n    \"    return transcription, response\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"transcription, response = test_full_pipeline()\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 10,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"import torch\\\\n\",\\n    \"from transformers import WhisperProcessor, WhisperForConditionalGeneration, WhisperFeatureExtractor, WhisperTokenizer\\\\n\",\\n    \"import os\\\\n\",\\n    \"import sounddevice as sd\\\\n\",\\n    \"import numpy as np\\\\n\",\\n    \"from scipy.io import wavfile\\\\n\",\\n    \"from TTS.api import TTS\\\\n\",\\n    \"\\\\n\",\\n    \"def text_to_speech(text, output_path=\\\\\"output.wav\\\\\"):\\\\n\",\\n    \"    # Initialize TTS with Spanish xtts model\\\\n\",\\n    \"    tts = TTS(model_name=\\\\\"tts_models/es/css10/vits\\\\\", progress_bar=False)\\\\n\",\\n    \"    \\\\n\",\\n    \"    # Generate speech\\\\n\",\\n    \"    tts.tts_to_file(text=text, file_path=output_path)\\\\n\",\\n    \"    \\\\n\",\\n    \"    # Play the audio\\\\n\",\\n    \"    sample_rate, audio = wavfile.read(output_path)\\\\n\",\\n    \"    sd.play(audio, sample_rate)\\\\n\",\\n    \"    sd.wait()\\\\n\",\\n    \"    \\\\n\",\\n    \"    return output_path\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 11,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \" > tts_models/es/css10/vits is already downloaded.\\\\n\",\\n      \" > Using model: vits\\\\n\",\\n      \" > Setting up Audio Processor...\\\\n\",\\n      \" | > sample_rate:22050\\\\n\",\\n      \" | > resample:False\\\\n\",\\n      \" | > num_mels:80\\\\n\",\\n      \" | > log_func:np.log10\\\\n\",\\n      \" | > min_level_db:0\\\\n\",\\n      \" | > frame_shift_ms:None\\\\n\",\\n      \" | > frame_length_ms:None\\\\n\",\\n      \" | > ref_level_db:None\\\\n\",\\n      \" | > fft_size:1024\\\\n\",\\n      \" | > power:None\\\\n\",\\n      \" | > preemphasis:0.0\\\\n\",\\n      \" | > griffin_lim_iters:None\\\\n\",\\n      \" | > signal_norm:None\\\\n\",\\n      \" | > symmetric_norm:None\\\\n\",\\n      \" | > mel_fmin:0\\\\n\",\\n      \" | > mel_fmax:None\\\\n\",\\n      \" | > pitch_fmin:None\\\\n\",\\n      \" | > pitch_fmax:None\\\\n\",\\n      \" | > spec_gain:20.0\\\\n\",\\n      \" | > stft_pad_mode:reflect\\\\n\",\\n      \" | > max_norm:1.0\\\\n\",\\n      \" | > clip_norm:True\\\\n\",\\n      \" | > do_trim_silence:False\\\\n\",\\n      \" | > trim_db:60\\\\n\",\\n      \" | > do_sound_norm:False\\\\n\",\\n      \" | > do_amp_to_db_linear:True\\\\n\",\\n      \" | > do_amp_to_db_mel:True\\\\n\",\\n      \" | > do_rms_norm:False\\\\n\",\\n      \" | > db_level:None\\\\n\",\\n      \" | > stats_path:None\\\\n\",\\n      \" | > base:10\\\\n\",\\n      \" | > hop_length:256\\\\n\",\\n      \" | > win_length:1024\\\\n\",\\n      \" > initialization of speaker-embedding layers.\\\\n\",\\n      \" > initialization of language-embedding layers.\\\\n\",\\n      \" > Text splitted to sentences.\\\\n\",\\n      \"[\\'Mirense a estos pinches gringos!\\']\\\\n\",\\n      \" > Processing time: 0.9494185447692871\\\\n\",\\n      \" > Real-time factor: 0.31086182751489044\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"output_wav = text_to_speech(\\\\\"Mirense a estos pinches gringos!\\\\\")\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 15,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"Available Spanish models:\\\\n\",\\n      \"\\\\n\",\\n      \" Name format: type/language/dataset/model\\\\n\",\\n      \" 1: tts_models/multilingual/multi-dataset/xtts_v2\\\\n\",\\n      \" 2: tts_models/multilingual/multi-dataset/xtts_v1.1\\\\n\",\\n      \" 3: tts_models/multilingual/multi-dataset/your_tts\\\\n\",\\n      \" 4: tts_models/multilingual/multi-dataset/bark\\\\n\",\\n      \" 5: tts_models/bg/cv/vits\\\\n\",\\n      \" 6: tts_models/cs/cv/vits\\\\n\",\\n      \" 7: tts_models/da/cv/vits\\\\n\",\\n      \" 8: tts_models/et/cv/vits\\\\n\",\\n      \" 9: tts_models/ga/cv/vits\\\\n\",\\n      \" 10: tts_models/en/ek1/tacotron2\\\\n\",\\n      \" 11: tts_models/en/ljspeech/tacotron2-DDC\\\\n\",\\n      \" 12: tts_models/en/ljspeech/tacotron2-DDC_ph\\\\n\",\\n      \" 13: tts_models/en/ljspeech/glow-tts\\\\n\",\\n      \" 14: tts_models/en/ljspeech/speedy-speech\\\\n\",\\n      \" 15: tts_models/en/ljspeech/tacotron2-DCA\\\\n\",\\n      \" 16: tts_models/en/ljspeech/vits\\\\n\",\\n      \" 17: tts_models/en/ljspeech/vits--neon\\\\n\",\\n      \" 18: tts_models/en/ljspeech/fast_pitch\\\\n\",\\n      \" 19: tts_models/en/ljspeech/overflow\\\\n\",\\n      \" 20: tts_models/en/ljspeech/neural_hmm\\\\n\",\\n      \" 21: tts_models/en/vctk/vits\\\\n\",\\n      \" 22: tts_models/en/vctk/fast_pitch\\\\n\",\\n      \" 23: tts_models/en/sam/tacotron-DDC\\\\n\",\\n      \" 24: tts_models/en/blizzard2013/capacitron-t2-c50\\\\n\",\\n      \" 25: tts_models/en/blizzard2013/capacitron-t2-c150_v2\\\\n\",\\n      \" 26: tts_models/en/multi-dataset/tortoise-v2\\\\n\",\\n      \" 27: tts_models/en/jenny/jenny\\\\n\",\\n      \" 28: tts_models/es/mai/tacotron2-DDC [already downloaded]\\\\n\",\\n      \" 29: tts_models/es/css10/vits [already downloaded]\\\\n\",\\n      \" 30: tts_models/fr/mai/tacotron2-DDC\\\\n\",\\n      \" 31: tts_models/fr/css10/vits\\\\n\",\\n      \" 32: tts_models/uk/mai/glow-tts\\\\n\",\\n      \" 33: tts_models/uk/mai/vits\\\\n\",\\n      \" 34: tts_models/zh-CN/baker/tacotron2-DDC-GST\\\\n\",\\n      \" 35: tts_models/nl/mai/tacotron2-DDC\\\\n\",\\n      \" 36: tts_models/nl/css10/vits\\\\n\",\\n      \" 37: tts_models/de/thorsten/tacotron2-DCA\\\\n\",\\n      \" 38: tts_models/de/thorsten/vits\\\\n\",\\n      \" 39: tts_models/de/thorsten/tacotron2-DDC\\\\n\",\\n      \" 40: tts_models/de/css10/vits-neon\\\\n\",\\n      \" 41: tts_models/ja/kokoro/tacotron2-DDC\\\\n\",\\n      \" 42: tts_models/tr/common-voice/glow-tts\\\\n\",\\n      \" 43: tts_models/it/mai_female/glow-tts\\\\n\",\\n      \" 44: tts_models/it/mai_female/vits\\\\n\",\\n      \" 45: tts_models/it/mai_male/glow-tts\\\\n\",\\n      \" 46: tts_models/it/mai_male/vits\\\\n\",\\n      \" 47: tts_models/ewe/openbible/vits\\\\n\",\\n      \" 48: tts_models/hau/openbible/vits\\\\n\",\\n      \" 49: tts_models/lin/openbible/vits\\\\n\",\\n      \" 50: tts_models/tw_akuapem/openbible/vits\\\\n\",\\n      \" 51: tts_models/tw_asante/openbible/vits\\\\n\",\\n      \" 52: tts_models/yor/openbible/vits\\\\n\",\\n      \" 53: tts_models/hu/css10/vits\\\\n\",\\n      \" 54: tts_models/el/cv/vits\\\\n\",\\n      \" 55: tts_models/fi/css10/vits\\\\n\",\\n      \" 56: tts_models/hr/cv/vits\\\\n\",\\n      \" 57: tts_models/lt/cv/vits\\\\n\",\\n      \" 58: tts_models/lv/cv/vits\\\\n\",\\n      \" 59: tts_models/mt/cv/vits\\\\n\",\\n      \" 60: tts_models/pl/mai_female/vits\\\\n\",\\n      \" 61: tts_models/pt/cv/vits\\\\n\",\\n      \" 62: tts_models/ro/cv/vits\\\\n\",\\n      \" 63: tts_models/sk/cv/vits\\\\n\",\\n      \" 64: tts_models/sl/cv/vits\\\\n\",\\n      \" 65: tts_models/sv/cv/vits\\\\n\",\\n      \" 66: tts_models/ca/custom/vits\\\\n\",\\n      \" 67: tts_models/fa/custom/glow-tts\\\\n\",\\n      \" 68: tts_models/bn/custom/vits-male\\\\n\",\\n      \" 69: tts_models/bn/custom/vits-female\\\\n\",\\n      \" 70: tts_models/be/common-voice/glow-tts\\\\n\",\\n      \"\\\\n\",\\n      \" Name format: type/language/dataset/model\\\\n\",\\n      \" 1: vocoder_models/universal/libri-tts/wavegrad\\\\n\",\\n      \" 2: vocoder_models/universal/libri-tts/fullband-melgan [already downloaded]\\\\n\",\\n      \" 3: vocoder_models/en/ek1/wavegrad\\\\n\",\\n      \" 4: vocoder_models/en/ljspeech/multiband-melgan\\\\n\",\\n      \" 5: vocoder_models/en/ljspeech/hifigan_v2\\\\n\",\\n      \" 6: vocoder_models/en/ljspeech/univnet\\\\n\",\\n      \" 7: vocoder_models/en/blizzard2013/hifigan_v2\\\\n\",\\n      \" 8: vocoder_models/en/vctk/hifigan_v2\\\\n\",\\n      \" 9: vocoder_models/en/sam/hifigan_v2\\\\n\",\\n      \" 10: vocoder_models/nl/mai/parallel-wavegan\\\\n\",\\n      \" 11: vocoder_models/de/thorsten/wavegrad\\\\n\",\\n      \" 12: vocoder_models/de/thorsten/fullband-melgan\\\\n\",\\n      \" 13: vocoder_models/de/thorsten/hifigan_v1\\\\n\",\\n      \" 14: vocoder_models/ja/kokoro/hifigan_v1\\\\n\",\\n      \" 15: vocoder_models/uk/mai/multiband-melgan\\\\n\",\\n      \" 16: vocoder_models/tr/common-voice/hifigan\\\\n\",\\n      \" 17: vocoder_models/be/common-voice/hifigan\\\\n\",\\n      \"\\\\n\",\\n      \" Name format: type/language/dataset/model\\\\n\",\\n      \" 1: voice_conversion_models/multilingual/vctk/freevc24\\\\n\",\\n      \"tts_models/es/mai/tacotron2-DDC\\\\n\",\\n      \"tts_models/es/css10/vits\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"from TTS.utils.manage import ModelManager\\\\n\",\\n    \"manager = ModelManager()\\\\n\",\\n    \"print(\\\\\"Available Spanish models:\\\\\")\\\\n\",\\n    \"for model in manager.list_models():\\\\n\",\\n    \"    if \\\\\"es\\\\\" in model or \\\\\"spanish\\\\\" in model.lower():\\\\n\",\\n    \"        print(model)\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 13,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stderr\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"ERROR: Could not find a version that satisfies the requirement time (from versions: none)\\\\n\",\\n      \"ERROR: No matching distribution found for time\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"!pip install time\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 14,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"import time\\\\n\",\\n    \"\\\\n\",\\n    \"def run_conversation():\\\\n\",\\n    \"    print(\\\\\"Starting conversation\\\\\")\\\\n\",\\n    \"    \\\\n\",\\n    \"    while True:\\\\n\",\\n    \"        try:\\\\n\",\\n    \"            \\\\n\",\\n    \"            \\\\n\",\\n    \"            #record user\\\\n\",\\n    \"            print(\\\\\"Recording now\\\\\")\\\\n\",\\n    \"            try:\\\\n\",\\n    \"                audio = record_audio()\\\\n\",\\n    \"            except Exception as e:\\\\n\",\\n    \"                print(f\\\\\"Error recording audio: {str(e)}\\\\\")\\\\n\",\\n    \"                print(\\\\\"Please try again.\\\\\")\\\\n\",\\n    \"                continue\\\\n\",\\n    \"            \\\\n\",\\n    \"            #transcribe audio\\\\n\",\\n    \"            try:\\\\n\",\\n    \"                transcription = transcribe_audio(audio, model=og_model)\\\\n\",\\n    \"                if not transcription.strip():\\\\n\",\\n    \"                    print(\\'No speech detected. Please try again\\')\\\\n\",\\n    \"                    continue\\\\n\",\\n    \"                print(f\\'\\\\\\\\nDid you say: {transcription}?\\')\\\\n\",\\n    \"            except Exception as e:\\\\n\",\\n    \"                print(f\\\\\"Error recording audio: {str(e)}\\\\\")\\\\n\",\\n    \"                print(\\\\\"Please try again.\\\\\")\\\\n\",\\n    \"                continue\\\\n\",\\n    \"            \\\\n\",\\n    \"            #if exit conversation\\\\n\",\\n    \"            if transcription.lower() in [\\'adiÃ³s\\', \\'adios\\', \\'chao\\', \\'bye\\', \\'goodbye\\', \\'done\\', \\'stop\\']:\\\\n\",\\n    \"                print(\\\\\"\\\\\\\\nEnding conversation. Chao!\\\\\")\\\\n\",\\n    \"                break\\\\n\",\\n    \"            \\\\n\",\\n    \"            #generate response\\\\n\",\\n    \"            try:\\\\n\",\\n    \"                response = generate_response(transcription)\\\\n\",\\n    \"            except Exception as e:\\\\n\",\\n    \"                    print(f\\\\\"Error generating response: {str(e)}\\\\\")\\\\n\",\\n    \"                    print(\\\\\"Please try again.\\\\\")\\\\n\",\\n    \"                    continue\\\\n\",\\n    \"                \\\\n\",\\n    \"            \\\\n\",\\n    \"            #convert response to speech output\\\\n\",\\n    \"            try:\\\\n\",\\n    \"                text_to_speech(response.text)\\\\n\",\\n    \"            except Exception as e:\\\\n\",\\n    \"                    print(f\\\\\"Error converting response to speech: {str(e)}\\\\\")\\\\n\",\\n    \"                    print(f\\\\\"Continuing without speech output.\\\\\\\\nResponse: {response.text}\\\\\")\\\\n\",\\n    \"                    time.sleep(3)\\\\n\",\\n    \"                    continue\\\\n\",\\n    \"                \\\\n\",\\n    \"                \\\\n\",\\n    \"        except KeyboardInterrupt:\\\\n\",\\n    \"            print(\\\\\"\\\\\\\\n\\\\\\\\nConversation interrupted by user. Â¡Hasta luego!\\\\\")\\\\n\",\\n    \"            break\\\\n\",\\n    \"        except Exception as e:\\\\n\",\\n    \"            print(f\\\\\"\\\\\\\\nUnexpected error: {str(e)}\\\\\")\\\\n\",\\n    \"            print(\\\\\"Please try again.\\\\\")\\\\n\",\\n    \"            continue\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"run_conversation()\"\\n   ]\\n  }\\n ],\\n \"metadata\": {\\n  \"kernelspec\": {\\n   \"display_name\": \".venv\",\\n   \"language\": \"python\",\\n   \"name\": \"python3\"\\n  },\\n  \"language_info\": {\\n   \"codemirror_mode\": {\\n    \"name\": \"ipython\",\\n    \"version\": 3\\n   },\\n   \"file_extension\": \".py\",\\n   \"mimetype\": \"text/x-python\",\\n   \"name\": \"python\",\\n   \"nbconvert_exporter\": \"python\",\\n   \"pygments_lexer\": \"ipython3\",\\n   \"version\": \"3.10.0\"\\n  }\\n },\\n \"nbformat\": 4,\\n \"nbformat_minor\": 2\\n}\\n', 'side_proj/spanish_conversation_bot.ipynb': '{\\n \"cells\": [\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 73,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"import torch\\\\n\",\\n    \"import gc\\\\n\",\\n    \"import os\\\\n\",\\n    \"\\\\n\",\\n    \"# Clear GPU memory\\\\n\",\\n    \"def clear_gpu_memory():\\\\n\",\\n    \"    gc.collect()\\\\n\",\\n    \"    torch.cuda.empty_cache()\\\\n\",\\n    \"    torch.cuda.reset_peak_memory_stats()\\\\n\",\\n    \"    \\\\n\",\\n    \"# Call the function\\\\n\",\\n    \"clear_gpu_memory()\\\\n\",\\n    \"\\\\n\",\\n    \"# Set memory optimization environment variables\\\\n\",\\n    \"os.environ[\\\\\"PYTORCH_CUDA_ALLOC_CONF\\\\\"] = \\\\\"expandable_segments:True\\\\\"\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 74,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"Using GPU: NVIDIA GeForce RTX 3060 Laptop GPU\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"# Check for GPU and set device\\\\n\",\\n    \"import torch\\\\n\",\\n    \"if torch.cuda.is_available():\\\\n\",\\n    \"    device = \\'cuda\\'\\\\n\",\\n    \"    print(f\\'Using GPU: {torch.cuda.get_device_name(0)}\\')\\\\n\",\\n    \"else:\\\\n\",\\n    \"    device = \\'cpu\\'\\\\n\",\\n    \"    print(\\'GPU not available, using CPU.\\')\\\\n\",\\n    \"# Use the \\'device\\' variable when loading models.\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 75,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"   from dotenv import load_dotenv\\\\n\",\\n    \"   import os\\\\n\",\\n    \"\\\\n\",\\n    \"   load_dotenv(\\\\\"HF.config\\\\\")\\\\n\",\\n    \"   hf_token = os.getenv(\\\\\"HF_TOKEN\\\\\")\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 76,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stderr\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you\\'ve just configured.\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"from huggingface_hub import  login\\\\n\",\\n    \"\\\\n\",\\n    \"#authenticating HF login, as CV17 dataset requires it\\\\n\",\\n    \"login(token=hf_token)\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 77,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"\\\\n\",\\n      \"Latin American Spanish sample:\\\\n\",\\n      \"\\\\n\",\\n      \"Dataset sizes:\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"from datasets import load_dataset, concatenate_datasets, DatasetDict\\\\n\",\\n    \"\\\\n\",\\n    \"\\'\\'\\'\\\\n\",\\n    \"# top commented out as anything from now commented out was attempted with common voice, but pivoted to smaller dataset\\\\n\",\\n    \"#columns_to_remove = [\\'client_id\\', \\'path\\', \\'up_votes\\', \\'down_votes\\', \\'age\\', \\'gender\\', \\'accent\\', \\'locale\\', \\'segment\\', \\'variant\\']\\\\n\",\\n    \"\\\\n\",\\n    \"#streaming so no fill download, spanish full dataset is 48 GB!\\\\n\",\\n    \"ds_train = load_dataset(\\\\n\",\\n    \"    \\\\\"mozilla-foundation/common_voice_6_0\\\\\",\\\\n\",\\n    \"    \\\\\"es\\\\\",\\\\n\",\\n    \"    split=\\\\\"train\\\\\",\\\\n\",\\n    \"    streaming=True,\\\\n\",\\n    \"    trust_remote_code=True\\\\n\",\\n    \").remove_columns(columns_to_remove)\\\\n\",\\n    \"\\\\n\",\\n    \"ds_test = load_dataset(\\\\n\",\\n    \"    \\\\\"mozilla-foundation/common_voice_6_0\\\\\",\\\\n\",\\n    \"    \\\\\"es\\\\\",\\\\n\",\\n    \"    split=\\\\\"test\\\\\",\\\\n\",\\n    \"    streaming=True,\\\\n\",\\n    \"    trust_remote_code=True\\\\n\",\\n    \").remove_columns(columns_to_remove)\\\\n\",\\n    \"\\\\n\",\\n    \"\\'\\'\\'\\\\n\",\\n    \"ds_fleur = DatasetDict()\\\\n\",\\n    \"columns_to_keep = [\\'audio\\', \\'transcription\\']  # These are the essential columns for speech recognition\\\\n\",\\n    \"\\\\n\",\\n    \"# Load Latin American Spanish data. If need European spanish in future, voxpopuli has almost all europ samples\\\\n\",\\n    \"ds_fleur[\\\\\"train\\\\\"] = load_dataset(\\\\n\",\\n    \"    \\\\\"google/fleurs\\\\\",\\\\n\",\\n    \"    \\\\\"es_419\\\\\",\\\\n\",\\n    \"    split=\\\\\"train\\\\\",\\\\n\",\\n    \"    trust_remote_code=True\\\\n\",\\n    \").select_columns(columns_to_keep)\\\\n\",\\n    \"\\\\n\",\\n    \"ds_fleur[\\\\\"test\\\\\"] = load_dataset(\\\\n\",\\n    \"    \\\\\"google/fleurs\\\\\",\\\\n\",\\n    \"    \\\\\"es_419\\\\\",\\\\n\",\\n    \"    split=\\\\\"test\\\\\",\\\\n\",\\n    \"    trust_remote_code=True\\\\n\",\\n    \").select_columns(columns_to_keep)\\\\n\",\\n    \"\\\\n\",\\n    \"print(\\\\\"\\\\\\\\nLatin American Spanish sample:\\\\\")\\\\n\",\\n    \"#print(ds_fleur[\\\\\"train\\\\\"][0])\\\\n\",\\n    \"\\\\n\",\\n    \"print(\\\\\"\\\\\\\\nDataset sizes:\\\\\")\\\\n\",\\n    \"#print(f\\\\\"Latin American Spanish train set: {len(list(ds_fleur[\\\\\"train\\\\\"]))} samples\\\\\")\\\\n\",\\n    \"#print(f\\\\\"Latin American Spanish test set: {len(list(ds_fleur[\\\\\"test\\\\\"]))} samples\\\\\")\\\\n\",\\n    \"\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 78,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from transformers import WhisperFeatureExtractor\\\\n\",\\n    \"\\\\n\",\\n    \"#load feature extractor from pre-trained checkpoint\\\\n\",\\n    \"feature_extractor = WhisperFeatureExtractor.from_pretrained(\\\\\"openai/whisper-small\\\\\")\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 79,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from transformers import WhisperTokenizer\\\\n\",\\n    \"\\\\n\",\\n    \"#load tokenizer, has very extensive byte-pair training\\\\n\",\\n    \"tokenizer = WhisperTokenizer.from_pretrained(\\\\\"openai/whisper-small\\\\\", language=\\\\\"Spanish\\\\\", task=\\\\\"transcribe\\\\\")\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 80,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"Input:                 los murales o garabatos indeseados reciben el nombre de grafiti\\\\n\",\\n      \"Decoded w/ special:    <|startoftranscript|><|es|><|transcribe|><|notimestamps|>los murales o garabatos indeseados reciben el nombre de grafiti<|endoftext|>\\\\n\",\\n      \"Decoded w/out special: los murales o garabatos indeseados reciben el nombre de grafiti\\\\n\",\\n      \"Are equal:             True\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"input_str = ds_fleur[\\\\\"train\\\\\"][0][\\\\\"transcription\\\\\"]\\\\n\",\\n    \"labels = tokenizer(input_str).input_ids\\\\n\",\\n    \"decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\\\\n\",\\n    \"decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\\\\n\",\\n    \"\\\\n\",\\n    \"#verifying tokenizer works\\\\n\",\\n    \"\\\\n\",\\n    \"print(f\\\\\"Input:                 {input_str}\\\\\")\\\\n\",\\n    \"print(f\\\\\"Decoded w/ special:    {decoded_with_special}\\\\\")\\\\n\",\\n    \"print(f\\\\\"Decoded w/out special: {decoded_str}\\\\\")\\\\n\",\\n    \"print(f\\\\\"Are equal:             {input_str == decoded_str}\\\\\")\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 81,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from transformers import WhisperProcessor\\\\n\",\\n    \"\\\\n\",\\n    \"processor = WhisperProcessor.from_pretrained(\\\\\"openai/whisper-small\\\\\", language=\\\\\"spanish\\\\\", task=\\\\\"transcribe\\\\\")\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 82,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"{\\'audio\\': {\\'path\\': \\'train/10005668950815513748.wav\\', \\'array\\': array([0., 0., 0., ..., 0., 0., 0.]), \\'sampling_rate\\': 16000}, \\'transcription\\': \\'los murales o garabatos indeseados reciben el nombre de grafiti\\'}\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"print(ds_fleur[\\\\\"train\\\\\"][0])\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 83,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from datasets import Audio\\\\n\",\\n    \"\\\\n\",\\n    \"#downsample audio to 16kHz to match that of Whisper\\'s sampling rate\\\\n\",\\n    \"ds_fleur = ds_fleur.cast_column(\\\\\"audio\\\\\", Audio(sampling_rate=16000))\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 84,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"{\\'audio\\': {\\'path\\': \\'train/10005668950815513748.wav\\', \\'array\\': array([0., 0., 0., ..., 0., 0., 0.]), \\'sampling_rate\\': 16000}, \\'transcription\\': \\'los murales o garabatos indeseados reciben el nombre de grafiti\\'}\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"print(ds_fleur[\\\\\"train\\\\\"][0])\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 85,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"def prepare_dataset(batch, feature_extractor=feature_extractor, tokenizer=tokenizer):   \\\\n\",\\n    \"# resample audio to 16kHz\\\\n\",\\n    \"    audio = batch[\\\\\"audio\\\\\"]\\\\n\",\\n    \"    \\\\n\",\\n    \"    # compute log-mel input feats from arrau\\\\n\",\\n    \"    inputs = feature_extractor(\\\\n\",\\n    \"        audio[\\\\\"array\\\\\"], \\\\n\",\\n    \"        sampling_rate=audio[\\\\\"sampling_rate\\\\\"]).input_features[0]\\\\n\",\\n    \"\\\\n\",\\n    \"    batch[\\\\\"input_features\\\\\"] = inputs.input_features[0]\\\\n\",\\n    \"    batch[\\\\\"attention_mask\\\\\"] = inputs.attention_mask[0]\\\\n\",\\n    \"    # encode target text to label ids\\\\n\",\\n    \"    batch[\\\\\"labels\\\\\"] = tokenizer(batch[\\\\\"transcription\\\\\"]).input_ids\\\\n\",\\n    \"    return batch\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 86,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"data\": {\\n      \"application/vnd.jupyter.widget-view+json\": {\\n       \"model_id\": \"7f511274f955434ba7da84a90501aeb0\",\\n       \"version_major\": 2,\\n       \"version_minor\": 0\\n      },\\n      \"text/plain\": [\\n       \"Map (num_proc=4):   0%|          | 0/2796 [00:00<?, ? examples/s]\"\\n      ]\\n     },\\n     \"metadata\": {},\\n     \"output_type\": \"display_data\"\\n    },\\n    {\\n     \"ename\": \"AttributeError\",\\n     \"evalue\": \"\\'numpy.ndarray\\' object has no attribute \\'input_features\\'\",\\n     \"output_type\": \"error\",\\n     \"traceback\": [\\n      \"\\\\u001b[1;31m---------------------------------------------------------------------------\\\\u001b[0m\",\\n      \"\\\\u001b[1;31mRemoteTraceback\\\\u001b[0m                           Traceback (most recent call last)\",\\n      \"\\\\u001b[1;31mRemoteTraceback\\\\u001b[0m: \\\\n\\\\\"\\\\\"\\\\\"\\\\nTraceback (most recent call last):\\\\n  File \\\\\"C:\\\\\\\\Users\\\\\\\\marte\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\gpu\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\multiprocess\\\\\\\\pool.py\\\\\", line 125, in worker\\\\n    result = (True, func(*args, **kwds))\\\\n  File \\\\\"C:\\\\\\\\Users\\\\\\\\marte\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\gpu\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\datasets\\\\\\\\utils\\\\\\\\py_utils.py\\\\\", line 688, in _write_generator_to_queue\\\\n    for i, result in enumerate(func(**kwargs)):\\\\n  File \\\\\"C:\\\\\\\\Users\\\\\\\\marte\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\gpu\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\datasets\\\\\\\\arrow_dataset.py\\\\\", line 3501, in _map_single\\\\n    for i, example in iter_outputs(shard_iterable):\\\\n  File \\\\\"C:\\\\\\\\Users\\\\\\\\marte\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\gpu\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\datasets\\\\\\\\arrow_dataset.py\\\\\", line 3475, in iter_outputs\\\\n    yield i, apply_function(example, i, offset=offset)\\\\n  File \\\\\"C:\\\\\\\\Users\\\\\\\\marte\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\gpu\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\datasets\\\\\\\\arrow_dataset.py\\\\\", line 3398, in apply_function\\\\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\\\\n  File \\\\\"C:\\\\\\\\Users\\\\\\\\marte\\\\\\\\AppData\\\\\\\\Local\\\\\\\\Temp\\\\\\\\ipykernel_31064\\\\\\\\2898349104.py\\\\\", line 10, in prepare_dataset\\\\nAttributeError: \\'numpy.ndarray\\' object has no attribute \\'input_features\\'\\\\n\\\\\"\\\\\"\\\\\"\",\\n      \"\\\\nThe above exception was the direct cause of the following exception:\\\\n\",\\n      \"\\\\u001b[1;31mAttributeError\\\\u001b[0m                            Traceback (most recent call last)\",\\n      \"Cell \\\\u001b[1;32mIn[86], line 1\\\\u001b[0m\\\\n\\\\u001b[1;32m----> 1\\\\u001b[0m ds_fleur \\\\u001b[38;5;241m=\\\\u001b[39m \\\\u001b[43mds_fleur\\\\u001b[49m\\\\u001b[38;5;241;43m.\\\\u001b[39;49m\\\\u001b[43mmap\\\\u001b[49m\\\\u001b[43m(\\\\u001b[49m\\\\n\\\\u001b[0;32m      2\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mprepare_dataset\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m      3\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mremove_columns\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mds_fleur\\\\u001b[49m\\\\u001b[38;5;241;43m.\\\\u001b[39;49m\\\\u001b[43mcolumn_names\\\\u001b[49m\\\\u001b[43m[\\\\u001b[49m\\\\u001b[38;5;124;43m\\\\\"\\\\u001b[39;49m\\\\u001b[38;5;124;43mtrain\\\\u001b[39;49m\\\\u001b[38;5;124;43m\\\\\"\\\\u001b[39;49m\\\\u001b[43m]\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m      4\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mnum_proc\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[38;5;241;43m4\\\\u001b[39;49m\\\\u001b[43m,\\\\u001b[49m\\\\u001b[43m  \\\\u001b[49m\\\\u001b[38;5;66;43;03m# Process 16 samples at a time \\\\u001b[39;49;00m\\\\n\\\\u001b[0;32m      5\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mfn_kwargs\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43m{\\\\u001b[49m\\\\u001b[38;5;124;43m\\\\\"\\\\u001b[39;49m\\\\u001b[38;5;124;43mfeature_extractor\\\\u001b[39;49m\\\\u001b[38;5;124;43m\\\\\"\\\\u001b[39;49m\\\\u001b[43m:\\\\u001b[49m\\\\u001b[43m \\\\u001b[49m\\\\u001b[43mfeature_extractor\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\u001b[43m \\\\u001b[49m\\\\u001b[38;5;124;43m\\\\\"\\\\u001b[39;49m\\\\u001b[38;5;124;43mtokenizer\\\\u001b[39;49m\\\\u001b[38;5;124;43m\\\\\"\\\\u001b[39;49m\\\\u001b[43m:\\\\u001b[49m\\\\u001b[43m \\\\u001b[49m\\\\u001b[43mtokenizer\\\\u001b[49m\\\\u001b[43m}\\\\u001b[49m\\\\n\\\\u001b[0;32m      6\\\\u001b[0m \\\\u001b[43m)\\\\u001b[49m\\\\n\",\\n      \"File \\\\u001b[1;32m~\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\gpu\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\datasets\\\\\\\\dataset_dict.py:944\\\\u001b[0m, in \\\\u001b[0;36mDatasetDict.map\\\\u001b[1;34m(self, function, with_indices, with_rank, with_split, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc, try_original_type)\\\\u001b[0m\\\\n\\\\u001b[0;32m    941\\\\u001b[0m \\\\u001b[38;5;28;01mif\\\\u001b[39;00m with_split:\\\\n\\\\u001b[0;32m    942\\\\u001b[0m     function \\\\u001b[38;5;241m=\\\\u001b[39m bind(function, split)\\\\n\\\\u001b[1;32m--> 944\\\\u001b[0m dataset_dict[split] \\\\u001b[38;5;241m=\\\\u001b[39m \\\\u001b[43mdataset\\\\u001b[49m\\\\u001b[38;5;241;43m.\\\\u001b[39;49m\\\\u001b[43mmap\\\\u001b[49m\\\\u001b[43m(\\\\u001b[49m\\\\n\\\\u001b[0;32m    945\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mfunction\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mfunction\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    946\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mwith_indices\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mwith_indices\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    947\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mwith_rank\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mwith_rank\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    948\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43minput_columns\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43minput_columns\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    949\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mbatched\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mbatched\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    950\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mbatch_size\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mbatch_size\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    951\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mdrop_last_batch\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mdrop_last_batch\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    952\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mremove_columns\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mremove_columns\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    953\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mkeep_in_memory\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mkeep_in_memory\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    954\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mload_from_cache_file\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mload_from_cache_file\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    955\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mcache_file_name\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mcache_file_names\\\\u001b[49m\\\\u001b[43m[\\\\u001b[49m\\\\u001b[43msplit\\\\u001b[49m\\\\u001b[43m]\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    956\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mwriter_batch_size\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mwriter_batch_size\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    957\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mfeatures\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mfeatures\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    958\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mdisable_nullable\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mdisable_nullable\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    959\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mfn_kwargs\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mfn_kwargs\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    960\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mnum_proc\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mnum_proc\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    961\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mdesc\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mdesc\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    962\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mtry_original_type\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mtry_original_type\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    963\\\\u001b[0m \\\\u001b[43m\\\\u001b[49m\\\\u001b[43m)\\\\u001b[49m\\\\n\\\\u001b[0;32m    965\\\\u001b[0m \\\\u001b[38;5;28;01mif\\\\u001b[39;00m with_split:\\\\n\\\\u001b[0;32m    966\\\\u001b[0m     function \\\\u001b[38;5;241m=\\\\u001b[39m function\\\\u001b[38;5;241m.\\\\u001b[39mfunc\\\\n\",\\n      \"File \\\\u001b[1;32m~\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\gpu\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\datasets\\\\\\\\arrow_dataset.py:557\\\\u001b[0m, in \\\\u001b[0;36mtransmit_format.<locals>.wrapper\\\\u001b[1;34m(*args, **kwargs)\\\\u001b[0m\\\\n\\\\u001b[0;32m    550\\\\u001b[0m self_format \\\\u001b[38;5;241m=\\\\u001b[39m {\\\\n\\\\u001b[0;32m    551\\\\u001b[0m     \\\\u001b[38;5;124m\\\\\"\\\\u001b[39m\\\\u001b[38;5;124mtype\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m: \\\\u001b[38;5;28mself\\\\u001b[39m\\\\u001b[38;5;241m.\\\\u001b[39m_format_type,\\\\n\\\\u001b[0;32m    552\\\\u001b[0m     \\\\u001b[38;5;124m\\\\\"\\\\u001b[39m\\\\u001b[38;5;124mformat_kwargs\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m: \\\\u001b[38;5;28mself\\\\u001b[39m\\\\u001b[38;5;241m.\\\\u001b[39m_format_kwargs,\\\\n\\\\u001b[0;32m    553\\\\u001b[0m     \\\\u001b[38;5;124m\\\\\"\\\\u001b[39m\\\\u001b[38;5;124mcolumns\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m: \\\\u001b[38;5;28mself\\\\u001b[39m\\\\u001b[38;5;241m.\\\\u001b[39m_format_columns,\\\\n\\\\u001b[0;32m    554\\\\u001b[0m     \\\\u001b[38;5;124m\\\\\"\\\\u001b[39m\\\\u001b[38;5;124moutput_all_columns\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m: \\\\u001b[38;5;28mself\\\\u001b[39m\\\\u001b[38;5;241m.\\\\u001b[39m_output_all_columns,\\\\n\\\\u001b[0;32m    555\\\\u001b[0m }\\\\n\\\\u001b[0;32m    556\\\\u001b[0m \\\\u001b[38;5;66;03m# apply actual function\\\\u001b[39;00m\\\\n\\\\u001b[1;32m--> 557\\\\u001b[0m out: Union[\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m\\\\u001b[38;5;124mDataset\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m, \\\\u001b[38;5;124m\\\\\"\\\\u001b[39m\\\\u001b[38;5;124mDatasetDict\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m] \\\\u001b[38;5;241m=\\\\u001b[39m func(\\\\u001b[38;5;28mself\\\\u001b[39m, \\\\u001b[38;5;241m*\\\\u001b[39margs, \\\\u001b[38;5;241m*\\\\u001b[39m\\\\u001b[38;5;241m*\\\\u001b[39mkwargs)\\\\n\\\\u001b[0;32m    558\\\\u001b[0m datasets: \\\\u001b[38;5;28mlist\\\\u001b[39m[\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m\\\\u001b[38;5;124mDataset\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m] \\\\u001b[38;5;241m=\\\\u001b[39m \\\\u001b[38;5;28mlist\\\\u001b[39m(out\\\\u001b[38;5;241m.\\\\u001b[39mvalues()) \\\\u001b[38;5;28;01mif\\\\u001b[39;00m \\\\u001b[38;5;28misinstance\\\\u001b[39m(out, \\\\u001b[38;5;28mdict\\\\u001b[39m) \\\\u001b[38;5;28;01melse\\\\u001b[39;00m [out]\\\\n\\\\u001b[0;32m    559\\\\u001b[0m \\\\u001b[38;5;66;03m# re-apply format to the output\\\\u001b[39;00m\\\\n\",\\n      \"File \\\\u001b[1;32m~\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\gpu\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\datasets\\\\\\\\arrow_dataset.py:3171\\\\u001b[0m, in \\\\u001b[0;36mDataset.map\\\\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\\\\u001b[0m\\\\n\\\\u001b[0;32m   3165\\\\u001b[0m logger\\\\u001b[38;5;241m.\\\\u001b[39minfo(\\\\u001b[38;5;124mf\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m\\\\u001b[38;5;124mSpawning \\\\u001b[39m\\\\u001b[38;5;132;01m{\\\\u001b[39;00mnum_proc\\\\u001b[38;5;132;01m}\\\\u001b[39;00m\\\\u001b[38;5;124m processes\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m)\\\\n\\\\u001b[0;32m   3166\\\\u001b[0m \\\\u001b[38;5;28;01mwith\\\\u001b[39;00m hf_tqdm(\\\\n\\\\u001b[0;32m   3167\\\\u001b[0m     unit\\\\u001b[38;5;241m=\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m\\\\u001b[38;5;124m examples\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m,\\\\n\\\\u001b[0;32m   3168\\\\u001b[0m     total\\\\u001b[38;5;241m=\\\\u001b[39mpbar_total,\\\\n\\\\u001b[0;32m   3169\\\\u001b[0m     desc\\\\u001b[38;5;241m=\\\\u001b[39m(desc \\\\u001b[38;5;129;01mor\\\\u001b[39;00m \\\\u001b[38;5;124m\\\\\"\\\\u001b[39m\\\\u001b[38;5;124mMap\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m) \\\\u001b[38;5;241m+\\\\u001b[39m \\\\u001b[38;5;124mf\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m\\\\u001b[38;5;124m (num_proc=\\\\u001b[39m\\\\u001b[38;5;132;01m{\\\\u001b[39;00mnum_proc\\\\u001b[38;5;132;01m}\\\\u001b[39;00m\\\\u001b[38;5;124m)\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m,\\\\n\\\\u001b[0;32m   3170\\\\u001b[0m ) \\\\u001b[38;5;28;01mas\\\\u001b[39;00m pbar:\\\\n\\\\u001b[1;32m-> 3171\\\\u001b[0m     \\\\u001b[38;5;28;01mfor\\\\u001b[39;00m rank, done, content \\\\u001b[38;5;129;01min\\\\u001b[39;00m iflatmap_unordered(\\\\n\\\\u001b[0;32m   3172\\\\u001b[0m         pool, Dataset\\\\u001b[38;5;241m.\\\\u001b[39m_map_single, kwargs_iterable\\\\u001b[38;5;241m=\\\\u001b[39mkwargs_per_job\\\\n\\\\u001b[0;32m   3173\\\\u001b[0m     ):\\\\n\\\\u001b[0;32m   3174\\\\u001b[0m         \\\\u001b[38;5;28;01mif\\\\u001b[39;00m done:\\\\n\\\\u001b[0;32m   3175\\\\u001b[0m             shards_done \\\\u001b[38;5;241m+\\\\u001b[39m\\\\u001b[38;5;241m=\\\\u001b[39m \\\\u001b[38;5;241m1\\\\u001b[39m\\\\n\",\\n      \"File \\\\u001b[1;32m~\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\gpu\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\datasets\\\\\\\\utils\\\\\\\\py_utils.py:728\\\\u001b[0m, in \\\\u001b[0;36miflatmap_unordered\\\\u001b[1;34m(pool, func, kwargs_iterable)\\\\u001b[0m\\\\n\\\\u001b[0;32m    725\\\\u001b[0m \\\\u001b[38;5;28;01mfinally\\\\u001b[39;00m:\\\\n\\\\u001b[0;32m    726\\\\u001b[0m     \\\\u001b[38;5;28;01mif\\\\u001b[39;00m \\\\u001b[38;5;129;01mnot\\\\u001b[39;00m pool_changed:\\\\n\\\\u001b[0;32m    727\\\\u001b[0m         \\\\u001b[38;5;66;03m# we get the result in case there\\'s an error to raise\\\\u001b[39;00m\\\\n\\\\u001b[1;32m--> 728\\\\u001b[0m         [async_result\\\\u001b[38;5;241m.\\\\u001b[39mget(timeout\\\\u001b[38;5;241m=\\\\u001b[39m\\\\u001b[38;5;241m0.05\\\\u001b[39m) \\\\u001b[38;5;28;01mfor\\\\u001b[39;00m async_result \\\\u001b[38;5;129;01min\\\\u001b[39;00m async_results]\\\\n\",\\n      \"File \\\\u001b[1;32m~\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\gpu\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\datasets\\\\\\\\utils\\\\\\\\py_utils.py:728\\\\u001b[0m, in \\\\u001b[0;36m<listcomp>\\\\u001b[1;34m(.0)\\\\u001b[0m\\\\n\\\\u001b[0;32m    725\\\\u001b[0m \\\\u001b[38;5;28;01mfinally\\\\u001b[39;00m:\\\\n\\\\u001b[0;32m    726\\\\u001b[0m     \\\\u001b[38;5;28;01mif\\\\u001b[39;00m \\\\u001b[38;5;129;01mnot\\\\u001b[39;00m pool_changed:\\\\n\\\\u001b[0;32m    727\\\\u001b[0m         \\\\u001b[38;5;66;03m# we get the result in case there\\'s an error to raise\\\\u001b[39;00m\\\\n\\\\u001b[1;32m--> 728\\\\u001b[0m         [\\\\u001b[43masync_result\\\\u001b[49m\\\\u001b[38;5;241;43m.\\\\u001b[39;49m\\\\u001b[43mget\\\\u001b[49m\\\\u001b[43m(\\\\u001b[49m\\\\u001b[43mtimeout\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[38;5;241;43m0.05\\\\u001b[39;49m\\\\u001b[43m)\\\\u001b[49m \\\\u001b[38;5;28;01mfor\\\\u001b[39;00m async_result \\\\u001b[38;5;129;01min\\\\u001b[39;00m async_results]\\\\n\",\\n      \"File \\\\u001b[1;32m~\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\gpu\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\multiprocess\\\\\\\\pool.py:771\\\\u001b[0m, in \\\\u001b[0;36mApplyResult.get\\\\u001b[1;34m(self, timeout)\\\\u001b[0m\\\\n\\\\u001b[0;32m    769\\\\u001b[0m     \\\\u001b[38;5;28;01mreturn\\\\u001b[39;00m \\\\u001b[38;5;28mself\\\\u001b[39m\\\\u001b[38;5;241m.\\\\u001b[39m_value\\\\n\\\\u001b[0;32m    770\\\\u001b[0m \\\\u001b[38;5;28;01melse\\\\u001b[39;00m:\\\\n\\\\u001b[1;32m--> 771\\\\u001b[0m     \\\\u001b[38;5;28;01mraise\\\\u001b[39;00m \\\\u001b[38;5;28mself\\\\u001b[39m\\\\u001b[38;5;241m.\\\\u001b[39m_value\\\\n\",\\n      \"\\\\u001b[1;31mAttributeError\\\\u001b[0m: \\'numpy.ndarray\\' object has no attribute \\'input_features\\'\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"ds_fleur = ds_fleur.map(\\\\n\",\\n    \"    prepare_dataset,\\\\n\",\\n    \"    remove_columns=ds_fleur.column_names[\\\\\"train\\\\\"],\\\\n\",\\n    \"    num_proc=4,  # Process 16 samples at a time \\\\n\",\\n    \"    fn_kwargs={\\\\\"feature_extractor\\\\\": feature_extractor, \\\\\"tokenizer\\\\\": tokenizer}\\\\n\",\\n    \")\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from transformers import WhisperForConditionalGeneration\\\\n\",\\n    \"\\\\n\",\\n    \"# gen text conditioned on an input(audio data)\\\\n\",\\n    \"model = WhisperForConditionalGeneration.from_pretrained(\\\\\"openai/whisper-small\\\\\")\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"model.generation_config.language = \\\\\"spanish\\\\\"\\\\n\",\\n    \"model.generation_config.task = \\\\\"transcribe\\\\\"\\\\n\",\\n    \"\\\\n\",\\n    \"model.config.pad_token_id = model.config.eos_token_id\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"import torch\\\\n\",\\n    \"\\\\n\",\\n    \"from dataclasses import dataclass\\\\n\",\\n    \"from typing import Any, Dict, List, Union\\\\n\",\\n    \"\\\\n\",\\n    \"@dataclass \\\\n\",\\n    \"class DataCollatorSpeechSeq2SeqWithPadding:\\\\n\",\\n    \"    processor: Any\\\\n\",\\n    \"    decoder_start_token_id: int\\\\n\",\\n    \"    # called when collator is used to batch samples together\\\\n\",\\n    \"    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\\\\n\",\\n    \"        \\\\n\",\\n    \"        # Extract audio features from each sample and pad them to same length\\\\n\",\\n    \"        input_features = [{\\\\\"input_features\\\\\": feature[\\\\\"input_features\\\\\"]} for feature in features]\\\\n\",\\n    \"        #uses processor  built earlier that can call extractor and tokenizer\\\\n\",\\n    \"        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\\\\\"pt\\\\\")\\\\n\",\\n    \"\\\\n\",\\n    \"        # Extract text labels from each sample and pad them to same length\\\\n\",\\n    \"        label_features = [{\\\\\"input_ids\\\\\": feature[\\\\\"labels\\\\\"]} for feature in features]\\\\n\",\\n    \"        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\\\\\"pt\\\\\")\\\\n\",\\n    \"\\\\n\",\\n    \"        # Replace padding tokens with -100 (which is ignored in loss calculation)\\\\n\",\\n    \"        #when labels attention mask does NotEqual (ne) 1, then returns True. When this is true, meaning padding token,\\\\n\",\\n    \"        #then it will replace it with a -100\\\\n\",\\n    \"        labels = labels_batch[\\\\\"input_ids\\\\\"].masked_fill(labels_batch.attention_mask.ne(1), -100) \\\\n\",\\n    \"\\\\n\",\\n    \"        # Remove the start token if it was added during tokenization\\\\n\",\\n    \"        # (it will be added again during generation)\\\\n\",\\n    \"        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\\\\n\",\\n    \"            labels = labels[:, 1:]\\\\n\",\\n    \"\\\\n\",\\n    \"        batch[\\\\\"labels\\\\\"] = labels\\\\n\",\\n    \"\\\\n\",\\n    \"        #batch with audio feats and labels\\\\n\",\\n    \"        return batch\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"#initialize collator\\\\n\",\\n    \"data_collator = DataCollatorSpeechSeq2SeqWithPadding(\\\\n\",\\n    \"    processor=processor,\\\\n\",\\n    \"    decoder_start_token_id=model.config.decoder_start_token_id,\\\\n\",\\n    \")\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"import evaluate\\\\n\",\\n    \"\\\\n\",\\n    \"metric = evaluate.load(\\\\\"wer\\\\\")\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"import numpy as np\\\\n\",\\n    \"\\'\\'\\'\\\\n\",\\n    \"- replaces -100 with `pad_token_id` in `label_ids`\\\\n\",\\n    \"    - then decodes predicted and label ids to string\\\\n\",\\n    \"    - Computes WER\\\\n\",\\n    \"\\'\\'\\'\\\\n\",\\n    \"\\\\n\",\\n    \"def compute_metrics(pred):\\\\n\",\\n    \"    pred_ids = pred.predictions\\\\n\",\\n    \"    label_ids = np.array(pred.label_ids)\\\\n\",\\n    \"\\\\n\",\\n    \"    # replace -100 with the pad_token_id\\\\n\",\\n    \"    label_ids[label_ids == -100] = tokenizer.pad_token_id\\\\n\",\\n    \"\\\\n\",\\n    \"    # we do not want to group tokens when computing the metrics\\\\n\",\\n    \"    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\\\\n\",\\n    \"    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\\\\n\",\\n    \"\\\\n\",\\n    \"    #two different lines because compute returns dict[Unknown, Unknown] or None. So, first need to get wer val to * by 100\\\\n\",\\n    \"    #but before run won\\'t let multiply by None, so need to check if None or a real value\\\\n\",\\n    \"    wer = metric.compute(predictions=pred_str, references=label_str)\\\\n\",\\n    \"\\\\n\",\\n    \"    return {\\\\\"wer\\\\\": 100 * wer}\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from transformers.training_args_seq2seq import Seq2SeqTrainingArguments\\\\n\",\\n    \"\\\\n\",\\n    \"#define arguments\\\\n\",\\n    \"training_args = Seq2SeqTrainingArguments(\\\\n\",\\n    \"    output_dir=\\\\\"./whisper-small-es\\\\\",  # change to a repo name of your choice\\\\n\",\\n    \"    run_name=\\\\\"whisper-spanish-training\\\\\",\\\\n\",\\n    \"    per_device_train_batch_size=16,\\\\n\",\\n    \"    gradient_accumulation_steps=2,  # increase by 2x for every 2x decrease in batch size\\\\n\",\\n    \"    learning_rate=1e-5,\\\\n\",\\n    \"    warmup_steps=200,\\\\n\",\\n    \"    max_steps=2500,\\\\n\",\\n    \"    gradient_checkpointing=True,\\\\n\",\\n    \"    fp16=True,\\\\n\",\\n    \"    per_device_eval_batch_size=8,\\\\n\",\\n    \"    predict_with_generate=True,\\\\n\",\\n    \"    generation_max_length=225,\\\\n\",\\n    \"    save_steps=500,\\\\n\",\\n    \"    eval_steps=500,\\\\n\",\\n    \"    logging_steps=50,\\\\n\",\\n    \"    report_to=[\\\\\"tensorboard\\\\\",\\\\\"wandb\\\\\"],\\\\n\",\\n    \"    load_best_model_at_end=True,\\\\n\",\\n    \"    metric_for_best_model=\\\\\"wer\\\\\",\\\\n\",\\n    \"    greater_is_better=False,\\\\n\",\\n    \"    push_to_hub=True,\\\\n\",\\n    \"    eval_strategy=\\\\\"steps\\\\\",\\\\n\",\\n    \"    save_strategy=\\\\\"steps\\\\\"\\\\n\",\\n    \")\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"# Clear memory before training\\\\n\",\\n    \"clear_gpu_memory()\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"import torch\\\\n\",\\n    \"import gc\\\\n\",\\n    \"\\'\\'\\'\\\\n\",\\n    \"# Clear GPU memory\\\\n\",\\n    \"gc.collect()\\\\n\",\\n    \"torch.cuda.empty_cache()\\\\n\",\\n    \"torch.cuda.reset_peak_memory_stats()\\\\n\",\\n    \"\\'\\'\\'\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from transformers.trainer_seq2seq import Seq2SeqTrainer\\\\n\",\\n    \"from torchdata.stateful_dataloader import StatefulDataLoader\\\\n\",\\n    \"\\\\n\",\\n    \"trainer = Seq2SeqTrainer(\\\\n\",\\n    \"    args=training_args,\\\\n\",\\n    \"    model=model,\\\\n\",\\n    \"    train_dataset=ds_fleur[\\\\\"train\\\\\"],\\\\n\",\\n    \"    eval_dataset=ds_fleur[\\\\\"test\\\\\"],\\\\n\",\\n    \"    data_collator=data_collator,\\\\n\",\\n    \"    compute_metrics=compute_metrics\\\\n\",\\n    \")\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from dotenv import load_dotenv\\\\n\",\\n    \"import os\\\\n\",\\n    \"\\\\n\",\\n    \"load_dotenv(\\\\\"HF.config\\\\\")\\\\n\",\\n    \"wandb_key = os.getenv(\\\\\"WANDB_API_KEY\\\\\")\\\\n\",\\n    \"wandb_nb_name = os.getenv(\\\\\"WANDB_NOTEBOOK_NAME\\\\\")\\\\n\",\\n    \"\\\\n\",\\n    \"trainer.train()\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": []\\n  }\\n ],\\n \"metadata\": {\\n  \"kernelspec\": {\\n   \"display_name\": \"Python [conda env:gpu] *\",\\n   \"language\": \"python\",\\n   \"name\": \"conda-env-gpu-py\"\\n  },\\n  \"language_info\": {\\n   \"codemirror_mode\": {\\n    \"name\": \"ipython\",\\n    \"version\": 3\\n   },\\n   \"file_extension\": \".py\",\\n   \"mimetype\": \"text/x-python\",\\n   \"name\": \"python\",\\n   \"nbconvert_exporter\": \"python\",\\n   \"pygments_lexer\": \"ipython3\",\\n   \"version\": \"3.9.18\"\\n  }\\n },\\n \"nbformat\": 4,\\n \"nbformat_minor\": 4\\n}\\n', 'side_proj/src/spanish_chat_bot/audio/recorder.py': '\"\"\"\\nAudio recorder.\\n\"\"\"\\nimport sounddevice as sd\\nimport numpy as np\\nfrom scipy.signal import butter, filtfilt\\n\\n\\ndef record_audio(duration=5, sample_rate=16000, cutoff_freq=100, filter_order=4):\\n    \"\"\"\\n    Record audio for set duration and process it\\n    \\n    Args:\\n        duration (int): Amount of time recording user audio in seconds\\n        sample rate (int): sample rate to record audio in Hz\\n        cutoff_freq (int): Cutoff frequency for high-pass filter in Hz\\n        filter_order (int): Order of the Butterworth filter\\n    Returns:\\n        numpy.ndarray: Processed audio signal normalized to [-1, 1]\\n        \\n    \"\"\"\\n    try:\\n        myrecording = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1)\\n        sd.wait()\\n        \\n        #convert to single array, then normalize audio\\n        audio = myrecording.flatten()\\n        audio = audio / np.max(np.abs(audio))\\n        \\n        # apply filter to reduce noise\\n        nyquist = sample_rate / 2 #nyquist rate is 1/2 of the frequency\\n        \\n        #filter_order: higher order = sharper cutoff\\n        #cutoff_freq/nyquist normalizes the cutoff frequency\\n        #btype = high pass filter\\n        b, a = butter(filter_order, cutoff_freq/nyquist, btype=\\'high\\')\\n        audio = filtfilt(b, a, audio)\\n        \\n        #normalize again\\n        audio = audio / np.max(np.abs(audio))\\n        return audio\\n        \\n    except Exception as e:\\n        raise RuntimeError(f\"Failed to record audio: {str(e)}\")', 'side_proj/src/spanish_chat_bot/audio/tts.py': '\\'\\'\\' \\nConverts text response to audio\\n\\'\\'\\'\\nimport sounddevice as sd\\nfrom scipy.io import wavfile\\nfrom TTS.api import TTS\\nfrom sympy import false\\n\\n\\ndef text_to_speech(text, output_path=\"output.wav\"):\\n    \"\"\"\\n    Take text input and convert it to wav file\\n    \\n    Args:\\n        text (string): Generated text from response generator\\n        output_path (string): path for wav file output\\n    Returns:\\n        string: path to wav file of response\\n    \"\"\"\\n    \\n    try: \\n    # Initialize TTS with Spanish xtts model\\n        tts = TTS(model_name=\"tts_models/es/css10/vits\", progress_bar=False)\\n        \\n        # Generate speech\\n        tts.tts_to_file(text=text, file_path=output_path)\\n        \\n        # Play the audio\\n        sample_rate, audio = wavfile.read(output_path)\\n        sd.play(audio, sample_rate)\\n        sd.wait()\\n        \\n        return output_path\\n    \\n    except Exception as e:\\n        raise RuntimeError(f\"Failed to convert text to audio: {str(e)}\")\\n        ', 'side_proj/src/spanish_chat_bot/models/response_generator.py': '\\'\\'\\'\\nGenerate text response\\n\\'\\'\\'\\nfrom dotenv import load_dotenv\\nload_dotenv(\"tokens.env\")\\nimport os\\nfrom typing import Optional, List\\nimport google.generativeai as gem\\nfrom google.api_core import exceptions as google_exceptions\\n\\nclass ResponseGenerator:\\n    def __init__(self, model_name: str = \"gemini-2.0-flash\", api_key: Optional[str] = None):\\n        \\'\\'\\'\\n        Initialize generator with model.\\n        \\n        Args:\\n            model_name (string): name of generative model to use\\n            api_key (str, optional): API key for model use if necessary. If not provided, will look for in env\\n        \\'\\'\\'\\n        self.api_key = api_key or os.getenv(\"GEMINI_API_KEY\")\\n        if not self.api_key:\\n            raise ValueError(\"Gemini API key not found. Please provide it or set GEMINI_API_KEY environment variable.\")\\n            \\n        # Configure the API\\n        gem.configure(api_key=self.api_key)\\n        \\n        # Initialize the model\\n        self.model = gem.GenerativeModel(model_name)\\n        \\n        # Optional: Store conversation history\\n        self.conversation_history: List[dict] = []\\n\\n    def generate(self, text: str) -> str:\\n        \"\"\"\\n        Generate a response to the user\\'s speech\\n        \\n        Args:\\n            text (str): user\\'s speech input converted to text\\n        Returns:\\n            str: Generated text\\n        Raises:\\n            RuntimeError: If there\\'s an error generating the response\\n        \"\"\"\\n        try:\\n            response = self.model.generate_content(\\n                    f\"Eres un asistente conversacional en espaÃ±ol. Adaptarse al contexto de la conversaciÃ³n. \"\\n                    f\"(Por ejemplo: si la persona te diga algo como si fuera tu amigo o un familiar, respondele con ese rol.)\"\\n                    f\"Tu no eres un chatbot, eres la persona.\"\\n                    f\"No usa los emojis\"\\n                    f\"Mantenga la duraciÃ³n de tu respuesta corta. Responde de manera natural y conversacional a: {text}\"\\n                )\\n            \\n            if not response or not response.text:\\n                raise RuntimeError(\"Received empty response from model\")\\n                \\n            return response.text\\n            \\n        except google_exceptions.ResourceExhausted:\\n            raise RuntimeError(\"API rate limit exceeded. Please try again later.\")\\n        except google_exceptions.ServiceUnavailable:\\n            raise RuntimeError(\"Gemini API service is currently unavailable. Please try again later.\")\\n        except google_exceptions.InvalidArgument as e:\\n            raise RuntimeError(f\"Invalid input to model: {str(e)}\")\\n        except Exception as e:\\n            raise RuntimeError(f\"Failed to generate response: {str(e)}\")\\n\\n\\n# Create a singleton instance\\n_generator: Optional[ResponseGenerator] = None\\n\\ndef generate_response(text: str, model_name: str = \"gemini-2.0-flash\", api_key: Optional[str] = None) -> str:\\n    \"\"\"\\n    Global function to generate responses using the singleton generator.\\n    \\n    Args:\\n        text (str): Text to generate response for\\n        model_name (str): Name of the model to use\\n        api_key (str, optional): API key for the model\\n        \\n    Returns:\\n        str: Generated response\\n    \"\"\"\\n    global _generator\\n    if _generator is None:\\n        _generator = ResponseGenerator(model_name=model_name, api_key=api_key)\\n    return _generator.generate(text)', 'side_proj/src/spanish_chat_bot/models/transcriber.py': '\"\"\"\\nAudio transcription functionality using Whisper model.\\n\"\"\"\\n\\nimport torch\\nfrom transformers import (\\n    WhisperProcessor,\\n    WhisperForConditionalGeneration,\\n    WhisperFeatureExtractor,\\n    WhisperTokenizer\\n)\\nfrom typing import Optional, Union\\nimport numpy as np\\n\\n\\nclass Transcriber:\\n    def __init__(self, model_name: str = \"openai/whisper-small\", device: Optional[str] = None):\\n        \"\"\"\\n        Initialize the transcriber with Whisper model.\\n        \\n        Args:\\n            model_name (str): Name of the Whisper model to use\\n            device (str, optional): Device to run the model on (\\'cuda\\' or \\'cpu\\')\\n        \"\"\"\\n        # looks for cuda device to run model on\\n        if device is None:\\n            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\\n            \\n        self.device = device\\n        self.feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name)\\n        self.tokenizer = WhisperTokenizer.from_pretrained(model_name)\\n        self.model = WhisperForConditionalGeneration.from_pretrained(model_name).to(self.device)\\n        \\n        # Configure model for Spanish + transcription\\n        self.model.config.forced_decoder_ids = self.tokenizer.get_decoder_prompt_ids(\\n            language=\"spanish\",\\n            task=\"transcribe\"\\n        )\\n\\n    def transcribe(self, audio: np.ndarray, sample_rate: int = 16000) -> str:\\n        \"\"\"\\n        Transcribe audio to text.\\n        \\n        Args:\\n            audio (numpy.ndarray): Audio array\\n            sample_rate (int): Audio sample rate\\n            \\n        Returns:\\n            str: Transcribed text\\n        \"\"\"\\n        #process audio to tensors\\n        inputs = self.feature_extractor(\\n            audio,\\n            sampling_rate=sample_rate,\\n            return_tensors=\"pt\"\\n        )\\n        input_features = inputs.input_features.to(self.device)\\n        \\n        # Generate transcription\\n        predicted_ids = self.model.generate(\\n            input_features,\\n            max_length=448,\\n            num_beams=5, \\n            temperature=0.7\\n        )\\n        transcription = self.tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)[0]\\n        \\n        return transcription\\n\\n\\n# Create a singleton instance, more than one transcriber != good\\n_transcriber: Optional[Transcriber] = None\\n\\ndef transcribe_audio(audio: np.ndarray, sample_rate: int = 16000, model_name: str = \"openai/whisper-small\") -> str:\\n    \"\"\"\\n    Global function to transcribe audio using the singleton transcriber.\\n    \\n    Args:\\n        audio (numpy.ndarray): Audio array\\n        sample_rate (int): Audio sample rate\\n        model_name (str): Name of the Whisper model to use\\n        \\n    Returns:\\n        str: Transcribed text\\n    \"\"\"\\n    global _transcriber\\n    if _transcriber is None:\\n        _transcriber = Transcriber(model_name=model_name)\\n    return _transcriber.transcribe(audio, sample_rate) ', 'side_proj/frontend/package.json': '{\\n  \"name\": \"frontend\",\\n  \"private\": true,\\n  \"version\": \"0.0.0\",\\n  \"type\": \"module\",\\n  \"scripts\": {\\n    \"dev\": \"vite\",\\n    \"build\": \"vite build\",\\n    \"lint\": \"eslint .\",\\n    \"preview\": \"vite preview\"\\n  },\\n  \"dependencies\": {\\n    \"@heroicons/react\": \"^2.2.0\",\\n    \"axios\": \"^1.9.0\",\\n    \"react\": \"^19.1.0\",\\n    \"react-dom\": \"^19.1.0\"\\n  },\\n  \"devDependencies\": {\\n    \"@eslint/js\": \"^9.25.0\",\\n    \"@tailwindcss/postcss\": \"^4.1.8\",\\n    \"@types/react\": \"^19.1.2\",\\n    \"@types/react-dom\": \"^19.1.2\",\\n    \"@vitejs/plugin-react\": \"^4.4.1\",\\n    \"autoprefixer\": \"^10.4.21\",\\n    \"eslint\": \"^9.25.0\",\\n    \"eslint-plugin-react-hooks\": \"^5.2.0\",\\n    \"eslint-plugin-react-refresh\": \"^0.4.19\",\\n    \"globals\": \"^16.0.0\",\\n    \"postcss\": \"^8.5.4\",\\n    \"tailwindcss\": \"^4.1.8\",\\n    \"vite\": \"^6.3.5\"\\n  }\\n}\\n', 'side_proj/frontend/vite.config.js': \"import { defineConfig } from 'vite'\\nimport react from '@vitejs/plugin-react'\\n\\n// https://vite.dev/config/\\nexport default defineConfig({\\n  plugins: [react()],\\n  css: {\\n    postcss: './postcss.config.js',\\n  },\\n  server: {\\n    proxy: {\\n      '/api': 'http://localhost:8000'\\n    }\\n  }\\n})\\n\"}}\n",
      "Tool: Fetching content for 12 files...\n",
      "Tool: Finished fetching all file contents.\n",
      "  ðŸ“¦ Got Tool Response from 'fetch_all_content': {'status': 'success', 'files_fetched': 12}...\n",
      "  ðŸ’¾ State Change: {'all_file_contents': {'side_proj/requirements.txt': 'torch>=2.0.0\\ntransformers>=4.30.0\\nsounddevice>=0.4.6\\nnumpy>=1.24.0\\nscipy>=1.10.0\\npython-dotenv>=1.0.0\\ngoogle-generativeai>=0.3.0\\nTTS>=0.17.0\\ndatasets>=2.12.0\\nevaluate>=0.4.0\\nwandb>=0.15.0\\nfastapi>=0.104.0\\nuvicorn>=0.24.0 ', 'side_proj/src/spanish_chat_bot/conversation.py': '\"\"\"\\nMain conversation handler that orchestrates the Spanish chat bot.\\n\"\"\"\\nfrom typing import Optional\\nimport time\\nfrom .audio import record_audio, text_to_speech\\nfrom .models import transcribe_audio, generate_response\\n\\nclass Conversation:\\n    def __init__(self):\\n        \"\"\"Initialize the conversation handler.\"\"\"\\n        self.is_active = False\\n        self.conversation_history = []\\n        self.audio_files = []\\n\\n\\n    def process_audio(self, audio_data):\\n        text = transcribe_audio(audio_data)\\n        \\n        response = generate_response(text)\\n        \\n        audio_path = text_to_speech(response)\\n        \\n        #conversation history\\n        self.conversation_history.append({\\n            \\'user\\': text,\\n            \\'bot\\': response,\\n            \\'audio_path\\': audio_path,\\n            \\'timestamp\\': time.time()\\n        })\\n        \\n        return {\\n            \\'transcription\\': text,\\n            \\'response\\': response,\\n            \\'audio_path\\': audio_path\\n        }\\n    \\n        \\n        \\n    def start(self):\\n        \"\"\"Start the conversation loop.\"\"\"\\n        self.is_active = True\\n        print(\"Â¡Hola! Estoy listo para conversar. Presiona Ctrl+C para salir.\")\\n        \\n        while self.is_active:\\n            try:\\n                #record audio\\n                print(\"\\\\nEscuchando...\")\\n                audio = record_audio()\\n                \\n                #transcribe audio to text\\n                text = transcribe_audio(audio)\\n                #print(f\"Dijiste: {text}? Y/N\")\\n                if not text:\\n                    print(\"No pude entender eso. Â¿PodrÃ­as repetirlo?\")\\n                    continue\\n                \\n                #generate response\\n                response = generate_response(text)\\n                \\n                #Convert response to speech and play it\\n                text_to_speech(response)\\n                \\n                #conversation history\\n                self.conversation_history.append({\\n                    \\'user\\': text,\\n                    \\'bot\\': response,\\n                    \\'timestamp\\': time.time()\\n                })\\n                \\n            except KeyboardInterrupt:\\n                print(\"\\\\nÂ¡Hasta luego!\")\\n                self.is_active = False\\n            except Exception as e:\\n                print(f\"Lo siento, hubo un error: {str(e)}\")\\n                continue\\n            \\n    def stop(self):\\n        \"\"\"Stop the conversation.\"\"\"\\n        self.is_active = False\\n\\n\\ndef main():\\n    \"\"\"Main entry point for the conversation.\"\"\"\\n    conversation = Conversation()\\n    conversation.start()\\n\\n\\nif __name__ == \"__main__\":\\n    main()', 'side_proj/src/spanish_chat_bot/endpoint_handler.py': 'from dotenv import load_dotenv\\nload_dotenv(\"tokens.env\")\\n\\n\\nfrom fastapi import FastAPI, WebSocket, HTTPException\\nfrom pydantic import BaseModel\\nimport numpy as np\\nimport traceback\\nimport logging\\nfrom .audio import record_audio, text_to_speech\\nfrom .models import transcribe_audio, generate_response\\n\\n# Set up logging\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger(__name__)\\n\\napp = FastAPI()\\n\\nclass AudioData(BaseModel):\\n    audioData: list[float]\\n\\nclass TextData(BaseModel):\\n    text: str\\n\\n@app.post(\"/api/start-recording\")\\nasync def start_recording():\\n    try:\\n        audio = record_audio()\\n        return {\"audioData\": audio.tolist()}\\n    except Exception as e:\\n        logger.error(f\"Error in start_recording: {str(e)}\")\\n        logger.error(traceback.format_exc())\\n        raise HTTPException(status_code=500, detail=str(e))\\n\\n@app.post(\"/api/transcribe\")\\nasync def handle_transcription(audio: AudioData):\\n    try:\\n        audio_array = np.array(audio.audioData)\\n        # Get the recorded audio turn into an array and transcribe it\\n        text = transcribe_audio(audio=audio_array)\\n        return {\"transcription\": text}\\n    except Exception as e:\\n        logger.error(f\"Error in handle_transcription: {str(e)}\")\\n        logger.error(traceback.format_exc())\\n        raise HTTPException(status_code=500, detail=str(e))\\n\\n@app.post(\"/api/generate\")\\nasync def handle_generation(text_data: TextData): #since frontend sends in json form, need to introduce it as TextData to validate JSON\\n    try:\\n        logger.info(f\"[testing] Starting generation for text: {text_data.text}\")\\n        \\n        # Generate response and convert to speech\\n        # generate_response expects text and optional model_name and api_key\\n        logger.info(\"[testing] Calling generate_response...\")\\n        response = generate_response(text=text_data.text)\\n        logger.info(f\"[testing] Generated response: {response}\")\\n        \\n        logger.info(\"[testing] Calling text_to_speech...\")\\n        audio_url = text_to_speech(response)\\n        logger.info(f\"[testing] Audio URL: {audio_url}\")\\n        \\n        return {\\n            \"response\": response,\\n            \"audioUrl\": audio_url\\n        }\\n    except Exception as e:\\n        logger.error(f\"Error in handle_generation: {str(e)}\")\\n        logger.error(traceback.format_exc())\\n        raise HTTPException(status_code=500, detail=str(e))\\n\\n@app.websocket(\"/ws\")\\nasync def websocket_endpoint(websocket: WebSocket):\\n    await websocket.accept()\\n    # Handle real-time updates if needed\\n', 'side_proj/frontend/src/App.jsx': 'import { useState } from \\'react\\'\\nimport \\'./App.css\\'\\nimport ChatMessage from \\'./components/Message\\'\\nimport RecordingInterface from \\'./components/RecordingInterface\\'\\n\\nfunction App() {\\n  const [messages, setMessages] = useState([]);\\n\\n  // This function will be called by RecordingInterface\\n  const handleNewMessage = (newMessage) => {\\n    // Add user message to the chat\\n    setMessages(prev => [...prev, { text: newMessage.user, isBot: false }]);\\n    \\n    // Add bot response to the chat\\n    setMessages(prev => [...prev, { text: newMessage.bot, isBot: true }]);\\n  };\\n\\n  //main container that is whole screen, gray bg\\n  return (\\n    <div className=\"min-h-screen bg-gray-100\">\\n      {/*center container with padding, with extra bottom padding to avoid overlap with fixed recorder */}\\n      <div className=\"container mx-auto p-4 pb-40\">\\n        {/*white chat card, takes remaining space, max width*/}\\n        <div className=\"bg-white rounded-lg shadow-lg p-6 max-w-xl mx-auto\">\\n          <h1 className=\"text-2xl font-bold text-center mb-6 text-gray-800\">\\n          {/*Title*/}\\n            Contigo\\n          </h1>\\n          {/*Message area - scrollable, takes remaining space*/}\\n          <div className=\"space-y-4\">\\n            {/*Messages rendered here*/}\\n            {messages.map((msg, index) => (\\n              <ChatMessage \\n                key={index}\\n                message={msg.text}\\n                isBot={msg.isBot}\\n              />\\n            ))}\\n          </div>\\n          {/*recording interface (margin top for separaton)*/}\\n          <div className=\"mt-6\">\\n            <RecordingInterface onNewMessage={handleNewMessage} />\\n          </div>\\n        </div>\\n      </div>\\n    </div>\\n  )\\n}\\n\\nexport default App\\n', 'side_proj/notebooks/inference_pipeline.ipynb': '{\\n \"cells\": [\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"import torch\\\\n\",\\n    \"from transformers import WhisperProcessor, WhisperForConditionalGeneration, WhisperFeatureExtractor, WhisperTokenizer\\\\n\",\\n    \"import os\\\\n\",\\n    \"\\\\n\",\\n    \"import torch\\\\n\",\\n    \"if torch.cuda.is_available():\\\\n\",\\n    \"    device = \\'cuda\\'\\\\n\",\\n    \"    print(f\\'Using GPU: {torch.cuda.get_device_name(0)}\\')\\\\n\",\\n    \"else:\\\\n\",\\n    \"    device = \\'cpu\\'\\\\n\",\\n    \"    print(\\'GPU not available, using CPU.\\')\\\\n\",\\n    \"    \\\\n\",\\n    \"device = torch.device(\\\\\"cuda\\\\\" if torch.cuda.is_available() else \\\\\"cpu\\\\\")\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 3,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"   from dotenv import load_dotenv\\\\n\",\\n    \"   import os\\\\n\",\\n    \"\\\\n\",\\n    \"   load_dotenv(\\\\\"HF.config\\\\\")\\\\n\",\\n    \"   hf_token = os.getenv(\\\\\"HF_TOKEN\\\\\")\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 3,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"#load feature extractor from pre-trained checkpoint\\\\n\",\\n    \"feature_extractor = WhisperFeatureExtractor.from_pretrained(\\\\\"openai/whisper-small\\\\\")\\\\n\",\\n    \"tokenizer = WhisperTokenizer.from_pretrained(\\\\\"openai/whisper-small\\\\\", language=\\\\\"Spanish\\\\\", task=\\\\\"transcribe\\\\\")\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 124,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"processor = WhisperProcessor.from_pretrained(\\\\\"openai/whisper-small\\\\\", language=\\\\\"spanish\\\\\", task=\\\\\"transcribe\\\\\")\\\\n\",\\n    \"og_model = WhisperForConditionalGeneration.from_pretrained(\\\\\"openai/whisper-small\\\\\")\\\\n\",\\n    \"model = WhisperForConditionalGeneration.from_pretrained(\\\\\"mevitts/whisper-small-es\\\\\")\\\\n\",\\n    \"\\\\n\",\\n    \"og_model = og_model.to(device)\\\\n\",\\n    \"model = model.to(device)\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 125,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from transformers import GenerationConfig\\\\n\",\\n    \"\\\\n\",\\n    \"\\\\n\",\\n    \"# Set model configuration\\\\n\",\\n    \"og_model.generation_config = GenerationConfig.from_pretrained(\\\\\"openai/whisper-small\\\\\")\\\\n\",\\n    \"og_model.generation_config.language = \\\\\"spanish\\\\\"\\\\n\",\\n    \"og_model.generation_config.task = \\\\\"transcribe\\\\\"\\\\n\",\\n    \"og_model.config.pad_token_id = og_model.config.eos_token_id\\\\n\",\\n    \"\\\\n\",\\n    \"model.generation_config = GenerationConfig.from_pretrained(\\\\\"openai/whisper-small\\\\\")\\\\n\",\\n    \"model.generation_config.language = \\\\\"spanish\\\\\"\\\\n\",\\n    \"model.generation_config.task = \\\\\"transcribe\\\\\"\\\\n\",\\n    \"model.config.pad_token_id = model.config.eos_token_id\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 121,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"import sounddevice as sd\\\\n\",\\n    \"import numpy as np\\\\n\",\\n    \"from scipy.signal import butter, filtfilt\\\\n\",\\n    \"\\\\n\",\\n    \"\\\\n\",\\n    \"def record_audio(duration=5, sample_rate=16000):\\\\n\",\\n    \"    \\\\\"\\\\\"\\\\\"Record audio for set duration\\\\\"\\\\\"\\\\\"\\\\n\",\\n    \"    print(\\\\\"Say what you want\\\\\")\\\\n\",\\n    \"    myrecording = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1)\\\\n\",\\n    \"    sd.wait()\\\\n\",\\n    \"    \\\\n\",\\n    \"    #convert to single array, then normalize audio\\\\n\",\\n    \"    audio = myrecording.flatten()\\\\n\",\\n    \"    \\\\n\",\\n    \"    audio = audio / np.max(np.abs(audio))\\\\n\",\\n    \"    \\\\n\",\\n    \"    # apply filter to reduce noise\\\\n\",\\n    \"    nyquist = sample_rate / 2\\\\n\",\\n    \"    cutoff = 100 #Hz\\\\n\",\\n    \"    b, a = butter(4, cutoff/nyquist, btype=\\'high\\')\\\\n\",\\n    \"    audio = filtfilt(b, a, audio)\\\\n\",\\n    \"    \\\\n\",\\n    \"    audio = audio / np.max(np.abs(audio))\\\\n\",\\n    \"    return audio\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 127,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"def transcribe_audio(audio, sample_rate=16000, model=og_model):\\\\n\",\\n    \"    \\\\n\",\\n    \"    \\\\n\",\\n    \"    inputs = feature_extractor(\\\\n\",\\n    \"        audio,\\\\n\",\\n    \"        sampling_rate=sample_rate,\\\\n\",\\n    \"        return_tensors=\\\\\"pt\\\\\"\\\\n\",\\n    \"    )\\\\n\",\\n    \"    input_features = inputs.input_features.to(device)\\\\n\",\\n    \"    \\\\n\",\\n    \"    predicted_ids = model.generate(\\\\n\",\\n    \"        input_features,\\\\n\",\\n    \"        max_length=448,  # Increased from default\\\\n\",\\n    \"        num_beams=5,     # Increased from default\\\\n\",\\n    \"        temperature=0.7  # Slightly reduced from default\\\\n\",\\n    \"    )\\\\n\",\\n    \"    transcription = tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)[0]\\\\n\",\\n    \"    \\\\n\",\\n    \"    return transcription\\\\n\",\\n    \"    \"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"rec = record_audio()\\\\n\",\\n    \"msg = transcribe_audio(rec, model=og_model)\\\\n\",\\n    \"matt_msg = transcribe_audio(rec, model=model)\\\\n\",\\n    \"print(msg)\\\\n\",\\n    \"print(matt_msg)\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 157,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"\\\\n\",\\n    \"from dotenv import load_dotenv\\\\n\",\\n    \"import os\\\\n\",\\n    \"import google.generativeai as gem\\\\n\",\\n    \"\\\\n\",\\n    \"load_dotenv(\\\\\"HF.config\\\\\")\\\\n\",\\n    \"google_api_key = os.getenv(\\\\\"GOOGLE_API_KEY\\\\\")\\\\n\",\\n    \"if not google_api_key:\\\\n\",\\n    \"    raise ValueError(\\\\\"GOOGLE_API_KEY not found in environment variables\\\\\")\\\\n\",\\n    \"#genai.configure(api_key=google_api_key)\\\\n\",\\n    \"gem.configure(api_key=google_api_key)\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 163,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"gemini = gem.GenerativeModel(\\'gemini-2.0-flash\\')\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 166,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"def generate_response(text):\\\\n\",\\n    \"    response = gemini.generate_content(\\\\n\",\\n    \"            f\\\\\"Eres un asistente conversacional en espaÃ±ol. Adaptarse al contexto de la conversaciÃ³n. (Por ejemplo: si la persona te diga algo como si fuera tu amigo o un familiar, respondele con ese rol.) Mantenga la duraciÃ³n de tu respuesta corta. Responde de manera natural y conversacional a: {text}\\\\\"\\\\n\",\\n    \"        )\\\\n\",\\n    \"    return response\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"generate_response(\\\\\"Cual es tu comida favorita?\\\\\")\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 160,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"def test_full_pipeline():\\\\n\",\\n    \"    print(\\\\\"Recording audio...\\\\\")\\\\n\",\\n    \"    audio = record_audio()\\\\n\",\\n    \"    \\\\n\",\\n    \"    print(\\\\\"\\\\\\\\nTranscribing audio...\\\\\")\\\\n\",\\n    \"    transcription = transcribe_audio(audio, model=og_model)\\\\n\",\\n    \"    print(f\\\\\"Transcription: {transcription}\\\\\")\\\\n\",\\n    \"    \\\\n\",\\n    \"    print(\\\\\"\\\\\\\\nGenerating response...\\\\\")\\\\n\",\\n    \"    response = generate_response(transcription)\\\\n\",\\n    \"    print(f\\\\\"Response: {response}\\\\\")\\\\n\",\\n    \"    \\\\n\",\\n    \"    return transcription, response\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"transcription, response = test_full_pipeline()\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 10,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"import torch\\\\n\",\\n    \"from transformers import WhisperProcessor, WhisperForConditionalGeneration, WhisperFeatureExtractor, WhisperTokenizer\\\\n\",\\n    \"import os\\\\n\",\\n    \"import sounddevice as sd\\\\n\",\\n    \"import numpy as np\\\\n\",\\n    \"from scipy.io import wavfile\\\\n\",\\n    \"from TTS.api import TTS\\\\n\",\\n    \"\\\\n\",\\n    \"def text_to_speech(text, output_path=\\\\\"output.wav\\\\\"):\\\\n\",\\n    \"    # Initialize TTS with Spanish xtts model\\\\n\",\\n    \"    tts = TTS(model_name=\\\\\"tts_models/es/css10/vits\\\\\", progress_bar=False)\\\\n\",\\n    \"    \\\\n\",\\n    \"    # Generate speech\\\\n\",\\n    \"    tts.tts_to_file(text=text, file_path=output_path)\\\\n\",\\n    \"    \\\\n\",\\n    \"    # Play the audio\\\\n\",\\n    \"    sample_rate, audio = wavfile.read(output_path)\\\\n\",\\n    \"    sd.play(audio, sample_rate)\\\\n\",\\n    \"    sd.wait()\\\\n\",\\n    \"    \\\\n\",\\n    \"    return output_path\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 11,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \" > tts_models/es/css10/vits is already downloaded.\\\\n\",\\n      \" > Using model: vits\\\\n\",\\n      \" > Setting up Audio Processor...\\\\n\",\\n      \" | > sample_rate:22050\\\\n\",\\n      \" | > resample:False\\\\n\",\\n      \" | > num_mels:80\\\\n\",\\n      \" | > log_func:np.log10\\\\n\",\\n      \" | > min_level_db:0\\\\n\",\\n      \" | > frame_shift_ms:None\\\\n\",\\n      \" | > frame_length_ms:None\\\\n\",\\n      \" | > ref_level_db:None\\\\n\",\\n      \" | > fft_size:1024\\\\n\",\\n      \" | > power:None\\\\n\",\\n      \" | > preemphasis:0.0\\\\n\",\\n      \" | > griffin_lim_iters:None\\\\n\",\\n      \" | > signal_norm:None\\\\n\",\\n      \" | > symmetric_norm:None\\\\n\",\\n      \" | > mel_fmin:0\\\\n\",\\n      \" | > mel_fmax:None\\\\n\",\\n      \" | > pitch_fmin:None\\\\n\",\\n      \" | > pitch_fmax:None\\\\n\",\\n      \" | > spec_gain:20.0\\\\n\",\\n      \" | > stft_pad_mode:reflect\\\\n\",\\n      \" | > max_norm:1.0\\\\n\",\\n      \" | > clip_norm:True\\\\n\",\\n      \" | > do_trim_silence:False\\\\n\",\\n      \" | > trim_db:60\\\\n\",\\n      \" | > do_sound_norm:False\\\\n\",\\n      \" | > do_amp_to_db_linear:True\\\\n\",\\n      \" | > do_amp_to_db_mel:True\\\\n\",\\n      \" | > do_rms_norm:False\\\\n\",\\n      \" | > db_level:None\\\\n\",\\n      \" | > stats_path:None\\\\n\",\\n      \" | > base:10\\\\n\",\\n      \" | > hop_length:256\\\\n\",\\n      \" | > win_length:1024\\\\n\",\\n      \" > initialization of speaker-embedding layers.\\\\n\",\\n      \" > initialization of language-embedding layers.\\\\n\",\\n      \" > Text splitted to sentences.\\\\n\",\\n      \"[\\'Mirense a estos pinches gringos!\\']\\\\n\",\\n      \" > Processing time: 0.9494185447692871\\\\n\",\\n      \" > Real-time factor: 0.31086182751489044\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"output_wav = text_to_speech(\\\\\"Mirense a estos pinches gringos!\\\\\")\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 15,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"Available Spanish models:\\\\n\",\\n      \"\\\\n\",\\n      \" Name format: type/language/dataset/model\\\\n\",\\n      \" 1: tts_models/multilingual/multi-dataset/xtts_v2\\\\n\",\\n      \" 2: tts_models/multilingual/multi-dataset/xtts_v1.1\\\\n\",\\n      \" 3: tts_models/multilingual/multi-dataset/your_tts\\\\n\",\\n      \" 4: tts_models/multilingual/multi-dataset/bark\\\\n\",\\n      \" 5: tts_models/bg/cv/vits\\\\n\",\\n      \" 6: tts_models/cs/cv/vits\\\\n\",\\n      \" 7: tts_models/da/cv/vits\\\\n\",\\n      \" 8: tts_models/et/cv/vits\\\\n\",\\n      \" 9: tts_models/ga/cv/vits\\\\n\",\\n      \" 10: tts_models/en/ek1/tacotron2\\\\n\",\\n      \" 11: tts_models/en/ljspeech/tacotron2-DDC\\\\n\",\\n      \" 12: tts_models/en/ljspeech/tacotron2-DDC_ph\\\\n\",\\n      \" 13: tts_models/en/ljspeech/glow-tts\\\\n\",\\n      \" 14: tts_models/en/ljspeech/speedy-speech\\\\n\",\\n      \" 15: tts_models/en/ljspeech/tacotron2-DCA\\\\n\",\\n      \" 16: tts_models/en/ljspeech/vits\\\\n\",\\n      \" 17: tts_models/en/ljspeech/vits--neon\\\\n\",\\n      \" 18: tts_models/en/ljspeech/fast_pitch\\\\n\",\\n      \" 19: tts_models/en/ljspeech/overflow\\\\n\",\\n      \" 20: tts_models/en/ljspeech/neural_hmm\\\\n\",\\n      \" 21: tts_models/en/vctk/vits\\\\n\",\\n      \" 22: tts_models/en/vctk/fast_pitch\\\\n\",\\n      \" 23: tts_models/en/sam/tacotron-DDC\\\\n\",\\n      \" 24: tts_models/en/blizzard2013/capacitron-t2-c50\\\\n\",\\n      \" 25: tts_models/en/blizzard2013/capacitron-t2-c150_v2\\\\n\",\\n      \" 26: tts_models/en/multi-dataset/tortoise-v2\\\\n\",\\n      \" 27: tts_models/en/jenny/jenny\\\\n\",\\n      \" 28: tts_models/es/mai/tacotron2-DDC [already downloaded]\\\\n\",\\n      \" 29: tts_models/es/css10/vits [already downloaded]\\\\n\",\\n      \" 30: tts_models/fr/mai/tacotron2-DDC\\\\n\",\\n      \" 31: tts_models/fr/css10/vits\\\\n\",\\n      \" 32: tts_models/uk/mai/glow-tts\\\\n\",\\n      \" 33: tts_models/uk/mai/vits\\\\n\",\\n      \" 34: tts_models/zh-CN/baker/tacotron2-DDC-GST\\\\n\",\\n      \" 35: tts_models/nl/mai/tacotron2-DDC\\\\n\",\\n      \" 36: tts_models/nl/css10/vits\\\\n\",\\n      \" 37: tts_models/de/thorsten/tacotron2-DCA\\\\n\",\\n      \" 38: tts_models/de/thorsten/vits\\\\n\",\\n      \" 39: tts_models/de/thorsten/tacotron2-DDC\\\\n\",\\n      \" 40: tts_models/de/css10/vits-neon\\\\n\",\\n      \" 41: tts_models/ja/kokoro/tacotron2-DDC\\\\n\",\\n      \" 42: tts_models/tr/common-voice/glow-tts\\\\n\",\\n      \" 43: tts_models/it/mai_female/glow-tts\\\\n\",\\n      \" 44: tts_models/it/mai_female/vits\\\\n\",\\n      \" 45: tts_models/it/mai_male/glow-tts\\\\n\",\\n      \" 46: tts_models/it/mai_male/vits\\\\n\",\\n      \" 47: tts_models/ewe/openbible/vits\\\\n\",\\n      \" 48: tts_models/hau/openbible/vits\\\\n\",\\n      \" 49: tts_models/lin/openbible/vits\\\\n\",\\n      \" 50: tts_models/tw_akuapem/openbible/vits\\\\n\",\\n      \" 51: tts_models/tw_asante/openbible/vits\\\\n\",\\n      \" 52: tts_models/yor/openbible/vits\\\\n\",\\n      \" 53: tts_models/hu/css10/vits\\\\n\",\\n      \" 54: tts_models/el/cv/vits\\\\n\",\\n      \" 55: tts_models/fi/css10/vits\\\\n\",\\n      \" 56: tts_models/hr/cv/vits\\\\n\",\\n      \" 57: tts_models/lt/cv/vits\\\\n\",\\n      \" 58: tts_models/lv/cv/vits\\\\n\",\\n      \" 59: tts_models/mt/cv/vits\\\\n\",\\n      \" 60: tts_models/pl/mai_female/vits\\\\n\",\\n      \" 61: tts_models/pt/cv/vits\\\\n\",\\n      \" 62: tts_models/ro/cv/vits\\\\n\",\\n      \" 63: tts_models/sk/cv/vits\\\\n\",\\n      \" 64: tts_models/sl/cv/vits\\\\n\",\\n      \" 65: tts_models/sv/cv/vits\\\\n\",\\n      \" 66: tts_models/ca/custom/vits\\\\n\",\\n      \" 67: tts_models/fa/custom/glow-tts\\\\n\",\\n      \" 68: tts_models/bn/custom/vits-male\\\\n\",\\n      \" 69: tts_models/bn/custom/vits-female\\\\n\",\\n      \" 70: tts_models/be/common-voice/glow-tts\\\\n\",\\n      \"\\\\n\",\\n      \" Name format: type/language/dataset/model\\\\n\",\\n      \" 1: vocoder_models/universal/libri-tts/wavegrad\\\\n\",\\n      \" 2: vocoder_models/universal/libri-tts/fullband-melgan [already downloaded]\\\\n\",\\n      \" 3: vocoder_models/en/ek1/wavegrad\\\\n\",\\n      \" 4: vocoder_models/en/ljspeech/multiband-melgan\\\\n\",\\n      \" 5: vocoder_models/en/ljspeech/hifigan_v2\\\\n\",\\n      \" 6: vocoder_models/en/ljspeech/univnet\\\\n\",\\n      \" 7: vocoder_models/en/blizzard2013/hifigan_v2\\\\n\",\\n      \" 8: vocoder_models/en/vctk/hifigan_v2\\\\n\",\\n      \" 9: vocoder_models/en/sam/hifigan_v2\\\\n\",\\n      \" 10: vocoder_models/nl/mai/parallel-wavegan\\\\n\",\\n      \" 11: vocoder_models/de/thorsten/wavegrad\\\\n\",\\n      \" 12: vocoder_models/de/thorsten/fullband-melgan\\\\n\",\\n      \" 13: vocoder_models/de/thorsten/hifigan_v1\\\\n\",\\n      \" 14: vocoder_models/ja/kokoro/hifigan_v1\\\\n\",\\n      \" 15: vocoder_models/uk/mai/multiband-melgan\\\\n\",\\n      \" 16: vocoder_models/tr/common-voice/hifigan\\\\n\",\\n      \" 17: vocoder_models/be/common-voice/hifigan\\\\n\",\\n      \"\\\\n\",\\n      \" Name format: type/language/dataset/model\\\\n\",\\n      \" 1: voice_conversion_models/multilingual/vctk/freevc24\\\\n\",\\n      \"tts_models/es/mai/tacotron2-DDC\\\\n\",\\n      \"tts_models/es/css10/vits\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"from TTS.utils.manage import ModelManager\\\\n\",\\n    \"manager = ModelManager()\\\\n\",\\n    \"print(\\\\\"Available Spanish models:\\\\\")\\\\n\",\\n    \"for model in manager.list_models():\\\\n\",\\n    \"    if \\\\\"es\\\\\" in model or \\\\\"spanish\\\\\" in model.lower():\\\\n\",\\n    \"        print(model)\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 13,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stderr\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"ERROR: Could not find a version that satisfies the requirement time (from versions: none)\\\\n\",\\n      \"ERROR: No matching distribution found for time\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"!pip install time\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 14,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"import time\\\\n\",\\n    \"\\\\n\",\\n    \"def run_conversation():\\\\n\",\\n    \"    print(\\\\\"Starting conversation\\\\\")\\\\n\",\\n    \"    \\\\n\",\\n    \"    while True:\\\\n\",\\n    \"        try:\\\\n\",\\n    \"            \\\\n\",\\n    \"            \\\\n\",\\n    \"            #record user\\\\n\",\\n    \"            print(\\\\\"Recording now\\\\\")\\\\n\",\\n    \"            try:\\\\n\",\\n    \"                audio = record_audio()\\\\n\",\\n    \"            except Exception as e:\\\\n\",\\n    \"                print(f\\\\\"Error recording audio: {str(e)}\\\\\")\\\\n\",\\n    \"                print(\\\\\"Please try again.\\\\\")\\\\n\",\\n    \"                continue\\\\n\",\\n    \"            \\\\n\",\\n    \"            #transcribe audio\\\\n\",\\n    \"            try:\\\\n\",\\n    \"                transcription = transcribe_audio(audio, model=og_model)\\\\n\",\\n    \"                if not transcription.strip():\\\\n\",\\n    \"                    print(\\'No speech detected. Please try again\\')\\\\n\",\\n    \"                    continue\\\\n\",\\n    \"                print(f\\'\\\\\\\\nDid you say: {transcription}?\\')\\\\n\",\\n    \"            except Exception as e:\\\\n\",\\n    \"                print(f\\\\\"Error recording audio: {str(e)}\\\\\")\\\\n\",\\n    \"                print(\\\\\"Please try again.\\\\\")\\\\n\",\\n    \"                continue\\\\n\",\\n    \"            \\\\n\",\\n    \"            #if exit conversation\\\\n\",\\n    \"            if transcription.lower() in [\\'adiÃ³s\\', \\'adios\\', \\'chao\\', \\'bye\\', \\'goodbye\\', \\'done\\', \\'stop\\']:\\\\n\",\\n    \"                print(\\\\\"\\\\\\\\nEnding conversation. Chao!\\\\\")\\\\n\",\\n    \"                break\\\\n\",\\n    \"            \\\\n\",\\n    \"            #generate response\\\\n\",\\n    \"            try:\\\\n\",\\n    \"                response = generate_response(transcription)\\\\n\",\\n    \"            except Exception as e:\\\\n\",\\n    \"                    print(f\\\\\"Error generating response: {str(e)}\\\\\")\\\\n\",\\n    \"                    print(\\\\\"Please try again.\\\\\")\\\\n\",\\n    \"                    continue\\\\n\",\\n    \"                \\\\n\",\\n    \"            \\\\n\",\\n    \"            #convert response to speech output\\\\n\",\\n    \"            try:\\\\n\",\\n    \"                text_to_speech(response.text)\\\\n\",\\n    \"            except Exception as e:\\\\n\",\\n    \"                    print(f\\\\\"Error converting response to speech: {str(e)}\\\\\")\\\\n\",\\n    \"                    print(f\\\\\"Continuing without speech output.\\\\\\\\nResponse: {response.text}\\\\\")\\\\n\",\\n    \"                    time.sleep(3)\\\\n\",\\n    \"                    continue\\\\n\",\\n    \"                \\\\n\",\\n    \"                \\\\n\",\\n    \"        except KeyboardInterrupt:\\\\n\",\\n    \"            print(\\\\\"\\\\\\\\n\\\\\\\\nConversation interrupted by user. Â¡Hasta luego!\\\\\")\\\\n\",\\n    \"            break\\\\n\",\\n    \"        except Exception as e:\\\\n\",\\n    \"            print(f\\\\\"\\\\\\\\nUnexpected error: {str(e)}\\\\\")\\\\n\",\\n    \"            print(\\\\\"Please try again.\\\\\")\\\\n\",\\n    \"            continue\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"run_conversation()\"\\n   ]\\n  }\\n ],\\n \"metadata\": {\\n  \"kernelspec\": {\\n   \"display_name\": \".venv\",\\n   \"language\": \"python\",\\n   \"name\": \"python3\"\\n  },\\n  \"language_info\": {\\n   \"codemirror_mode\": {\\n    \"name\": \"ipython\",\\n    \"version\": 3\\n   },\\n   \"file_extension\": \".py\",\\n   \"mimetype\": \"text/x-python\",\\n   \"name\": \"python\",\\n   \"nbconvert_exporter\": \"python\",\\n   \"pygments_lexer\": \"ipython3\",\\n   \"version\": \"3.10.0\"\\n  }\\n },\\n \"nbformat\": 4,\\n \"nbformat_minor\": 2\\n}\\n', 'side_proj/spanish_conversation_bot.ipynb': '{\\n \"cells\": [\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 73,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"import torch\\\\n\",\\n    \"import gc\\\\n\",\\n    \"import os\\\\n\",\\n    \"\\\\n\",\\n    \"# Clear GPU memory\\\\n\",\\n    \"def clear_gpu_memory():\\\\n\",\\n    \"    gc.collect()\\\\n\",\\n    \"    torch.cuda.empty_cache()\\\\n\",\\n    \"    torch.cuda.reset_peak_memory_stats()\\\\n\",\\n    \"    \\\\n\",\\n    \"# Call the function\\\\n\",\\n    \"clear_gpu_memory()\\\\n\",\\n    \"\\\\n\",\\n    \"# Set memory optimization environment variables\\\\n\",\\n    \"os.environ[\\\\\"PYTORCH_CUDA_ALLOC_CONF\\\\\"] = \\\\\"expandable_segments:True\\\\\"\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 74,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"Using GPU: NVIDIA GeForce RTX 3060 Laptop GPU\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"# Check for GPU and set device\\\\n\",\\n    \"import torch\\\\n\",\\n    \"if torch.cuda.is_available():\\\\n\",\\n    \"    device = \\'cuda\\'\\\\n\",\\n    \"    print(f\\'Using GPU: {torch.cuda.get_device_name(0)}\\')\\\\n\",\\n    \"else:\\\\n\",\\n    \"    device = \\'cpu\\'\\\\n\",\\n    \"    print(\\'GPU not available, using CPU.\\')\\\\n\",\\n    \"# Use the \\'device\\' variable when loading models.\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 75,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"   from dotenv import load_dotenv\\\\n\",\\n    \"   import os\\\\n\",\\n    \"\\\\n\",\\n    \"   load_dotenv(\\\\\"HF.config\\\\\")\\\\n\",\\n    \"   hf_token = os.getenv(\\\\\"HF_TOKEN\\\\\")\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 76,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stderr\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you\\'ve just configured.\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"from huggingface_hub import  login\\\\n\",\\n    \"\\\\n\",\\n    \"#authenticating HF login, as CV17 dataset requires it\\\\n\",\\n    \"login(token=hf_token)\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 77,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"\\\\n\",\\n      \"Latin American Spanish sample:\\\\n\",\\n      \"\\\\n\",\\n      \"Dataset sizes:\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"from datasets import load_dataset, concatenate_datasets, DatasetDict\\\\n\",\\n    \"\\\\n\",\\n    \"\\'\\'\\'\\\\n\",\\n    \"# top commented out as anything from now commented out was attempted with common voice, but pivoted to smaller dataset\\\\n\",\\n    \"#columns_to_remove = [\\'client_id\\', \\'path\\', \\'up_votes\\', \\'down_votes\\', \\'age\\', \\'gender\\', \\'accent\\', \\'locale\\', \\'segment\\', \\'variant\\']\\\\n\",\\n    \"\\\\n\",\\n    \"#streaming so no fill download, spanish full dataset is 48 GB!\\\\n\",\\n    \"ds_train = load_dataset(\\\\n\",\\n    \"    \\\\\"mozilla-foundation/common_voice_6_0\\\\\",\\\\n\",\\n    \"    \\\\\"es\\\\\",\\\\n\",\\n    \"    split=\\\\\"train\\\\\",\\\\n\",\\n    \"    streaming=True,\\\\n\",\\n    \"    trust_remote_code=True\\\\n\",\\n    \").remove_columns(columns_to_remove)\\\\n\",\\n    \"\\\\n\",\\n    \"ds_test = load_dataset(\\\\n\",\\n    \"    \\\\\"mozilla-foundation/common_voice_6_0\\\\\",\\\\n\",\\n    \"    \\\\\"es\\\\\",\\\\n\",\\n    \"    split=\\\\\"test\\\\\",\\\\n\",\\n    \"    streaming=True,\\\\n\",\\n    \"    trust_remote_code=True\\\\n\",\\n    \").remove_columns(columns_to_remove)\\\\n\",\\n    \"\\\\n\",\\n    \"\\'\\'\\'\\\\n\",\\n    \"ds_fleur = DatasetDict()\\\\n\",\\n    \"columns_to_keep = [\\'audio\\', \\'transcription\\']  # These are the essential columns for speech recognition\\\\n\",\\n    \"\\\\n\",\\n    \"# Load Latin American Spanish data. If need European spanish in future, voxpopuli has almost all europ samples\\\\n\",\\n    \"ds_fleur[\\\\\"train\\\\\"] = load_dataset(\\\\n\",\\n    \"    \\\\\"google/fleurs\\\\\",\\\\n\",\\n    \"    \\\\\"es_419\\\\\",\\\\n\",\\n    \"    split=\\\\\"train\\\\\",\\\\n\",\\n    \"    trust_remote_code=True\\\\n\",\\n    \").select_columns(columns_to_keep)\\\\n\",\\n    \"\\\\n\",\\n    \"ds_fleur[\\\\\"test\\\\\"] = load_dataset(\\\\n\",\\n    \"    \\\\\"google/fleurs\\\\\",\\\\n\",\\n    \"    \\\\\"es_419\\\\\",\\\\n\",\\n    \"    split=\\\\\"test\\\\\",\\\\n\",\\n    \"    trust_remote_code=True\\\\n\",\\n    \").select_columns(columns_to_keep)\\\\n\",\\n    \"\\\\n\",\\n    \"print(\\\\\"\\\\\\\\nLatin American Spanish sample:\\\\\")\\\\n\",\\n    \"#print(ds_fleur[\\\\\"train\\\\\"][0])\\\\n\",\\n    \"\\\\n\",\\n    \"print(\\\\\"\\\\\\\\nDataset sizes:\\\\\")\\\\n\",\\n    \"#print(f\\\\\"Latin American Spanish train set: {len(list(ds_fleur[\\\\\"train\\\\\"]))} samples\\\\\")\\\\n\",\\n    \"#print(f\\\\\"Latin American Spanish test set: {len(list(ds_fleur[\\\\\"test\\\\\"]))} samples\\\\\")\\\\n\",\\n    \"\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 78,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from transformers import WhisperFeatureExtractor\\\\n\",\\n    \"\\\\n\",\\n    \"#load feature extractor from pre-trained checkpoint\\\\n\",\\n    \"feature_extractor = WhisperFeatureExtractor.from_pretrained(\\\\\"openai/whisper-small\\\\\")\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 79,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from transformers import WhisperTokenizer\\\\n\",\\n    \"\\\\n\",\\n    \"#load tokenizer, has very extensive byte-pair training\\\\n\",\\n    \"tokenizer = WhisperTokenizer.from_pretrained(\\\\\"openai/whisper-small\\\\\", language=\\\\\"Spanish\\\\\", task=\\\\\"transcribe\\\\\")\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 80,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"Input:                 los murales o garabatos indeseados reciben el nombre de grafiti\\\\n\",\\n      \"Decoded w/ special:    <|startoftranscript|><|es|><|transcribe|><|notimestamps|>los murales o garabatos indeseados reciben el nombre de grafiti<|endoftext|>\\\\n\",\\n      \"Decoded w/out special: los murales o garabatos indeseados reciben el nombre de grafiti\\\\n\",\\n      \"Are equal:             True\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"input_str = ds_fleur[\\\\\"train\\\\\"][0][\\\\\"transcription\\\\\"]\\\\n\",\\n    \"labels = tokenizer(input_str).input_ids\\\\n\",\\n    \"decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\\\\n\",\\n    \"decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\\\\n\",\\n    \"\\\\n\",\\n    \"#verifying tokenizer works\\\\n\",\\n    \"\\\\n\",\\n    \"print(f\\\\\"Input:                 {input_str}\\\\\")\\\\n\",\\n    \"print(f\\\\\"Decoded w/ special:    {decoded_with_special}\\\\\")\\\\n\",\\n    \"print(f\\\\\"Decoded w/out special: {decoded_str}\\\\\")\\\\n\",\\n    \"print(f\\\\\"Are equal:             {input_str == decoded_str}\\\\\")\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 81,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from transformers import WhisperProcessor\\\\n\",\\n    \"\\\\n\",\\n    \"processor = WhisperProcessor.from_pretrained(\\\\\"openai/whisper-small\\\\\", language=\\\\\"spanish\\\\\", task=\\\\\"transcribe\\\\\")\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 82,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"{\\'audio\\': {\\'path\\': \\'train/10005668950815513748.wav\\', \\'array\\': array([0., 0., 0., ..., 0., 0., 0.]), \\'sampling_rate\\': 16000}, \\'transcription\\': \\'los murales o garabatos indeseados reciben el nombre de grafiti\\'}\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"print(ds_fleur[\\\\\"train\\\\\"][0])\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 83,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from datasets import Audio\\\\n\",\\n    \"\\\\n\",\\n    \"#downsample audio to 16kHz to match that of Whisper\\'s sampling rate\\\\n\",\\n    \"ds_fleur = ds_fleur.cast_column(\\\\\"audio\\\\\", Audio(sampling_rate=16000))\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 84,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"{\\'audio\\': {\\'path\\': \\'train/10005668950815513748.wav\\', \\'array\\': array([0., 0., 0., ..., 0., 0., 0.]), \\'sampling_rate\\': 16000}, \\'transcription\\': \\'los murales o garabatos indeseados reciben el nombre de grafiti\\'}\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"print(ds_fleur[\\\\\"train\\\\\"][0])\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 85,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"def prepare_dataset(batch, feature_extractor=feature_extractor, tokenizer=tokenizer):   \\\\n\",\\n    \"# resample audio to 16kHz\\\\n\",\\n    \"    audio = batch[\\\\\"audio\\\\\"]\\\\n\",\\n    \"    \\\\n\",\\n    \"    # compute log-mel input feats from arrau\\\\n\",\\n    \"    inputs = feature_extractor(\\\\n\",\\n    \"        audio[\\\\\"array\\\\\"], \\\\n\",\\n    \"        sampling_rate=audio[\\\\\"sampling_rate\\\\\"]).input_features[0]\\\\n\",\\n    \"\\\\n\",\\n    \"    batch[\\\\\"input_features\\\\\"] = inputs.input_features[0]\\\\n\",\\n    \"    batch[\\\\\"attention_mask\\\\\"] = inputs.attention_mask[0]\\\\n\",\\n    \"    # encode target text to label ids\\\\n\",\\n    \"    batch[\\\\\"labels\\\\\"] = tokenizer(batch[\\\\\"transcription\\\\\"]).input_ids\\\\n\",\\n    \"    return batch\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 86,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"data\": {\\n      \"application/vnd.jupyter.widget-view+json\": {\\n       \"model_id\": \"7f511274f955434ba7da84a90501aeb0\",\\n       \"version_major\": 2,\\n       \"version_minor\": 0\\n      },\\n      \"text/plain\": [\\n       \"Map (num_proc=4):   0%|          | 0/2796 [00:00<?, ? examples/s]\"\\n      ]\\n     },\\n     \"metadata\": {},\\n     \"output_type\": \"display_data\"\\n    },\\n    {\\n     \"ename\": \"AttributeError\",\\n     \"evalue\": \"\\'numpy.ndarray\\' object has no attribute \\'input_features\\'\",\\n     \"output_type\": \"error\",\\n     \"traceback\": [\\n      \"\\\\u001b[1;31m---------------------------------------------------------------------------\\\\u001b[0m\",\\n      \"\\\\u001b[1;31mRemoteTraceback\\\\u001b[0m                           Traceback (most recent call last)\",\\n      \"\\\\u001b[1;31mRemoteTraceback\\\\u001b[0m: \\\\n\\\\\"\\\\\"\\\\\"\\\\nTraceback (most recent call last):\\\\n  File \\\\\"C:\\\\\\\\Users\\\\\\\\marte\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\gpu\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\multiprocess\\\\\\\\pool.py\\\\\", line 125, in worker\\\\n    result = (True, func(*args, **kwds))\\\\n  File \\\\\"C:\\\\\\\\Users\\\\\\\\marte\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\gpu\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\datasets\\\\\\\\utils\\\\\\\\py_utils.py\\\\\", line 688, in _write_generator_to_queue\\\\n    for i, result in enumerate(func(**kwargs)):\\\\n  File \\\\\"C:\\\\\\\\Users\\\\\\\\marte\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\gpu\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\datasets\\\\\\\\arrow_dataset.py\\\\\", line 3501, in _map_single\\\\n    for i, example in iter_outputs(shard_iterable):\\\\n  File \\\\\"C:\\\\\\\\Users\\\\\\\\marte\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\gpu\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\datasets\\\\\\\\arrow_dataset.py\\\\\", line 3475, in iter_outputs\\\\n    yield i, apply_function(example, i, offset=offset)\\\\n  File \\\\\"C:\\\\\\\\Users\\\\\\\\marte\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\gpu\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\datasets\\\\\\\\arrow_dataset.py\\\\\", line 3398, in apply_function\\\\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\\\\n  File \\\\\"C:\\\\\\\\Users\\\\\\\\marte\\\\\\\\AppData\\\\\\\\Local\\\\\\\\Temp\\\\\\\\ipykernel_31064\\\\\\\\2898349104.py\\\\\", line 10, in prepare_dataset\\\\nAttributeError: \\'numpy.ndarray\\' object has no attribute \\'input_features\\'\\\\n\\\\\"\\\\\"\\\\\"\",\\n      \"\\\\nThe above exception was the direct cause of the following exception:\\\\n\",\\n      \"\\\\u001b[1;31mAttributeError\\\\u001b[0m                            Traceback (most recent call last)\",\\n      \"Cell \\\\u001b[1;32mIn[86], line 1\\\\u001b[0m\\\\n\\\\u001b[1;32m----> 1\\\\u001b[0m ds_fleur \\\\u001b[38;5;241m=\\\\u001b[39m \\\\u001b[43mds_fleur\\\\u001b[49m\\\\u001b[38;5;241;43m.\\\\u001b[39;49m\\\\u001b[43mmap\\\\u001b[49m\\\\u001b[43m(\\\\u001b[49m\\\\n\\\\u001b[0;32m      2\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mprepare_dataset\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m      3\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mremove_columns\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mds_fleur\\\\u001b[49m\\\\u001b[38;5;241;43m.\\\\u001b[39;49m\\\\u001b[43mcolumn_names\\\\u001b[49m\\\\u001b[43m[\\\\u001b[49m\\\\u001b[38;5;124;43m\\\\\"\\\\u001b[39;49m\\\\u001b[38;5;124;43mtrain\\\\u001b[39;49m\\\\u001b[38;5;124;43m\\\\\"\\\\u001b[39;49m\\\\u001b[43m]\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m      4\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mnum_proc\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[38;5;241;43m4\\\\u001b[39;49m\\\\u001b[43m,\\\\u001b[49m\\\\u001b[43m  \\\\u001b[49m\\\\u001b[38;5;66;43;03m# Process 16 samples at a time \\\\u001b[39;49;00m\\\\n\\\\u001b[0;32m      5\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mfn_kwargs\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43m{\\\\u001b[49m\\\\u001b[38;5;124;43m\\\\\"\\\\u001b[39;49m\\\\u001b[38;5;124;43mfeature_extractor\\\\u001b[39;49m\\\\u001b[38;5;124;43m\\\\\"\\\\u001b[39;49m\\\\u001b[43m:\\\\u001b[49m\\\\u001b[43m \\\\u001b[49m\\\\u001b[43mfeature_extractor\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\u001b[43m \\\\u001b[49m\\\\u001b[38;5;124;43m\\\\\"\\\\u001b[39;49m\\\\u001b[38;5;124;43mtokenizer\\\\u001b[39;49m\\\\u001b[38;5;124;43m\\\\\"\\\\u001b[39;49m\\\\u001b[43m:\\\\u001b[49m\\\\u001b[43m \\\\u001b[49m\\\\u001b[43mtokenizer\\\\u001b[49m\\\\u001b[43m}\\\\u001b[49m\\\\n\\\\u001b[0;32m      6\\\\u001b[0m \\\\u001b[43m)\\\\u001b[49m\\\\n\",\\n      \"File \\\\u001b[1;32m~\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\gpu\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\datasets\\\\\\\\dataset_dict.py:944\\\\u001b[0m, in \\\\u001b[0;36mDatasetDict.map\\\\u001b[1;34m(self, function, with_indices, with_rank, with_split, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc, try_original_type)\\\\u001b[0m\\\\n\\\\u001b[0;32m    941\\\\u001b[0m \\\\u001b[38;5;28;01mif\\\\u001b[39;00m with_split:\\\\n\\\\u001b[0;32m    942\\\\u001b[0m     function \\\\u001b[38;5;241m=\\\\u001b[39m bind(function, split)\\\\n\\\\u001b[1;32m--> 944\\\\u001b[0m dataset_dict[split] \\\\u001b[38;5;241m=\\\\u001b[39m \\\\u001b[43mdataset\\\\u001b[49m\\\\u001b[38;5;241;43m.\\\\u001b[39;49m\\\\u001b[43mmap\\\\u001b[49m\\\\u001b[43m(\\\\u001b[49m\\\\n\\\\u001b[0;32m    945\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mfunction\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mfunction\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    946\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mwith_indices\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mwith_indices\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    947\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mwith_rank\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mwith_rank\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    948\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43minput_columns\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43minput_columns\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    949\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mbatched\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mbatched\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    950\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mbatch_size\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mbatch_size\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    951\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mdrop_last_batch\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mdrop_last_batch\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    952\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mremove_columns\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mremove_columns\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    953\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mkeep_in_memory\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mkeep_in_memory\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    954\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mload_from_cache_file\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mload_from_cache_file\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    955\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mcache_file_name\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mcache_file_names\\\\u001b[49m\\\\u001b[43m[\\\\u001b[49m\\\\u001b[43msplit\\\\u001b[49m\\\\u001b[43m]\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    956\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mwriter_batch_size\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mwriter_batch_size\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    957\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mfeatures\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mfeatures\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    958\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mdisable_nullable\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mdisable_nullable\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    959\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mfn_kwargs\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mfn_kwargs\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    960\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mnum_proc\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mnum_proc\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    961\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mdesc\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mdesc\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    962\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mtry_original_type\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mtry_original_type\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    963\\\\u001b[0m \\\\u001b[43m\\\\u001b[49m\\\\u001b[43m)\\\\u001b[49m\\\\n\\\\u001b[0;32m    965\\\\u001b[0m \\\\u001b[38;5;28;01mif\\\\u001b[39;00m with_split:\\\\n\\\\u001b[0;32m    966\\\\u001b[0m     function \\\\u001b[38;5;241m=\\\\u001b[39m function\\\\u001b[38;5;241m.\\\\u001b[39mfunc\\\\n\",\\n      \"File \\\\u001b[1;32m~\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\gpu\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\datasets\\\\\\\\arrow_dataset.py:557\\\\u001b[0m, in \\\\u001b[0;36mtransmit_format.<locals>.wrapper\\\\u001b[1;34m(*args, **kwargs)\\\\u001b[0m\\\\n\\\\u001b[0;32m    550\\\\u001b[0m self_format \\\\u001b[38;5;241m=\\\\u001b[39m {\\\\n\\\\u001b[0;32m    551\\\\u001b[0m     \\\\u001b[38;5;124m\\\\\"\\\\u001b[39m\\\\u001b[38;5;124mtype\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m: \\\\u001b[38;5;28mself\\\\u001b[39m\\\\u001b[38;5;241m.\\\\u001b[39m_format_type,\\\\n\\\\u001b[0;32m    552\\\\u001b[0m     \\\\u001b[38;5;124m\\\\\"\\\\u001b[39m\\\\u001b[38;5;124mformat_kwargs\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m: \\\\u001b[38;5;28mself\\\\u001b[39m\\\\u001b[38;5;241m.\\\\u001b[39m_format_kwargs,\\\\n\\\\u001b[0;32m    553\\\\u001b[0m     \\\\u001b[38;5;124m\\\\\"\\\\u001b[39m\\\\u001b[38;5;124mcolumns\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m: \\\\u001b[38;5;28mself\\\\u001b[39m\\\\u001b[38;5;241m.\\\\u001b[39m_format_columns,\\\\n\\\\u001b[0;32m    554\\\\u001b[0m     \\\\u001b[38;5;124m\\\\\"\\\\u001b[39m\\\\u001b[38;5;124moutput_all_columns\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m: \\\\u001b[38;5;28mself\\\\u001b[39m\\\\u001b[38;5;241m.\\\\u001b[39m_output_all_columns,\\\\n\\\\u001b[0;32m    555\\\\u001b[0m }\\\\n\\\\u001b[0;32m    556\\\\u001b[0m \\\\u001b[38;5;66;03m# apply actual function\\\\u001b[39;00m\\\\n\\\\u001b[1;32m--> 557\\\\u001b[0m out: Union[\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m\\\\u001b[38;5;124mDataset\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m, \\\\u001b[38;5;124m\\\\\"\\\\u001b[39m\\\\u001b[38;5;124mDatasetDict\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m] \\\\u001b[38;5;241m=\\\\u001b[39m func(\\\\u001b[38;5;28mself\\\\u001b[39m, \\\\u001b[38;5;241m*\\\\u001b[39margs, \\\\u001b[38;5;241m*\\\\u001b[39m\\\\u001b[38;5;241m*\\\\u001b[39mkwargs)\\\\n\\\\u001b[0;32m    558\\\\u001b[0m datasets: \\\\u001b[38;5;28mlist\\\\u001b[39m[\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m\\\\u001b[38;5;124mDataset\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m] \\\\u001b[38;5;241m=\\\\u001b[39m \\\\u001b[38;5;28mlist\\\\u001b[39m(out\\\\u001b[38;5;241m.\\\\u001b[39mvalues()) \\\\u001b[38;5;28;01mif\\\\u001b[39;00m \\\\u001b[38;5;28misinstance\\\\u001b[39m(out, \\\\u001b[38;5;28mdict\\\\u001b[39m) \\\\u001b[38;5;28;01melse\\\\u001b[39;00m [out]\\\\n\\\\u001b[0;32m    559\\\\u001b[0m \\\\u001b[38;5;66;03m# re-apply format to the output\\\\u001b[39;00m\\\\n\",\\n      \"File \\\\u001b[1;32m~\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\gpu\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\datasets\\\\\\\\arrow_dataset.py:3171\\\\u001b[0m, in \\\\u001b[0;36mDataset.map\\\\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\\\\u001b[0m\\\\n\\\\u001b[0;32m   3165\\\\u001b[0m logger\\\\u001b[38;5;241m.\\\\u001b[39minfo(\\\\u001b[38;5;124mf\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m\\\\u001b[38;5;124mSpawning \\\\u001b[39m\\\\u001b[38;5;132;01m{\\\\u001b[39;00mnum_proc\\\\u001b[38;5;132;01m}\\\\u001b[39;00m\\\\u001b[38;5;124m processes\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m)\\\\n\\\\u001b[0;32m   3166\\\\u001b[0m \\\\u001b[38;5;28;01mwith\\\\u001b[39;00m hf_tqdm(\\\\n\\\\u001b[0;32m   3167\\\\u001b[0m     unit\\\\u001b[38;5;241m=\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m\\\\u001b[38;5;124m examples\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m,\\\\n\\\\u001b[0;32m   3168\\\\u001b[0m     total\\\\u001b[38;5;241m=\\\\u001b[39mpbar_total,\\\\n\\\\u001b[0;32m   3169\\\\u001b[0m     desc\\\\u001b[38;5;241m=\\\\u001b[39m(desc \\\\u001b[38;5;129;01mor\\\\u001b[39;00m \\\\u001b[38;5;124m\\\\\"\\\\u001b[39m\\\\u001b[38;5;124mMap\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m) \\\\u001b[38;5;241m+\\\\u001b[39m \\\\u001b[38;5;124mf\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m\\\\u001b[38;5;124m (num_proc=\\\\u001b[39m\\\\u001b[38;5;132;01m{\\\\u001b[39;00mnum_proc\\\\u001b[38;5;132;01m}\\\\u001b[39;00m\\\\u001b[38;5;124m)\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m,\\\\n\\\\u001b[0;32m   3170\\\\u001b[0m ) \\\\u001b[38;5;28;01mas\\\\u001b[39;00m pbar:\\\\n\\\\u001b[1;32m-> 3171\\\\u001b[0m     \\\\u001b[38;5;28;01mfor\\\\u001b[39;00m rank, done, content \\\\u001b[38;5;129;01min\\\\u001b[39;00m iflatmap_unordered(\\\\n\\\\u001b[0;32m   3172\\\\u001b[0m         pool, Dataset\\\\u001b[38;5;241m.\\\\u001b[39m_map_single, kwargs_iterable\\\\u001b[38;5;241m=\\\\u001b[39mkwargs_per_job\\\\n\\\\u001b[0;32m   3173\\\\u001b[0m     ):\\\\n\\\\u001b[0;32m   3174\\\\u001b[0m         \\\\u001b[38;5;28;01mif\\\\u001b[39;00m done:\\\\n\\\\u001b[0;32m   3175\\\\u001b[0m             shards_done \\\\u001b[38;5;241m+\\\\u001b[39m\\\\u001b[38;5;241m=\\\\u001b[39m \\\\u001b[38;5;241m1\\\\u001b[39m\\\\n\",\\n      \"File \\\\u001b[1;32m~\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\gpu\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\datasets\\\\\\\\utils\\\\\\\\py_utils.py:728\\\\u001b[0m, in \\\\u001b[0;36miflatmap_unordered\\\\u001b[1;34m(pool, func, kwargs_iterable)\\\\u001b[0m\\\\n\\\\u001b[0;32m    725\\\\u001b[0m \\\\u001b[38;5;28;01mfinally\\\\u001b[39;00m:\\\\n\\\\u001b[0;32m    726\\\\u001b[0m     \\\\u001b[38;5;28;01mif\\\\u001b[39;00m \\\\u001b[38;5;129;01mnot\\\\u001b[39;00m pool_changed:\\\\n\\\\u001b[0;32m    727\\\\u001b[0m         \\\\u001b[38;5;66;03m# we get the result in case there\\'s an error to raise\\\\u001b[39;00m\\\\n\\\\u001b[1;32m--> 728\\\\u001b[0m         [async_result\\\\u001b[38;5;241m.\\\\u001b[39mget(timeout\\\\u001b[38;5;241m=\\\\u001b[39m\\\\u001b[38;5;241m0.05\\\\u001b[39m) \\\\u001b[38;5;28;01mfor\\\\u001b[39;00m async_result \\\\u001b[38;5;129;01min\\\\u001b[39;00m async_results]\\\\n\",\\n      \"File \\\\u001b[1;32m~\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\gpu\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\datasets\\\\\\\\utils\\\\\\\\py_utils.py:728\\\\u001b[0m, in \\\\u001b[0;36m<listcomp>\\\\u001b[1;34m(.0)\\\\u001b[0m\\\\n\\\\u001b[0;32m    725\\\\u001b[0m \\\\u001b[38;5;28;01mfinally\\\\u001b[39;00m:\\\\n\\\\u001b[0;32m    726\\\\u001b[0m     \\\\u001b[38;5;28;01mif\\\\u001b[39;00m \\\\u001b[38;5;129;01mnot\\\\u001b[39;00m pool_changed:\\\\n\\\\u001b[0;32m    727\\\\u001b[0m         \\\\u001b[38;5;66;03m# we get the result in case there\\'s an error to raise\\\\u001b[39;00m\\\\n\\\\u001b[1;32m--> 728\\\\u001b[0m         [\\\\u001b[43masync_result\\\\u001b[49m\\\\u001b[38;5;241;43m.\\\\u001b[39;49m\\\\u001b[43mget\\\\u001b[49m\\\\u001b[43m(\\\\u001b[49m\\\\u001b[43mtimeout\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[38;5;241;43m0.05\\\\u001b[39;49m\\\\u001b[43m)\\\\u001b[49m \\\\u001b[38;5;28;01mfor\\\\u001b[39;00m async_result \\\\u001b[38;5;129;01min\\\\u001b[39;00m async_results]\\\\n\",\\n      \"File \\\\u001b[1;32m~\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\gpu\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\multiprocess\\\\\\\\pool.py:771\\\\u001b[0m, in \\\\u001b[0;36mApplyResult.get\\\\u001b[1;34m(self, timeout)\\\\u001b[0m\\\\n\\\\u001b[0;32m    769\\\\u001b[0m     \\\\u001b[38;5;28;01mreturn\\\\u001b[39;00m \\\\u001b[38;5;28mself\\\\u001b[39m\\\\u001b[38;5;241m.\\\\u001b[39m_value\\\\n\\\\u001b[0;32m    770\\\\u001b[0m \\\\u001b[38;5;28;01melse\\\\u001b[39;00m:\\\\n\\\\u001b[1;32m--> 771\\\\u001b[0m     \\\\u001b[38;5;28;01mraise\\\\u001b[39;00m \\\\u001b[38;5;28mself\\\\u001b[39m\\\\u001b[38;5;241m.\\\\u001b[39m_value\\\\n\",\\n      \"\\\\u001b[1;31mAttributeError\\\\u001b[0m: \\'numpy.ndarray\\' object has no attribute \\'input_features\\'\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"ds_fleur = ds_fleur.map(\\\\n\",\\n    \"    prepare_dataset,\\\\n\",\\n    \"    remove_columns=ds_fleur.column_names[\\\\\"train\\\\\"],\\\\n\",\\n    \"    num_proc=4,  # Process 16 samples at a time \\\\n\",\\n    \"    fn_kwargs={\\\\\"feature_extractor\\\\\": feature_extractor, \\\\\"tokenizer\\\\\": tokenizer}\\\\n\",\\n    \")\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from transformers import WhisperForConditionalGeneration\\\\n\",\\n    \"\\\\n\",\\n    \"# gen text conditioned on an input(audio data)\\\\n\",\\n    \"model = WhisperForConditionalGeneration.from_pretrained(\\\\\"openai/whisper-small\\\\\")\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"model.generation_config.language = \\\\\"spanish\\\\\"\\\\n\",\\n    \"model.generation_config.task = \\\\\"transcribe\\\\\"\\\\n\",\\n    \"\\\\n\",\\n    \"model.config.pad_token_id = model.config.eos_token_id\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"import torch\\\\n\",\\n    \"\\\\n\",\\n    \"from dataclasses import dataclass\\\\n\",\\n    \"from typing import Any, Dict, List, Union\\\\n\",\\n    \"\\\\n\",\\n    \"@dataclass \\\\n\",\\n    \"class DataCollatorSpeechSeq2SeqWithPadding:\\\\n\",\\n    \"    processor: Any\\\\n\",\\n    \"    decoder_start_token_id: int\\\\n\",\\n    \"    # called when collator is used to batch samples together\\\\n\",\\n    \"    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\\\\n\",\\n    \"        \\\\n\",\\n    \"        # Extract audio features from each sample and pad them to same length\\\\n\",\\n    \"        input_features = [{\\\\\"input_features\\\\\": feature[\\\\\"input_features\\\\\"]} for feature in features]\\\\n\",\\n    \"        #uses processor  built earlier that can call extractor and tokenizer\\\\n\",\\n    \"        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\\\\\"pt\\\\\")\\\\n\",\\n    \"\\\\n\",\\n    \"        # Extract text labels from each sample and pad them to same length\\\\n\",\\n    \"        label_features = [{\\\\\"input_ids\\\\\": feature[\\\\\"labels\\\\\"]} for feature in features]\\\\n\",\\n    \"        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\\\\\"pt\\\\\")\\\\n\",\\n    \"\\\\n\",\\n    \"        # Replace padding tokens with -100 (which is ignored in loss calculation)\\\\n\",\\n    \"        #when labels attention mask does NotEqual (ne) 1, then returns True. When this is true, meaning padding token,\\\\n\",\\n    \"        #then it will replace it with a -100\\\\n\",\\n    \"        labels = labels_batch[\\\\\"input_ids\\\\\"].masked_fill(labels_batch.attention_mask.ne(1), -100) \\\\n\",\\n    \"\\\\n\",\\n    \"        # Remove the start token if it was added during tokenization\\\\n\",\\n    \"        # (it will be added again during generation)\\\\n\",\\n    \"        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\\\\n\",\\n    \"            labels = labels[:, 1:]\\\\n\",\\n    \"\\\\n\",\\n    \"        batch[\\\\\"labels\\\\\"] = labels\\\\n\",\\n    \"\\\\n\",\\n    \"        #batch with audio feats and labels\\\\n\",\\n    \"        return batch\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"#initialize collator\\\\n\",\\n    \"data_collator = DataCollatorSpeechSeq2SeqWithPadding(\\\\n\",\\n    \"    processor=processor,\\\\n\",\\n    \"    decoder_start_token_id=model.config.decoder_start_token_id,\\\\n\",\\n    \")\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"import evaluate\\\\n\",\\n    \"\\\\n\",\\n    \"metric = evaluate.load(\\\\\"wer\\\\\")\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"import numpy as np\\\\n\",\\n    \"\\'\\'\\'\\\\n\",\\n    \"- replaces -100 with `pad_token_id` in `label_ids`\\\\n\",\\n    \"    - then decodes predicted and label ids to string\\\\n\",\\n    \"    - Computes WER\\\\n\",\\n    \"\\'\\'\\'\\\\n\",\\n    \"\\\\n\",\\n    \"def compute_metrics(pred):\\\\n\",\\n    \"    pred_ids = pred.predictions\\\\n\",\\n    \"    label_ids = np.array(pred.label_ids)\\\\n\",\\n    \"\\\\n\",\\n    \"    # replace -100 with the pad_token_id\\\\n\",\\n    \"    label_ids[label_ids == -100] = tokenizer.pad_token_id\\\\n\",\\n    \"\\\\n\",\\n    \"    # we do not want to group tokens when computing the metrics\\\\n\",\\n    \"    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\\\\n\",\\n    \"    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\\\\n\",\\n    \"\\\\n\",\\n    \"    #two different lines because compute returns dict[Unknown, Unknown] or None. So, first need to get wer val to * by 100\\\\n\",\\n    \"    #but before run won\\'t let multiply by None, so need to check if None or a real value\\\\n\",\\n    \"    wer = metric.compute(predictions=pred_str, references=label_str)\\\\n\",\\n    \"\\\\n\",\\n    \"    return {\\\\\"wer\\\\\": 100 * wer}\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from transformers.training_args_seq2seq import Seq2SeqTrainingArguments\\\\n\",\\n    \"\\\\n\",\\n    \"#define arguments\\\\n\",\\n    \"training_args = Seq2SeqTrainingArguments(\\\\n\",\\n    \"    output_dir=\\\\\"./whisper-small-es\\\\\",  # change to a repo name of your choice\\\\n\",\\n    \"    run_name=\\\\\"whisper-spanish-training\\\\\",\\\\n\",\\n    \"    per_device_train_batch_size=16,\\\\n\",\\n    \"    gradient_accumulation_steps=2,  # increase by 2x for every 2x decrease in batch size\\\\n\",\\n    \"    learning_rate=1e-5,\\\\n\",\\n    \"    warmup_steps=200,\\\\n\",\\n    \"    max_steps=2500,\\\\n\",\\n    \"    gradient_checkpointing=True,\\\\n\",\\n    \"    fp16=True,\\\\n\",\\n    \"    per_device_eval_batch_size=8,\\\\n\",\\n    \"    predict_with_generate=True,\\\\n\",\\n    \"    generation_max_length=225,\\\\n\",\\n    \"    save_steps=500,\\\\n\",\\n    \"    eval_steps=500,\\\\n\",\\n    \"    logging_steps=50,\\\\n\",\\n    \"    report_to=[\\\\\"tensorboard\\\\\",\\\\\"wandb\\\\\"],\\\\n\",\\n    \"    load_best_model_at_end=True,\\\\n\",\\n    \"    metric_for_best_model=\\\\\"wer\\\\\",\\\\n\",\\n    \"    greater_is_better=False,\\\\n\",\\n    \"    push_to_hub=True,\\\\n\",\\n    \"    eval_strategy=\\\\\"steps\\\\\",\\\\n\",\\n    \"    save_strategy=\\\\\"steps\\\\\"\\\\n\",\\n    \")\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"# Clear memory before training\\\\n\",\\n    \"clear_gpu_memory()\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"import torch\\\\n\",\\n    \"import gc\\\\n\",\\n    \"\\'\\'\\'\\\\n\",\\n    \"# Clear GPU memory\\\\n\",\\n    \"gc.collect()\\\\n\",\\n    \"torch.cuda.empty_cache()\\\\n\",\\n    \"torch.cuda.reset_peak_memory_stats()\\\\n\",\\n    \"\\'\\'\\'\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from transformers.trainer_seq2seq import Seq2SeqTrainer\\\\n\",\\n    \"from torchdata.stateful_dataloader import StatefulDataLoader\\\\n\",\\n    \"\\\\n\",\\n    \"trainer = Seq2SeqTrainer(\\\\n\",\\n    \"    args=training_args,\\\\n\",\\n    \"    model=model,\\\\n\",\\n    \"    train_dataset=ds_fleur[\\\\\"train\\\\\"],\\\\n\",\\n    \"    eval_dataset=ds_fleur[\\\\\"test\\\\\"],\\\\n\",\\n    \"    data_collator=data_collator,\\\\n\",\\n    \"    compute_metrics=compute_metrics\\\\n\",\\n    \")\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from dotenv import load_dotenv\\\\n\",\\n    \"import os\\\\n\",\\n    \"\\\\n\",\\n    \"load_dotenv(\\\\\"HF.config\\\\\")\\\\n\",\\n    \"wandb_key = os.getenv(\\\\\"WANDB_API_KEY\\\\\")\\\\n\",\\n    \"wandb_nb_name = os.getenv(\\\\\"WANDB_NOTEBOOK_NAME\\\\\")\\\\n\",\\n    \"\\\\n\",\\n    \"trainer.train()\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": []\\n  }\\n ],\\n \"metadata\": {\\n  \"kernelspec\": {\\n   \"display_name\": \"Python [conda env:gpu] *\",\\n   \"language\": \"python\",\\n   \"name\": \"conda-env-gpu-py\"\\n  },\\n  \"language_info\": {\\n   \"codemirror_mode\": {\\n    \"name\": \"ipython\",\\n    \"version\": 3\\n   },\\n   \"file_extension\": \".py\",\\n   \"mimetype\": \"text/x-python\",\\n   \"name\": \"python\",\\n   \"nbconvert_exporter\": \"python\",\\n   \"pygments_lexer\": \"ipython3\",\\n   \"version\": \"3.9.18\"\\n  }\\n },\\n \"nbformat\": 4,\\n \"nbformat_minor\": 4\\n}\\n', 'side_proj/src/spanish_chat_bot/audio/recorder.py': '\"\"\"\\nAudio recorder.\\n\"\"\"\\nimport sounddevice as sd\\nimport numpy as np\\nfrom scipy.signal import butter, filtfilt\\n\\n\\ndef record_audio(duration=5, sample_rate=16000, cutoff_freq=100, filter_order=4):\\n    \"\"\"\\n    Record audio for set duration and process it\\n    \\n    Args:\\n        duration (int): Amount of time recording user audio in seconds\\n        sample rate (int): sample rate to record audio in Hz\\n        cutoff_freq (int): Cutoff frequency for high-pass filter in Hz\\n        filter_order (int): Order of the Butterworth filter\\n    Returns:\\n        numpy.ndarray: Processed audio signal normalized to [-1, 1]\\n        \\n    \"\"\"\\n    try:\\n        myrecording = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1)\\n        sd.wait()\\n        \\n        #convert to single array, then normalize audio\\n        audio = myrecording.flatten()\\n        audio = audio / np.max(np.abs(audio))\\n        \\n        # apply filter to reduce noise\\n        nyquist = sample_rate / 2 #nyquist rate is 1/2 of the frequency\\n        \\n        #filter_order: higher order = sharper cutoff\\n        #cutoff_freq/nyquist normalizes the cutoff frequency\\n        #btype = high pass filter\\n        b, a = butter(filter_order, cutoff_freq/nyquist, btype=\\'high\\')\\n        audio = filtfilt(b, a, audio)\\n        \\n        #normalize again\\n        audio = audio / np.max(np.abs(audio))\\n        return audio\\n        \\n    except Exception as e:\\n        raise RuntimeError(f\"Failed to record audio: {str(e)}\")', 'side_proj/src/spanish_chat_bot/audio/tts.py': '\\'\\'\\' \\nConverts text response to audio\\n\\'\\'\\'\\nimport sounddevice as sd\\nfrom scipy.io import wavfile\\nfrom TTS.api import TTS\\nfrom sympy import false\\n\\n\\ndef text_to_speech(text, output_path=\"output.wav\"):\\n    \"\"\"\\n    Take text input and convert it to wav file\\n    \\n    Args:\\n        text (string): Generated text from response generator\\n        output_path (string): path for wav file output\\n    Returns:\\n        string: path to wav file of response\\n    \"\"\"\\n    \\n    try: \\n    # Initialize TTS with Spanish xtts model\\n        tts = TTS(model_name=\"tts_models/es/css10/vits\", progress_bar=False)\\n        \\n        # Generate speech\\n        tts.tts_to_file(text=text, file_path=output_path)\\n        \\n        # Play the audio\\n        sample_rate, audio = wavfile.read(output_path)\\n        sd.play(audio, sample_rate)\\n        sd.wait()\\n        \\n        return output_path\\n    \\n    except Exception as e:\\n        raise RuntimeError(f\"Failed to convert text to audio: {str(e)}\")\\n        ', 'side_proj/src/spanish_chat_bot/models/response_generator.py': '\\'\\'\\'\\nGenerate text response\\n\\'\\'\\'\\nfrom dotenv import load_dotenv\\nload_dotenv(\"tokens.env\")\\nimport os\\nfrom typing import Optional, List\\nimport google.generativeai as gem\\nfrom google.api_core import exceptions as google_exceptions\\n\\nclass ResponseGenerator:\\n    def __init__(self, model_name: str = \"gemini-2.0-flash\", api_key: Optional[str] = None):\\n        \\'\\'\\'\\n        Initialize generator with model.\\n        \\n        Args:\\n            model_name (string): name of generative model to use\\n            api_key (str, optional): API key for model use if necessary. If not provided, will look for in env\\n        \\'\\'\\'\\n        self.api_key = api_key or os.getenv(\"GEMINI_API_KEY\")\\n        if not self.api_key:\\n            raise ValueError(\"Gemini API key not found. Please provide it or set GEMINI_API_KEY environment variable.\")\\n            \\n        # Configure the API\\n        gem.configure(api_key=self.api_key)\\n        \\n        # Initialize the model\\n        self.model = gem.GenerativeModel(model_name)\\n        \\n        # Optional: Store conversation history\\n        self.conversation_history: List[dict] = []\\n\\n    def generate(self, text: str) -> str:\\n        \"\"\"\\n        Generate a response to the user\\'s speech\\n        \\n        Args:\\n            text (str): user\\'s speech input converted to text\\n        Returns:\\n            str: Generated text\\n        Raises:\\n            RuntimeError: If there\\'s an error generating the response\\n        \"\"\"\\n        try:\\n            response = self.model.generate_content(\\n                    f\"Eres un asistente conversacional en espaÃ±ol. Adaptarse al contexto de la conversaciÃ³n. \"\\n                    f\"(Por ejemplo: si la persona te diga algo como si fuera tu amigo o un familiar, respondele con ese rol.)\"\\n                    f\"Tu no eres un chatbot, eres la persona.\"\\n                    f\"No usa los emojis\"\\n                    f\"Mantenga la duraciÃ³n de tu respuesta corta. Responde de manera natural y conversacional a: {text}\"\\n                )\\n            \\n            if not response or not response.text:\\n                raise RuntimeError(\"Received empty response from model\")\\n                \\n            return response.text\\n            \\n        except google_exceptions.ResourceExhausted:\\n            raise RuntimeError(\"API rate limit exceeded. Please try again later.\")\\n        except google_exceptions.ServiceUnavailable:\\n            raise RuntimeError(\"Gemini API service is currently unavailable. Please try again later.\")\\n        except google_exceptions.InvalidArgument as e:\\n            raise RuntimeError(f\"Invalid input to model: {str(e)}\")\\n        except Exception as e:\\n            raise RuntimeError(f\"Failed to generate response: {str(e)}\")\\n\\n\\n# Create a singleton instance\\n_generator: Optional[ResponseGenerator] = None\\n\\ndef generate_response(text: str, model_name: str = \"gemini-2.0-flash\", api_key: Optional[str] = None) -> str:\\n    \"\"\"\\n    Global function to generate responses using the singleton generator.\\n    \\n    Args:\\n        text (str): Text to generate response for\\n        model_name (str): Name of the model to use\\n        api_key (str, optional): API key for the model\\n        \\n    Returns:\\n        str: Generated response\\n    \"\"\"\\n    global _generator\\n    if _generator is None:\\n        _generator = ResponseGenerator(model_name=model_name, api_key=api_key)\\n    return _generator.generate(text)', 'side_proj/src/spanish_chat_bot/models/transcriber.py': '\"\"\"\\nAudio transcription functionality using Whisper model.\\n\"\"\"\\n\\nimport torch\\nfrom transformers import (\\n    WhisperProcessor,\\n    WhisperForConditionalGeneration,\\n    WhisperFeatureExtractor,\\n    WhisperTokenizer\\n)\\nfrom typing import Optional, Union\\nimport numpy as np\\n\\n\\nclass Transcriber:\\n    def __init__(self, model_name: str = \"openai/whisper-small\", device: Optional[str] = None):\\n        \"\"\"\\n        Initialize the transcriber with Whisper model.\\n        \\n        Args:\\n            model_name (str): Name of the Whisper model to use\\n            device (str, optional): Device to run the model on (\\'cuda\\' or \\'cpu\\')\\n        \"\"\"\\n        # looks for cuda device to run model on\\n        if device is None:\\n            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\\n            \\n        self.device = device\\n        self.feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name)\\n        self.tokenizer = WhisperTokenizer.from_pretrained(model_name)\\n        self.model = WhisperForConditionalGeneration.from_pretrained(model_name).to(self.device)\\n        \\n        # Configure model for Spanish + transcription\\n        self.model.config.forced_decoder_ids = self.tokenizer.get_decoder_prompt_ids(\\n            language=\"spanish\",\\n            task=\"transcribe\"\\n        )\\n\\n    def transcribe(self, audio: np.ndarray, sample_rate: int = 16000) -> str:\\n        \"\"\"\\n        Transcribe audio to text.\\n        \\n        Args:\\n            audio (numpy.ndarray): Audio array\\n            sample_rate (int): Audio sample rate\\n            \\n        Returns:\\n            str: Transcribed text\\n        \"\"\"\\n        #process audio to tensors\\n        inputs = self.feature_extractor(\\n            audio,\\n            sampling_rate=sample_rate,\\n            return_tensors=\"pt\"\\n        )\\n        input_features = inputs.input_features.to(self.device)\\n        \\n        # Generate transcription\\n        predicted_ids = self.model.generate(\\n            input_features,\\n            max_length=448,\\n            num_beams=5, \\n            temperature=0.7\\n        )\\n        transcription = self.tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)[0]\\n        \\n        return transcription\\n\\n\\n# Create a singleton instance, more than one transcriber != good\\n_transcriber: Optional[Transcriber] = None\\n\\ndef transcribe_audio(audio: np.ndarray, sample_rate: int = 16000, model_name: str = \"openai/whisper-small\") -> str:\\n    \"\"\"\\n    Global function to transcribe audio using the singleton transcriber.\\n    \\n    Args:\\n        audio (numpy.ndarray): Audio array\\n        sample_rate (int): Audio sample rate\\n        model_name (str): Name of the Whisper model to use\\n        \\n    Returns:\\n        str: Transcribed text\\n    \"\"\"\\n    global _transcriber\\n    if _transcriber is None:\\n        _transcriber = Transcriber(model_name=model_name)\\n    return _transcriber.transcribe(audio, sample_rate) ', 'side_proj/frontend/package.json': '{\\n  \"name\": \"frontend\",\\n  \"private\": true,\\n  \"version\": \"0.0.0\",\\n  \"type\": \"module\",\\n  \"scripts\": {\\n    \"dev\": \"vite\",\\n    \"build\": \"vite build\",\\n    \"lint\": \"eslint .\",\\n    \"preview\": \"vite preview\"\\n  },\\n  \"dependencies\": {\\n    \"@heroicons/react\": \"^2.2.0\",\\n    \"axios\": \"^1.9.0\",\\n    \"react\": \"^19.1.0\",\\n    \"react-dom\": \"^19.1.0\"\\n  },\\n  \"devDependencies\": {\\n    \"@eslint/js\": \"^9.25.0\",\\n    \"@tailwindcss/postcss\": \"^4.1.8\",\\n    \"@types/react\": \"^19.1.2\",\\n    \"@types/react-dom\": \"^19.1.2\",\\n    \"@vitejs/plugin-react\": \"^4.4.1\",\\n    \"autoprefixer\": \"^10.4.21\",\\n    \"eslint\": \"^9.25.0\",\\n    \"eslint-plugin-react-hooks\": \"^5.2.0\",\\n    \"eslint-plugin-react-refresh\": \"^0.4.19\",\\n    \"globals\": \"^16.0.0\",\\n    \"postcss\": \"^8.5.4\",\\n    \"tailwindcss\": \"^4.1.8\",\\n    \"vite\": \"^6.3.5\"\\n  }\\n}\\n', 'side_proj/frontend/vite.config.js': \"import { defineConfig } from 'vite'\\nimport react from '@vitejs/plugin-react'\\n\\n// https://vite.dev/config/\\nexport default defineConfig({\\n  plugins: [react()],\\n  css: {\\n    postcss: './postcss.config.js',\\n  },\\n  server: {\\n    proxy: {\\n      '/api': 'http://localhost:8000'\\n    }\\n  }\\n})\\n\"}}\n",
      "Tool: Fetching content for 12 files...\n",
      "Tool: Finished fetching all file contents.\n",
      "  ðŸ“¦ Got Tool Response from 'fetch_all_content': {'status': 'success', 'files_fetched': 12}...\n",
      "  ðŸ’¾ State Change: {'all_file_contents': {'side_proj/requirements.txt': 'torch>=2.0.0\\ntransformers>=4.30.0\\nsounddevice>=0.4.6\\nnumpy>=1.24.0\\nscipy>=1.10.0\\npython-dotenv>=1.0.0\\ngoogle-generativeai>=0.3.0\\nTTS>=0.17.0\\ndatasets>=2.12.0\\nevaluate>=0.4.0\\nwandb>=0.15.0\\nfastapi>=0.104.0\\nuvicorn>=0.24.0 ', 'side_proj/src/spanish_chat_bot/conversation.py': '\"\"\"\\nMain conversation handler that orchestrates the Spanish chat bot.\\n\"\"\"\\nfrom typing import Optional\\nimport time\\nfrom .audio import record_audio, text_to_speech\\nfrom .models import transcribe_audio, generate_response\\n\\nclass Conversation:\\n    def __init__(self):\\n        \"\"\"Initialize the conversation handler.\"\"\"\\n        self.is_active = False\\n        self.conversation_history = []\\n        self.audio_files = []\\n\\n\\n    def process_audio(self, audio_data):\\n        text = transcribe_audio(audio_data)\\n        \\n        response = generate_response(text)\\n        \\n        audio_path = text_to_speech(response)\\n        \\n        #conversation history\\n        self.conversation_history.append({\\n            \\'user\\': text,\\n            \\'bot\\': response,\\n            \\'audio_path\\': audio_path,\\n            \\'timestamp\\': time.time()\\n        })\\n        \\n        return {\\n            \\'transcription\\': text,\\n            \\'response\\': response,\\n            \\'audio_path\\': audio_path\\n        }\\n    \\n        \\n        \\n    def start(self):\\n        \"\"\"Start the conversation loop.\"\"\"\\n        self.is_active = True\\n        print(\"Â¡Hola! Estoy listo para conversar. Presiona Ctrl+C para salir.\")\\n        \\n        while self.is_active:\\n            try:\\n                #record audio\\n                print(\"\\\\nEscuchando...\")\\n                audio = record_audio()\\n                \\n                #transcribe audio to text\\n                text = transcribe_audio(audio)\\n                #print(f\"Dijiste: {text}? Y/N\")\\n                if not text:\\n                    print(\"No pude entender eso. Â¿PodrÃ­as repetirlo?\")\\n                    continue\\n                \\n                #generate response\\n                response = generate_response(text)\\n                \\n                #Convert response to speech and play it\\n                text_to_speech(response)\\n                \\n                #conversation history\\n                self.conversation_history.append({\\n                    \\'user\\': text,\\n                    \\'bot\\': response,\\n                    \\'timestamp\\': time.time()\\n                })\\n                \\n            except KeyboardInterrupt:\\n                print(\"\\\\nÂ¡Hasta luego!\")\\n                self.is_active = False\\n            except Exception as e:\\n                print(f\"Lo siento, hubo un error: {str(e)}\")\\n                continue\\n            \\n    def stop(self):\\n        \"\"\"Stop the conversation.\"\"\"\\n        self.is_active = False\\n\\n\\ndef main():\\n    \"\"\"Main entry point for the conversation.\"\"\"\\n    conversation = Conversation()\\n    conversation.start()\\n\\n\\nif __name__ == \"__main__\":\\n    main()', 'side_proj/src/spanish_chat_bot/endpoint_handler.py': 'from dotenv import load_dotenv\\nload_dotenv(\"tokens.env\")\\n\\n\\nfrom fastapi import FastAPI, WebSocket, HTTPException\\nfrom pydantic import BaseModel\\nimport numpy as np\\nimport traceback\\nimport logging\\nfrom .audio import record_audio, text_to_speech\\nfrom .models import transcribe_audio, generate_response\\n\\n# Set up logging\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger(__name__)\\n\\napp = FastAPI()\\n\\nclass AudioData(BaseModel):\\n    audioData: list[float]\\n\\nclass TextData(BaseModel):\\n    text: str\\n\\n@app.post(\"/api/start-recording\")\\nasync def start_recording():\\n    try:\\n        audio = record_audio()\\n        return {\"audioData\": audio.tolist()}\\n    except Exception as e:\\n        logger.error(f\"Error in start_recording: {str(e)}\")\\n        logger.error(traceback.format_exc())\\n        raise HTTPException(status_code=500, detail=str(e))\\n\\n@app.post(\"/api/transcribe\")\\nasync def handle_transcription(audio: AudioData):\\n    try:\\n        audio_array = np.array(audio.audioData)\\n        # Get the recorded audio turn into an array and transcribe it\\n        text = transcribe_audio(audio=audio_array)\\n        return {\"transcription\": text}\\n    except Exception as e:\\n        logger.error(f\"Error in handle_transcription: {str(e)}\")\\n        logger.error(traceback.format_exc())\\n        raise HTTPException(status_code=500, detail=str(e))\\n\\n@app.post(\"/api/generate\")\\nasync def handle_generation(text_data: TextData): #since frontend sends in json form, need to introduce it as TextData to validate JSON\\n    try:\\n        logger.info(f\"[testing] Starting generation for text: {text_data.text}\")\\n        \\n        # Generate response and convert to speech\\n        # generate_response expects text and optional model_name and api_key\\n        logger.info(\"[testing] Calling generate_response...\")\\n        response = generate_response(text=text_data.text)\\n        logger.info(f\"[testing] Generated response: {response}\")\\n        \\n        logger.info(\"[testing] Calling text_to_speech...\")\\n        audio_url = text_to_speech(response)\\n        logger.info(f\"[testing] Audio URL: {audio_url}\")\\n        \\n        return {\\n            \"response\": response,\\n            \"audioUrl\": audio_url\\n        }\\n    except Exception as e:\\n        logger.error(f\"Error in handle_generation: {str(e)}\")\\n        logger.error(traceback.format_exc())\\n        raise HTTPException(status_code=500, detail=str(e))\\n\\n@app.websocket(\"/ws\")\\nasync def websocket_endpoint(websocket: WebSocket):\\n    await websocket.accept()\\n    # Handle real-time updates if needed\\n', 'side_proj/frontend/src/App.jsx': 'import { useState } from \\'react\\'\\nimport \\'./App.css\\'\\nimport ChatMessage from \\'./components/Message\\'\\nimport RecordingInterface from \\'./components/RecordingInterface\\'\\n\\nfunction App() {\\n  const [messages, setMessages] = useState([]);\\n\\n  // This function will be called by RecordingInterface\\n  const handleNewMessage = (newMessage) => {\\n    // Add user message to the chat\\n    setMessages(prev => [...prev, { text: newMessage.user, isBot: false }]);\\n    \\n    // Add bot response to the chat\\n    setMessages(prev => [...prev, { text: newMessage.bot, isBot: true }]);\\n  };\\n\\n  //main container that is whole screen, gray bg\\n  return (\\n    <div className=\"min-h-screen bg-gray-100\">\\n      {/*center container with padding, with extra bottom padding to avoid overlap with fixed recorder */}\\n      <div className=\"container mx-auto p-4 pb-40\">\\n        {/*white chat card, takes remaining space, max width*/}\\n        <div className=\"bg-white rounded-lg shadow-lg p-6 max-w-xl mx-auto\">\\n          <h1 className=\"text-2xl font-bold text-center mb-6 text-gray-800\">\\n          {/*Title*/}\\n            Contigo\\n          </h1>\\n          {/*Message area - scrollable, takes remaining space*/}\\n          <div className=\"space-y-4\">\\n            {/*Messages rendered here*/}\\n            {messages.map((msg, index) => (\\n              <ChatMessage \\n                key={index}\\n                message={msg.text}\\n                isBot={msg.isBot}\\n              />\\n            ))}\\n          </div>\\n          {/*recording interface (margin top for separaton)*/}\\n          <div className=\"mt-6\">\\n            <RecordingInterface onNewMessage={handleNewMessage} />\\n          </div>\\n        </div>\\n      </div>\\n    </div>\\n  )\\n}\\n\\nexport default App\\n', 'side_proj/notebooks/inference_pipeline.ipynb': '{\\n \"cells\": [\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"import torch\\\\n\",\\n    \"from transformers import WhisperProcessor, WhisperForConditionalGeneration, WhisperFeatureExtractor, WhisperTokenizer\\\\n\",\\n    \"import os\\\\n\",\\n    \"\\\\n\",\\n    \"import torch\\\\n\",\\n    \"if torch.cuda.is_available():\\\\n\",\\n    \"    device = \\'cuda\\'\\\\n\",\\n    \"    print(f\\'Using GPU: {torch.cuda.get_device_name(0)}\\')\\\\n\",\\n    \"else:\\\\n\",\\n    \"    device = \\'cpu\\'\\\\n\",\\n    \"    print(\\'GPU not available, using CPU.\\')\\\\n\",\\n    \"    \\\\n\",\\n    \"device = torch.device(\\\\\"cuda\\\\\" if torch.cuda.is_available() else \\\\\"cpu\\\\\")\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 3,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"   from dotenv import load_dotenv\\\\n\",\\n    \"   import os\\\\n\",\\n    \"\\\\n\",\\n    \"   load_dotenv(\\\\\"HF.config\\\\\")\\\\n\",\\n    \"   hf_token = os.getenv(\\\\\"HF_TOKEN\\\\\")\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 3,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"#load feature extractor from pre-trained checkpoint\\\\n\",\\n    \"feature_extractor = WhisperFeatureExtractor.from_pretrained(\\\\\"openai/whisper-small\\\\\")\\\\n\",\\n    \"tokenizer = WhisperTokenizer.from_pretrained(\\\\\"openai/whisper-small\\\\\", language=\\\\\"Spanish\\\\\", task=\\\\\"transcribe\\\\\")\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 124,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"processor = WhisperProcessor.from_pretrained(\\\\\"openai/whisper-small\\\\\", language=\\\\\"spanish\\\\\", task=\\\\\"transcribe\\\\\")\\\\n\",\\n    \"og_model = WhisperForConditionalGeneration.from_pretrained(\\\\\"openai/whisper-small\\\\\")\\\\n\",\\n    \"model = WhisperForConditionalGeneration.from_pretrained(\\\\\"mevitts/whisper-small-es\\\\\")\\\\n\",\\n    \"\\\\n\",\\n    \"og_model = og_model.to(device)\\\\n\",\\n    \"model = model.to(device)\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 125,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from transformers import GenerationConfig\\\\n\",\\n    \"\\\\n\",\\n    \"\\\\n\",\\n    \"# Set model configuration\\\\n\",\\n    \"og_model.generation_config = GenerationConfig.from_pretrained(\\\\\"openai/whisper-small\\\\\")\\\\n\",\\n    \"og_model.generation_config.language = \\\\\"spanish\\\\\"\\\\n\",\\n    \"og_model.generation_config.task = \\\\\"transcribe\\\\\"\\\\n\",\\n    \"og_model.config.pad_token_id = og_model.config.eos_token_id\\\\n\",\\n    \"\\\\n\",\\n    \"model.generation_config = GenerationConfig.from_pretrained(\\\\\"openai/whisper-small\\\\\")\\\\n\",\\n    \"model.generation_config.language = \\\\\"spanish\\\\\"\\\\n\",\\n    \"model.generation_config.task = \\\\\"transcribe\\\\\"\\\\n\",\\n    \"model.config.pad_token_id = model.config.eos_token_id\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 121,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"import sounddevice as sd\\\\n\",\\n    \"import numpy as np\\\\n\",\\n    \"from scipy.signal import butter, filtfilt\\\\n\",\\n    \"\\\\n\",\\n    \"\\\\n\",\\n    \"def record_audio(duration=5, sample_rate=16000):\\\\n\",\\n    \"    \\\\\"\\\\\"\\\\\"Record audio for set duration\\\\\"\\\\\"\\\\\"\\\\n\",\\n    \"    print(\\\\\"Say what you want\\\\\")\\\\n\",\\n    \"    myrecording = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1)\\\\n\",\\n    \"    sd.wait()\\\\n\",\\n    \"    \\\\n\",\\n    \"    #convert to single array, then normalize audio\\\\n\",\\n    \"    audio = myrecording.flatten()\\\\n\",\\n    \"    \\\\n\",\\n    \"    audio = audio / np.max(np.abs(audio))\\\\n\",\\n    \"    \\\\n\",\\n    \"    # apply filter to reduce noise\\\\n\",\\n    \"    nyquist = sample_rate / 2\\\\n\",\\n    \"    cutoff = 100 #Hz\\\\n\",\\n    \"    b, a = butter(4, cutoff/nyquist, btype=\\'high\\')\\\\n\",\\n    \"    audio = filtfilt(b, a, audio)\\\\n\",\\n    \"    \\\\n\",\\n    \"    audio = audio / np.max(np.abs(audio))\\\\n\",\\n    \"    return audio\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 127,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"def transcribe_audio(audio, sample_rate=16000, model=og_model):\\\\n\",\\n    \"    \\\\n\",\\n    \"    \\\\n\",\\n    \"    inputs = feature_extractor(\\\\n\",\\n    \"        audio,\\\\n\",\\n    \"        sampling_rate=sample_rate,\\\\n\",\\n    \"        return_tensors=\\\\\"pt\\\\\"\\\\n\",\\n    \"    )\\\\n\",\\n    \"    input_features = inputs.input_features.to(device)\\\\n\",\\n    \"    \\\\n\",\\n    \"    predicted_ids = model.generate(\\\\n\",\\n    \"        input_features,\\\\n\",\\n    \"        max_length=448,  # Increased from default\\\\n\",\\n    \"        num_beams=5,     # Increased from default\\\\n\",\\n    \"        temperature=0.7  # Slightly reduced from default\\\\n\",\\n    \"    )\\\\n\",\\n    \"    transcription = tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)[0]\\\\n\",\\n    \"    \\\\n\",\\n    \"    return transcription\\\\n\",\\n    \"    \"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"rec = record_audio()\\\\n\",\\n    \"msg = transcribe_audio(rec, model=og_model)\\\\n\",\\n    \"matt_msg = transcribe_audio(rec, model=model)\\\\n\",\\n    \"print(msg)\\\\n\",\\n    \"print(matt_msg)\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 157,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"\\\\n\",\\n    \"from dotenv import load_dotenv\\\\n\",\\n    \"import os\\\\n\",\\n    \"import google.generativeai as gem\\\\n\",\\n    \"\\\\n\",\\n    \"load_dotenv(\\\\\"HF.config\\\\\")\\\\n\",\\n    \"google_api_key = os.getenv(\\\\\"GOOGLE_API_KEY\\\\\")\\\\n\",\\n    \"if not google_api_key:\\\\n\",\\n    \"    raise ValueError(\\\\\"GOOGLE_API_KEY not found in environment variables\\\\\")\\\\n\",\\n    \"#genai.configure(api_key=google_api_key)\\\\n\",\\n    \"gem.configure(api_key=google_api_key)\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 163,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"gemini = gem.GenerativeModel(\\'gemini-2.0-flash\\')\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 166,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"def generate_response(text):\\\\n\",\\n    \"    response = gemini.generate_content(\\\\n\",\\n    \"            f\\\\\"Eres un asistente conversacional en espaÃ±ol. Adaptarse al contexto de la conversaciÃ³n. (Por ejemplo: si la persona te diga algo como si fuera tu amigo o un familiar, respondele con ese rol.) Mantenga la duraciÃ³n de tu respuesta corta. Responde de manera natural y conversacional a: {text}\\\\\"\\\\n\",\\n    \"        )\\\\n\",\\n    \"    return response\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"generate_response(\\\\\"Cual es tu comida favorita?\\\\\")\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 160,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"def test_full_pipeline():\\\\n\",\\n    \"    print(\\\\\"Recording audio...\\\\\")\\\\n\",\\n    \"    audio = record_audio()\\\\n\",\\n    \"    \\\\n\",\\n    \"    print(\\\\\"\\\\\\\\nTranscribing audio...\\\\\")\\\\n\",\\n    \"    transcription = transcribe_audio(audio, model=og_model)\\\\n\",\\n    \"    print(f\\\\\"Transcription: {transcription}\\\\\")\\\\n\",\\n    \"    \\\\n\",\\n    \"    print(\\\\\"\\\\\\\\nGenerating response...\\\\\")\\\\n\",\\n    \"    response = generate_response(transcription)\\\\n\",\\n    \"    print(f\\\\\"Response: {response}\\\\\")\\\\n\",\\n    \"    \\\\n\",\\n    \"    return transcription, response\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"transcription, response = test_full_pipeline()\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 10,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"import torch\\\\n\",\\n    \"from transformers import WhisperProcessor, WhisperForConditionalGeneration, WhisperFeatureExtractor, WhisperTokenizer\\\\n\",\\n    \"import os\\\\n\",\\n    \"import sounddevice as sd\\\\n\",\\n    \"import numpy as np\\\\n\",\\n    \"from scipy.io import wavfile\\\\n\",\\n    \"from TTS.api import TTS\\\\n\",\\n    \"\\\\n\",\\n    \"def text_to_speech(text, output_path=\\\\\"output.wav\\\\\"):\\\\n\",\\n    \"    # Initialize TTS with Spanish xtts model\\\\n\",\\n    \"    tts = TTS(model_name=\\\\\"tts_models/es/css10/vits\\\\\", progress_bar=False)\\\\n\",\\n    \"    \\\\n\",\\n    \"    # Generate speech\\\\n\",\\n    \"    tts.tts_to_file(text=text, file_path=output_path)\\\\n\",\\n    \"    \\\\n\",\\n    \"    # Play the audio\\\\n\",\\n    \"    sample_rate, audio = wavfile.read(output_path)\\\\n\",\\n    \"    sd.play(audio, sample_rate)\\\\n\",\\n    \"    sd.wait()\\\\n\",\\n    \"    \\\\n\",\\n    \"    return output_path\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 11,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \" > tts_models/es/css10/vits is already downloaded.\\\\n\",\\n      \" > Using model: vits\\\\n\",\\n      \" > Setting up Audio Processor...\\\\n\",\\n      \" | > sample_rate:22050\\\\n\",\\n      \" | > resample:False\\\\n\",\\n      \" | > num_mels:80\\\\n\",\\n      \" | > log_func:np.log10\\\\n\",\\n      \" | > min_level_db:0\\\\n\",\\n      \" | > frame_shift_ms:None\\\\n\",\\n      \" | > frame_length_ms:None\\\\n\",\\n      \" | > ref_level_db:None\\\\n\",\\n      \" | > fft_size:1024\\\\n\",\\n      \" | > power:None\\\\n\",\\n      \" | > preemphasis:0.0\\\\n\",\\n      \" | > griffin_lim_iters:None\\\\n\",\\n      \" | > signal_norm:None\\\\n\",\\n      \" | > symmetric_norm:None\\\\n\",\\n      \" | > mel_fmin:0\\\\n\",\\n      \" | > mel_fmax:None\\\\n\",\\n      \" | > pitch_fmin:None\\\\n\",\\n      \" | > pitch_fmax:None\\\\n\",\\n      \" | > spec_gain:20.0\\\\n\",\\n      \" | > stft_pad_mode:reflect\\\\n\",\\n      \" | > max_norm:1.0\\\\n\",\\n      \" | > clip_norm:True\\\\n\",\\n      \" | > do_trim_silence:False\\\\n\",\\n      \" | > trim_db:60\\\\n\",\\n      \" | > do_sound_norm:False\\\\n\",\\n      \" | > do_amp_to_db_linear:True\\\\n\",\\n      \" | > do_amp_to_db_mel:True\\\\n\",\\n      \" | > do_rms_norm:False\\\\n\",\\n      \" | > db_level:None\\\\n\",\\n      \" | > stats_path:None\\\\n\",\\n      \" | > base:10\\\\n\",\\n      \" | > hop_length:256\\\\n\",\\n      \" | > win_length:1024\\\\n\",\\n      \" > initialization of speaker-embedding layers.\\\\n\",\\n      \" > initialization of language-embedding layers.\\\\n\",\\n      \" > Text splitted to sentences.\\\\n\",\\n      \"[\\'Mirense a estos pinches gringos!\\']\\\\n\",\\n      \" > Processing time: 0.9494185447692871\\\\n\",\\n      \" > Real-time factor: 0.31086182751489044\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"output_wav = text_to_speech(\\\\\"Mirense a estos pinches gringos!\\\\\")\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 15,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"Available Spanish models:\\\\n\",\\n      \"\\\\n\",\\n      \" Name format: type/language/dataset/model\\\\n\",\\n      \" 1: tts_models/multilingual/multi-dataset/xtts_v2\\\\n\",\\n      \" 2: tts_models/multilingual/multi-dataset/xtts_v1.1\\\\n\",\\n      \" 3: tts_models/multilingual/multi-dataset/your_tts\\\\n\",\\n      \" 4: tts_models/multilingual/multi-dataset/bark\\\\n\",\\n      \" 5: tts_models/bg/cv/vits\\\\n\",\\n      \" 6: tts_models/cs/cv/vits\\\\n\",\\n      \" 7: tts_models/da/cv/vits\\\\n\",\\n      \" 8: tts_models/et/cv/vits\\\\n\",\\n      \" 9: tts_models/ga/cv/vits\\\\n\",\\n      \" 10: tts_models/en/ek1/tacotron2\\\\n\",\\n      \" 11: tts_models/en/ljspeech/tacotron2-DDC\\\\n\",\\n      \" 12: tts_models/en/ljspeech/tacotron2-DDC_ph\\\\n\",\\n      \" 13: tts_models/en/ljspeech/glow-tts\\\\n\",\\n      \" 14: tts_models/en/ljspeech/speedy-speech\\\\n\",\\n      \" 15: tts_models/en/ljspeech/tacotron2-DCA\\\\n\",\\n      \" 16: tts_models/en/ljspeech/vits\\\\n\",\\n      \" 17: tts_models/en/ljspeech/vits--neon\\\\n\",\\n      \" 18: tts_models/en/ljspeech/fast_pitch\\\\n\",\\n      \" 19: tts_models/en/ljspeech/overflow\\\\n\",\\n      \" 20: tts_models/en/ljspeech/neural_hmm\\\\n\",\\n      \" 21: tts_models/en/vctk/vits\\\\n\",\\n      \" 22: tts_models/en/vctk/fast_pitch\\\\n\",\\n      \" 23: tts_models/en/sam/tacotron-DDC\\\\n\",\\n      \" 24: tts_models/en/blizzard2013/capacitron-t2-c50\\\\n\",\\n      \" 25: tts_models/en/blizzard2013/capacitron-t2-c150_v2\\\\n\",\\n      \" 26: tts_models/en/multi-dataset/tortoise-v2\\\\n\",\\n      \" 27: tts_models/en/jenny/jenny\\\\n\",\\n      \" 28: tts_models/es/mai/tacotron2-DDC [already downloaded]\\\\n\",\\n      \" 29: tts_models/es/css10/vits [already downloaded]\\\\n\",\\n      \" 30: tts_models/fr/mai/tacotron2-DDC\\\\n\",\\n      \" 31: tts_models/fr/css10/vits\\\\n\",\\n      \" 32: tts_models/uk/mai/glow-tts\\\\n\",\\n      \" 33: tts_models/uk/mai/vits\\\\n\",\\n      \" 34: tts_models/zh-CN/baker/tacotron2-DDC-GST\\\\n\",\\n      \" 35: tts_models/nl/mai/tacotron2-DDC\\\\n\",\\n      \" 36: tts_models/nl/css10/vits\\\\n\",\\n      \" 37: tts_models/de/thorsten/tacotron2-DCA\\\\n\",\\n      \" 38: tts_models/de/thorsten/vits\\\\n\",\\n      \" 39: tts_models/de/thorsten/tacotron2-DDC\\\\n\",\\n      \" 40: tts_models/de/css10/vits-neon\\\\n\",\\n      \" 41: tts_models/ja/kokoro/tacotron2-DDC\\\\n\",\\n      \" 42: tts_models/tr/common-voice/glow-tts\\\\n\",\\n      \" 43: tts_models/it/mai_female/glow-tts\\\\n\",\\n      \" 44: tts_models/it/mai_female/vits\\\\n\",\\n      \" 45: tts_models/it/mai_male/glow-tts\\\\n\",\\n      \" 46: tts_models/it/mai_male/vits\\\\n\",\\n      \" 47: tts_models/ewe/openbible/vits\\\\n\",\\n      \" 48: tts_models/hau/openbible/vits\\\\n\",\\n      \" 49: tts_models/lin/openbible/vits\\\\n\",\\n      \" 50: tts_models/tw_akuapem/openbible/vits\\\\n\",\\n      \" 51: tts_models/tw_asante/openbible/vits\\\\n\",\\n      \" 52: tts_models/yor/openbible/vits\\\\n\",\\n      \" 53: tts_models/hu/css10/vits\\\\n\",\\n      \" 54: tts_models/el/cv/vits\\\\n\",\\n      \" 55: tts_models/fi/css10/vits\\\\n\",\\n      \" 56: tts_models/hr/cv/vits\\\\n\",\\n      \" 57: tts_models/lt/cv/vits\\\\n\",\\n      \" 58: tts_models/lv/cv/vits\\\\n\",\\n      \" 59: tts_models/mt/cv/vits\\\\n\",\\n      \" 60: tts_models/pl/mai_female/vits\\\\n\",\\n      \" 61: tts_models/pt/cv/vits\\\\n\",\\n      \" 62: tts_models/ro/cv/vits\\\\n\",\\n      \" 63: tts_models/sk/cv/vits\\\\n\",\\n      \" 64: tts_models/sl/cv/vits\\\\n\",\\n      \" 65: tts_models/sv/cv/vits\\\\n\",\\n      \" 66: tts_models/ca/custom/vits\\\\n\",\\n      \" 67: tts_models/fa/custom/glow-tts\\\\n\",\\n      \" 68: tts_models/bn/custom/vits-male\\\\n\",\\n      \" 69: tts_models/bn/custom/vits-female\\\\n\",\\n      \" 70: tts_models/be/common-voice/glow-tts\\\\n\",\\n      \"\\\\n\",\\n      \" Name format: type/language/dataset/model\\\\n\",\\n      \" 1: vocoder_models/universal/libri-tts/wavegrad\\\\n\",\\n      \" 2: vocoder_models/universal/libri-tts/fullband-melgan [already downloaded]\\\\n\",\\n      \" 3: vocoder_models/en/ek1/wavegrad\\\\n\",\\n      \" 4: vocoder_models/en/ljspeech/multiband-melgan\\\\n\",\\n      \" 5: vocoder_models/en/ljspeech/hifigan_v2\\\\n\",\\n      \" 6: vocoder_models/en/ljspeech/univnet\\\\n\",\\n      \" 7: vocoder_models/en/blizzard2013/hifigan_v2\\\\n\",\\n      \" 8: vocoder_models/en/vctk/hifigan_v2\\\\n\",\\n      \" 9: vocoder_models/en/sam/hifigan_v2\\\\n\",\\n      \" 10: vocoder_models/nl/mai/parallel-wavegan\\\\n\",\\n      \" 11: vocoder_models/de/thorsten/wavegrad\\\\n\",\\n      \" 12: vocoder_models/de/thorsten/fullband-melgan\\\\n\",\\n      \" 13: vocoder_models/de/thorsten/hifigan_v1\\\\n\",\\n      \" 14: vocoder_models/ja/kokoro/hifigan_v1\\\\n\",\\n      \" 15: vocoder_models/uk/mai/multiband-melgan\\\\n\",\\n      \" 16: vocoder_models/tr/common-voice/hifigan\\\\n\",\\n      \" 17: vocoder_models/be/common-voice/hifigan\\\\n\",\\n      \"\\\\n\",\\n      \" Name format: type/language/dataset/model\\\\n\",\\n      \" 1: voice_conversion_models/multilingual/vctk/freevc24\\\\n\",\\n      \"tts_models/es/mai/tacotron2-DDC\\\\n\",\\n      \"tts_models/es/css10/vits\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"from TTS.utils.manage import ModelManager\\\\n\",\\n    \"manager = ModelManager()\\\\n\",\\n    \"print(\\\\\"Available Spanish models:\\\\\")\\\\n\",\\n    \"for model in manager.list_models():\\\\n\",\\n    \"    if \\\\\"es\\\\\" in model or \\\\\"spanish\\\\\" in model.lower():\\\\n\",\\n    \"        print(model)\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 13,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stderr\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"ERROR: Could not find a version that satisfies the requirement time (from versions: none)\\\\n\",\\n      \"ERROR: No matching distribution found for time\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"!pip install time\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 14,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"import time\\\\n\",\\n    \"\\\\n\",\\n    \"def run_conversation():\\\\n\",\\n    \"    print(\\\\\"Starting conversation\\\\\")\\\\n\",\\n    \"    \\\\n\",\\n    \"    while True:\\\\n\",\\n    \"        try:\\\\n\",\\n    \"            \\\\n\",\\n    \"            \\\\n\",\\n    \"            #record user\\\\n\",\\n    \"            print(\\\\\"Recording now\\\\\")\\\\n\",\\n    \"            try:\\\\n\",\\n    \"                audio = record_audio()\\\\n\",\\n    \"            except Exception as e:\\\\n\",\\n    \"                print(f\\\\\"Error recording audio: {str(e)}\\\\\")\\\\n\",\\n    \"                print(\\\\\"Please try again.\\\\\")\\\\n\",\\n    \"                continue\\\\n\",\\n    \"            \\\\n\",\\n    \"            #transcribe audio\\\\n\",\\n    \"            try:\\\\n\",\\n    \"                transcription = transcribe_audio(audio, model=og_model)\\\\n\",\\n    \"                if not transcription.strip():\\\\n\",\\n    \"                    print(\\'No speech detected. Please try again\\')\\\\n\",\\n    \"                    continue\\\\n\",\\n    \"                print(f\\'\\\\\\\\nDid you say: {transcription}?\\')\\\\n\",\\n    \"            except Exception as e:\\\\n\",\\n    \"                print(f\\\\\"Error recording audio: {str(e)}\\\\\")\\\\n\",\\n    \"                print(\\\\\"Please try again.\\\\\")\\\\n\",\\n    \"                continue\\\\n\",\\n    \"            \\\\n\",\\n    \"            #if exit conversation\\\\n\",\\n    \"            if transcription.lower() in [\\'adiÃ³s\\', \\'adios\\', \\'chao\\', \\'bye\\', \\'goodbye\\', \\'done\\', \\'stop\\']:\\\\n\",\\n    \"                print(\\\\\"\\\\\\\\nEnding conversation. Chao!\\\\\")\\\\n\",\\n    \"                break\\\\n\",\\n    \"            \\\\n\",\\n    \"            #generate response\\\\n\",\\n    \"            try:\\\\n\",\\n    \"                response = generate_response(transcription)\\\\n\",\\n    \"            except Exception as e:\\\\n\",\\n    \"                    print(f\\\\\"Error generating response: {str(e)}\\\\\")\\\\n\",\\n    \"                    print(\\\\\"Please try again.\\\\\")\\\\n\",\\n    \"                    continue\\\\n\",\\n    \"                \\\\n\",\\n    \"            \\\\n\",\\n    \"            #convert response to speech output\\\\n\",\\n    \"            try:\\\\n\",\\n    \"                text_to_speech(response.text)\\\\n\",\\n    \"            except Exception as e:\\\\n\",\\n    \"                    print(f\\\\\"Error converting response to speech: {str(e)}\\\\\")\\\\n\",\\n    \"                    print(f\\\\\"Continuing without speech output.\\\\\\\\nResponse: {response.text}\\\\\")\\\\n\",\\n    \"                    time.sleep(3)\\\\n\",\\n    \"                    continue\\\\n\",\\n    \"                \\\\n\",\\n    \"                \\\\n\",\\n    \"        except KeyboardInterrupt:\\\\n\",\\n    \"            print(\\\\\"\\\\\\\\n\\\\\\\\nConversation interrupted by user. Â¡Hasta luego!\\\\\")\\\\n\",\\n    \"            break\\\\n\",\\n    \"        except Exception as e:\\\\n\",\\n    \"            print(f\\\\\"\\\\\\\\nUnexpected error: {str(e)}\\\\\")\\\\n\",\\n    \"            print(\\\\\"Please try again.\\\\\")\\\\n\",\\n    \"            continue\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"run_conversation()\"\\n   ]\\n  }\\n ],\\n \"metadata\": {\\n  \"kernelspec\": {\\n   \"display_name\": \".venv\",\\n   \"language\": \"python\",\\n   \"name\": \"python3\"\\n  },\\n  \"language_info\": {\\n   \"codemirror_mode\": {\\n    \"name\": \"ipython\",\\n    \"version\": 3\\n   },\\n   \"file_extension\": \".py\",\\n   \"mimetype\": \"text/x-python\",\\n   \"name\": \"python\",\\n   \"nbconvert_exporter\": \"python\",\\n   \"pygments_lexer\": \"ipython3\",\\n   \"version\": \"3.10.0\"\\n  }\\n },\\n \"nbformat\": 4,\\n \"nbformat_minor\": 2\\n}\\n', 'side_proj/spanish_conversation_bot.ipynb': '{\\n \"cells\": [\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 73,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"import torch\\\\n\",\\n    \"import gc\\\\n\",\\n    \"import os\\\\n\",\\n    \"\\\\n\",\\n    \"# Clear GPU memory\\\\n\",\\n    \"def clear_gpu_memory():\\\\n\",\\n    \"    gc.collect()\\\\n\",\\n    \"    torch.cuda.empty_cache()\\\\n\",\\n    \"    torch.cuda.reset_peak_memory_stats()\\\\n\",\\n    \"    \\\\n\",\\n    \"# Call the function\\\\n\",\\n    \"clear_gpu_memory()\\\\n\",\\n    \"\\\\n\",\\n    \"# Set memory optimization environment variables\\\\n\",\\n    \"os.environ[\\\\\"PYTORCH_CUDA_ALLOC_CONF\\\\\"] = \\\\\"expandable_segments:True\\\\\"\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 74,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"Using GPU: NVIDIA GeForce RTX 3060 Laptop GPU\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"# Check for GPU and set device\\\\n\",\\n    \"import torch\\\\n\",\\n    \"if torch.cuda.is_available():\\\\n\",\\n    \"    device = \\'cuda\\'\\\\n\",\\n    \"    print(f\\'Using GPU: {torch.cuda.get_device_name(0)}\\')\\\\n\",\\n    \"else:\\\\n\",\\n    \"    device = \\'cpu\\'\\\\n\",\\n    \"    print(\\'GPU not available, using CPU.\\')\\\\n\",\\n    \"# Use the \\'device\\' variable when loading models.\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 75,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"   from dotenv import load_dotenv\\\\n\",\\n    \"   import os\\\\n\",\\n    \"\\\\n\",\\n    \"   load_dotenv(\\\\\"HF.config\\\\\")\\\\n\",\\n    \"   hf_token = os.getenv(\\\\\"HF_TOKEN\\\\\")\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 76,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stderr\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you\\'ve just configured.\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"from huggingface_hub import  login\\\\n\",\\n    \"\\\\n\",\\n    \"#authenticating HF login, as CV17 dataset requires it\\\\n\",\\n    \"login(token=hf_token)\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 77,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"\\\\n\",\\n      \"Latin American Spanish sample:\\\\n\",\\n      \"\\\\n\",\\n      \"Dataset sizes:\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"from datasets import load_dataset, concatenate_datasets, DatasetDict\\\\n\",\\n    \"\\\\n\",\\n    \"\\'\\'\\'\\\\n\",\\n    \"# top commented out as anything from now commented out was attempted with common voice, but pivoted to smaller dataset\\\\n\",\\n    \"#columns_to_remove = [\\'client_id\\', \\'path\\', \\'up_votes\\', \\'down_votes\\', \\'age\\', \\'gender\\', \\'accent\\', \\'locale\\', \\'segment\\', \\'variant\\']\\\\n\",\\n    \"\\\\n\",\\n    \"#streaming so no fill download, spanish full dataset is 48 GB!\\\\n\",\\n    \"ds_train = load_dataset(\\\\n\",\\n    \"    \\\\\"mozilla-foundation/common_voice_6_0\\\\\",\\\\n\",\\n    \"    \\\\\"es\\\\\",\\\\n\",\\n    \"    split=\\\\\"train\\\\\",\\\\n\",\\n    \"    streaming=True,\\\\n\",\\n    \"    trust_remote_code=True\\\\n\",\\n    \").remove_columns(columns_to_remove)\\\\n\",\\n    \"\\\\n\",\\n    \"ds_test = load_dataset(\\\\n\",\\n    \"    \\\\\"mozilla-foundation/common_voice_6_0\\\\\",\\\\n\",\\n    \"    \\\\\"es\\\\\",\\\\n\",\\n    \"    split=\\\\\"test\\\\\",\\\\n\",\\n    \"    streaming=True,\\\\n\",\\n    \"    trust_remote_code=True\\\\n\",\\n    \").remove_columns(columns_to_remove)\\\\n\",\\n    \"\\\\n\",\\n    \"\\'\\'\\'\\\\n\",\\n    \"ds_fleur = DatasetDict()\\\\n\",\\n    \"columns_to_keep = [\\'audio\\', \\'transcription\\']  # These are the essential columns for speech recognition\\\\n\",\\n    \"\\\\n\",\\n    \"# Load Latin American Spanish data. If need European spanish in future, voxpopuli has almost all europ samples\\\\n\",\\n    \"ds_fleur[\\\\\"train\\\\\"] = load_dataset(\\\\n\",\\n    \"    \\\\\"google/fleurs\\\\\",\\\\n\",\\n    \"    \\\\\"es_419\\\\\",\\\\n\",\\n    \"    split=\\\\\"train\\\\\",\\\\n\",\\n    \"    trust_remote_code=True\\\\n\",\\n    \").select_columns(columns_to_keep)\\\\n\",\\n    \"\\\\n\",\\n    \"ds_fleur[\\\\\"test\\\\\"] = load_dataset(\\\\n\",\\n    \"    \\\\\"google/fleurs\\\\\",\\\\n\",\\n    \"    \\\\\"es_419\\\\\",\\\\n\",\\n    \"    split=\\\\\"test\\\\\",\\\\n\",\\n    \"    trust_remote_code=True\\\\n\",\\n    \").select_columns(columns_to_keep)\\\\n\",\\n    \"\\\\n\",\\n    \"print(\\\\\"\\\\\\\\nLatin American Spanish sample:\\\\\")\\\\n\",\\n    \"#print(ds_fleur[\\\\\"train\\\\\"][0])\\\\n\",\\n    \"\\\\n\",\\n    \"print(\\\\\"\\\\\\\\nDataset sizes:\\\\\")\\\\n\",\\n    \"#print(f\\\\\"Latin American Spanish train set: {len(list(ds_fleur[\\\\\"train\\\\\"]))} samples\\\\\")\\\\n\",\\n    \"#print(f\\\\\"Latin American Spanish test set: {len(list(ds_fleur[\\\\\"test\\\\\"]))} samples\\\\\")\\\\n\",\\n    \"\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 78,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from transformers import WhisperFeatureExtractor\\\\n\",\\n    \"\\\\n\",\\n    \"#load feature extractor from pre-trained checkpoint\\\\n\",\\n    \"feature_extractor = WhisperFeatureExtractor.from_pretrained(\\\\\"openai/whisper-small\\\\\")\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 79,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from transformers import WhisperTokenizer\\\\n\",\\n    \"\\\\n\",\\n    \"#load tokenizer, has very extensive byte-pair training\\\\n\",\\n    \"tokenizer = WhisperTokenizer.from_pretrained(\\\\\"openai/whisper-small\\\\\", language=\\\\\"Spanish\\\\\", task=\\\\\"transcribe\\\\\")\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 80,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"Input:                 los murales o garabatos indeseados reciben el nombre de grafiti\\\\n\",\\n      \"Decoded w/ special:    <|startoftranscript|><|es|><|transcribe|><|notimestamps|>los murales o garabatos indeseados reciben el nombre de grafiti<|endoftext|>\\\\n\",\\n      \"Decoded w/out special: los murales o garabatos indeseados reciben el nombre de grafiti\\\\n\",\\n      \"Are equal:             True\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"input_str = ds_fleur[\\\\\"train\\\\\"][0][\\\\\"transcription\\\\\"]\\\\n\",\\n    \"labels = tokenizer(input_str).input_ids\\\\n\",\\n    \"decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\\\\n\",\\n    \"decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\\\\n\",\\n    \"\\\\n\",\\n    \"#verifying tokenizer works\\\\n\",\\n    \"\\\\n\",\\n    \"print(f\\\\\"Input:                 {input_str}\\\\\")\\\\n\",\\n    \"print(f\\\\\"Decoded w/ special:    {decoded_with_special}\\\\\")\\\\n\",\\n    \"print(f\\\\\"Decoded w/out special: {decoded_str}\\\\\")\\\\n\",\\n    \"print(f\\\\\"Are equal:             {input_str == decoded_str}\\\\\")\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 81,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from transformers import WhisperProcessor\\\\n\",\\n    \"\\\\n\",\\n    \"processor = WhisperProcessor.from_pretrained(\\\\\"openai/whisper-small\\\\\", language=\\\\\"spanish\\\\\", task=\\\\\"transcribe\\\\\")\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 82,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"{\\'audio\\': {\\'path\\': \\'train/10005668950815513748.wav\\', \\'array\\': array([0., 0., 0., ..., 0., 0., 0.]), \\'sampling_rate\\': 16000}, \\'transcription\\': \\'los murales o garabatos indeseados reciben el nombre de grafiti\\'}\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"print(ds_fleur[\\\\\"train\\\\\"][0])\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 83,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from datasets import Audio\\\\n\",\\n    \"\\\\n\",\\n    \"#downsample audio to 16kHz to match that of Whisper\\'s sampling rate\\\\n\",\\n    \"ds_fleur = ds_fleur.cast_column(\\\\\"audio\\\\\", Audio(sampling_rate=16000))\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 84,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"{\\'audio\\': {\\'path\\': \\'train/10005668950815513748.wav\\', \\'array\\': array([0., 0., 0., ..., 0., 0., 0.]), \\'sampling_rate\\': 16000}, \\'transcription\\': \\'los murales o garabatos indeseados reciben el nombre de grafiti\\'}\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"print(ds_fleur[\\\\\"train\\\\\"][0])\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 85,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"def prepare_dataset(batch, feature_extractor=feature_extractor, tokenizer=tokenizer):   \\\\n\",\\n    \"# resample audio to 16kHz\\\\n\",\\n    \"    audio = batch[\\\\\"audio\\\\\"]\\\\n\",\\n    \"    \\\\n\",\\n    \"    # compute log-mel input feats from arrau\\\\n\",\\n    \"    inputs = feature_extractor(\\\\n\",\\n    \"        audio[\\\\\"array\\\\\"], \\\\n\",\\n    \"        sampling_rate=audio[\\\\\"sampling_rate\\\\\"]).input_features[0]\\\\n\",\\n    \"\\\\n\",\\n    \"    batch[\\\\\"input_features\\\\\"] = inputs.input_features[0]\\\\n\",\\n    \"    batch[\\\\\"attention_mask\\\\\"] = inputs.attention_mask[0]\\\\n\",\\n    \"    # encode target text to label ids\\\\n\",\\n    \"    batch[\\\\\"labels\\\\\"] = tokenizer(batch[\\\\\"transcription\\\\\"]).input_ids\\\\n\",\\n    \"    return batch\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 86,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"data\": {\\n      \"application/vnd.jupyter.widget-view+json\": {\\n       \"model_id\": \"7f511274f955434ba7da84a90501aeb0\",\\n       \"version_major\": 2,\\n       \"version_minor\": 0\\n      },\\n      \"text/plain\": [\\n       \"Map (num_proc=4):   0%|          | 0/2796 [00:00<?, ? examples/s]\"\\n      ]\\n     },\\n     \"metadata\": {},\\n     \"output_type\": \"display_data\"\\n    },\\n    {\\n     \"ename\": \"AttributeError\",\\n     \"evalue\": \"\\'numpy.ndarray\\' object has no attribute \\'input_features\\'\",\\n     \"output_type\": \"error\",\\n     \"traceback\": [\\n      \"\\\\u001b[1;31m---------------------------------------------------------------------------\\\\u001b[0m\",\\n      \"\\\\u001b[1;31mRemoteTraceback\\\\u001b[0m                           Traceback (most recent call last)\",\\n      \"\\\\u001b[1;31mRemoteTraceback\\\\u001b[0m: \\\\n\\\\\"\\\\\"\\\\\"\\\\nTraceback (most recent call last):\\\\n  File \\\\\"C:\\\\\\\\Users\\\\\\\\marte\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\gpu\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\multiprocess\\\\\\\\pool.py\\\\\", line 125, in worker\\\\n    result = (True, func(*args, **kwds))\\\\n  File \\\\\"C:\\\\\\\\Users\\\\\\\\marte\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\gpu\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\datasets\\\\\\\\utils\\\\\\\\py_utils.py\\\\\", line 688, in _write_generator_to_queue\\\\n    for i, result in enumerate(func(**kwargs)):\\\\n  File \\\\\"C:\\\\\\\\Users\\\\\\\\marte\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\gpu\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\datasets\\\\\\\\arrow_dataset.py\\\\\", line 3501, in _map_single\\\\n    for i, example in iter_outputs(shard_iterable):\\\\n  File \\\\\"C:\\\\\\\\Users\\\\\\\\marte\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\gpu\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\datasets\\\\\\\\arrow_dataset.py\\\\\", line 3475, in iter_outputs\\\\n    yield i, apply_function(example, i, offset=offset)\\\\n  File \\\\\"C:\\\\\\\\Users\\\\\\\\marte\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\gpu\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\datasets\\\\\\\\arrow_dataset.py\\\\\", line 3398, in apply_function\\\\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\\\\n  File \\\\\"C:\\\\\\\\Users\\\\\\\\marte\\\\\\\\AppData\\\\\\\\Local\\\\\\\\Temp\\\\\\\\ipykernel_31064\\\\\\\\2898349104.py\\\\\", line 10, in prepare_dataset\\\\nAttributeError: \\'numpy.ndarray\\' object has no attribute \\'input_features\\'\\\\n\\\\\"\\\\\"\\\\\"\",\\n      \"\\\\nThe above exception was the direct cause of the following exception:\\\\n\",\\n      \"\\\\u001b[1;31mAttributeError\\\\u001b[0m                            Traceback (most recent call last)\",\\n      \"Cell \\\\u001b[1;32mIn[86], line 1\\\\u001b[0m\\\\n\\\\u001b[1;32m----> 1\\\\u001b[0m ds_fleur \\\\u001b[38;5;241m=\\\\u001b[39m \\\\u001b[43mds_fleur\\\\u001b[49m\\\\u001b[38;5;241;43m.\\\\u001b[39;49m\\\\u001b[43mmap\\\\u001b[49m\\\\u001b[43m(\\\\u001b[49m\\\\n\\\\u001b[0;32m      2\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mprepare_dataset\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m      3\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mremove_columns\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mds_fleur\\\\u001b[49m\\\\u001b[38;5;241;43m.\\\\u001b[39;49m\\\\u001b[43mcolumn_names\\\\u001b[49m\\\\u001b[43m[\\\\u001b[49m\\\\u001b[38;5;124;43m\\\\\"\\\\u001b[39;49m\\\\u001b[38;5;124;43mtrain\\\\u001b[39;49m\\\\u001b[38;5;124;43m\\\\\"\\\\u001b[39;49m\\\\u001b[43m]\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m      4\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mnum_proc\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[38;5;241;43m4\\\\u001b[39;49m\\\\u001b[43m,\\\\u001b[49m\\\\u001b[43m  \\\\u001b[49m\\\\u001b[38;5;66;43;03m# Process 16 samples at a time \\\\u001b[39;49;00m\\\\n\\\\u001b[0;32m      5\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mfn_kwargs\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43m{\\\\u001b[49m\\\\u001b[38;5;124;43m\\\\\"\\\\u001b[39;49m\\\\u001b[38;5;124;43mfeature_extractor\\\\u001b[39;49m\\\\u001b[38;5;124;43m\\\\\"\\\\u001b[39;49m\\\\u001b[43m:\\\\u001b[49m\\\\u001b[43m \\\\u001b[49m\\\\u001b[43mfeature_extractor\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\u001b[43m \\\\u001b[49m\\\\u001b[38;5;124;43m\\\\\"\\\\u001b[39;49m\\\\u001b[38;5;124;43mtokenizer\\\\u001b[39;49m\\\\u001b[38;5;124;43m\\\\\"\\\\u001b[39;49m\\\\u001b[43m:\\\\u001b[49m\\\\u001b[43m \\\\u001b[49m\\\\u001b[43mtokenizer\\\\u001b[49m\\\\u001b[43m}\\\\u001b[49m\\\\n\\\\u001b[0;32m      6\\\\u001b[0m \\\\u001b[43m)\\\\u001b[49m\\\\n\",\\n      \"File \\\\u001b[1;32m~\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\gpu\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\datasets\\\\\\\\dataset_dict.py:944\\\\u001b[0m, in \\\\u001b[0;36mDatasetDict.map\\\\u001b[1;34m(self, function, with_indices, with_rank, with_split, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc, try_original_type)\\\\u001b[0m\\\\n\\\\u001b[0;32m    941\\\\u001b[0m \\\\u001b[38;5;28;01mif\\\\u001b[39;00m with_split:\\\\n\\\\u001b[0;32m    942\\\\u001b[0m     function \\\\u001b[38;5;241m=\\\\u001b[39m bind(function, split)\\\\n\\\\u001b[1;32m--> 944\\\\u001b[0m dataset_dict[split] \\\\u001b[38;5;241m=\\\\u001b[39m \\\\u001b[43mdataset\\\\u001b[49m\\\\u001b[38;5;241;43m.\\\\u001b[39;49m\\\\u001b[43mmap\\\\u001b[49m\\\\u001b[43m(\\\\u001b[49m\\\\n\\\\u001b[0;32m    945\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mfunction\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mfunction\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    946\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mwith_indices\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mwith_indices\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    947\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mwith_rank\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mwith_rank\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    948\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43minput_columns\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43minput_columns\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    949\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mbatched\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mbatched\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    950\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mbatch_size\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mbatch_size\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    951\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mdrop_last_batch\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mdrop_last_batch\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    952\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mremove_columns\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mremove_columns\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    953\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mkeep_in_memory\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mkeep_in_memory\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    954\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mload_from_cache_file\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mload_from_cache_file\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    955\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mcache_file_name\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mcache_file_names\\\\u001b[49m\\\\u001b[43m[\\\\u001b[49m\\\\u001b[43msplit\\\\u001b[49m\\\\u001b[43m]\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    956\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mwriter_batch_size\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mwriter_batch_size\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    957\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mfeatures\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mfeatures\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    958\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mdisable_nullable\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mdisable_nullable\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    959\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mfn_kwargs\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mfn_kwargs\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    960\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mnum_proc\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mnum_proc\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    961\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mdesc\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mdesc\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    962\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mtry_original_type\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mtry_original_type\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[0;32m    963\\\\u001b[0m \\\\u001b[43m\\\\u001b[49m\\\\u001b[43m)\\\\u001b[49m\\\\n\\\\u001b[0;32m    965\\\\u001b[0m \\\\u001b[38;5;28;01mif\\\\u001b[39;00m with_split:\\\\n\\\\u001b[0;32m    966\\\\u001b[0m     function \\\\u001b[38;5;241m=\\\\u001b[39m function\\\\u001b[38;5;241m.\\\\u001b[39mfunc\\\\n\",\\n      \"File \\\\u001b[1;32m~\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\gpu\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\datasets\\\\\\\\arrow_dataset.py:557\\\\u001b[0m, in \\\\u001b[0;36mtransmit_format.<locals>.wrapper\\\\u001b[1;34m(*args, **kwargs)\\\\u001b[0m\\\\n\\\\u001b[0;32m    550\\\\u001b[0m self_format \\\\u001b[38;5;241m=\\\\u001b[39m {\\\\n\\\\u001b[0;32m    551\\\\u001b[0m     \\\\u001b[38;5;124m\\\\\"\\\\u001b[39m\\\\u001b[38;5;124mtype\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m: \\\\u001b[38;5;28mself\\\\u001b[39m\\\\u001b[38;5;241m.\\\\u001b[39m_format_type,\\\\n\\\\u001b[0;32m    552\\\\u001b[0m     \\\\u001b[38;5;124m\\\\\"\\\\u001b[39m\\\\u001b[38;5;124mformat_kwargs\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m: \\\\u001b[38;5;28mself\\\\u001b[39m\\\\u001b[38;5;241m.\\\\u001b[39m_format_kwargs,\\\\n\\\\u001b[0;32m    553\\\\u001b[0m     \\\\u001b[38;5;124m\\\\\"\\\\u001b[39m\\\\u001b[38;5;124mcolumns\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m: \\\\u001b[38;5;28mself\\\\u001b[39m\\\\u001b[38;5;241m.\\\\u001b[39m_format_columns,\\\\n\\\\u001b[0;32m    554\\\\u001b[0m     \\\\u001b[38;5;124m\\\\\"\\\\u001b[39m\\\\u001b[38;5;124moutput_all_columns\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m: \\\\u001b[38;5;28mself\\\\u001b[39m\\\\u001b[38;5;241m.\\\\u001b[39m_output_all_columns,\\\\n\\\\u001b[0;32m    555\\\\u001b[0m }\\\\n\\\\u001b[0;32m    556\\\\u001b[0m \\\\u001b[38;5;66;03m# apply actual function\\\\u001b[39;00m\\\\n\\\\u001b[1;32m--> 557\\\\u001b[0m out: Union[\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m\\\\u001b[38;5;124mDataset\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m, \\\\u001b[38;5;124m\\\\\"\\\\u001b[39m\\\\u001b[38;5;124mDatasetDict\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m] \\\\u001b[38;5;241m=\\\\u001b[39m func(\\\\u001b[38;5;28mself\\\\u001b[39m, \\\\u001b[38;5;241m*\\\\u001b[39margs, \\\\u001b[38;5;241m*\\\\u001b[39m\\\\u001b[38;5;241m*\\\\u001b[39mkwargs)\\\\n\\\\u001b[0;32m    558\\\\u001b[0m datasets: \\\\u001b[38;5;28mlist\\\\u001b[39m[\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m\\\\u001b[38;5;124mDataset\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m] \\\\u001b[38;5;241m=\\\\u001b[39m \\\\u001b[38;5;28mlist\\\\u001b[39m(out\\\\u001b[38;5;241m.\\\\u001b[39mvalues()) \\\\u001b[38;5;28;01mif\\\\u001b[39;00m \\\\u001b[38;5;28misinstance\\\\u001b[39m(out, \\\\u001b[38;5;28mdict\\\\u001b[39m) \\\\u001b[38;5;28;01melse\\\\u001b[39;00m [out]\\\\n\\\\u001b[0;32m    559\\\\u001b[0m \\\\u001b[38;5;66;03m# re-apply format to the output\\\\u001b[39;00m\\\\n\",\\n      \"File \\\\u001b[1;32m~\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\gpu\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\datasets\\\\\\\\arrow_dataset.py:3171\\\\u001b[0m, in \\\\u001b[0;36mDataset.map\\\\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\\\\u001b[0m\\\\n\\\\u001b[0;32m   3165\\\\u001b[0m logger\\\\u001b[38;5;241m.\\\\u001b[39minfo(\\\\u001b[38;5;124mf\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m\\\\u001b[38;5;124mSpawning \\\\u001b[39m\\\\u001b[38;5;132;01m{\\\\u001b[39;00mnum_proc\\\\u001b[38;5;132;01m}\\\\u001b[39;00m\\\\u001b[38;5;124m processes\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m)\\\\n\\\\u001b[0;32m   3166\\\\u001b[0m \\\\u001b[38;5;28;01mwith\\\\u001b[39;00m hf_tqdm(\\\\n\\\\u001b[0;32m   3167\\\\u001b[0m     unit\\\\u001b[38;5;241m=\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m\\\\u001b[38;5;124m examples\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m,\\\\n\\\\u001b[0;32m   3168\\\\u001b[0m     total\\\\u001b[38;5;241m=\\\\u001b[39mpbar_total,\\\\n\\\\u001b[0;32m   3169\\\\u001b[0m     desc\\\\u001b[38;5;241m=\\\\u001b[39m(desc \\\\u001b[38;5;129;01mor\\\\u001b[39;00m \\\\u001b[38;5;124m\\\\\"\\\\u001b[39m\\\\u001b[38;5;124mMap\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m) \\\\u001b[38;5;241m+\\\\u001b[39m \\\\u001b[38;5;124mf\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m\\\\u001b[38;5;124m (num_proc=\\\\u001b[39m\\\\u001b[38;5;132;01m{\\\\u001b[39;00mnum_proc\\\\u001b[38;5;132;01m}\\\\u001b[39;00m\\\\u001b[38;5;124m)\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m,\\\\n\\\\u001b[0;32m   3170\\\\u001b[0m ) \\\\u001b[38;5;28;01mas\\\\u001b[39;00m pbar:\\\\n\\\\u001b[1;32m-> 3171\\\\u001b[0m     \\\\u001b[38;5;28;01mfor\\\\u001b[39;00m rank, done, content \\\\u001b[38;5;129;01min\\\\u001b[39;00m iflatmap_unordered(\\\\n\\\\u001b[0;32m   3172\\\\u001b[0m         pool, Dataset\\\\u001b[38;5;241m.\\\\u001b[39m_map_single, kwargs_iterable\\\\u001b[38;5;241m=\\\\u001b[39mkwargs_per_job\\\\n\\\\u001b[0;32m   3173\\\\u001b[0m     ):\\\\n\\\\u001b[0;32m   3174\\\\u001b[0m         \\\\u001b[38;5;28;01mif\\\\u001b[39;00m done:\\\\n\\\\u001b[0;32m   3175\\\\u001b[0m             shards_done \\\\u001b[38;5;241m+\\\\u001b[39m\\\\u001b[38;5;241m=\\\\u001b[39m \\\\u001b[38;5;241m1\\\\u001b[39m\\\\n\",\\n      \"File \\\\u001b[1;32m~\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\gpu\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\datasets\\\\\\\\utils\\\\\\\\py_utils.py:728\\\\u001b[0m, in \\\\u001b[0;36miflatmap_unordered\\\\u001b[1;34m(pool, func, kwargs_iterable)\\\\u001b[0m\\\\n\\\\u001b[0;32m    725\\\\u001b[0m \\\\u001b[38;5;28;01mfinally\\\\u001b[39;00m:\\\\n\\\\u001b[0;32m    726\\\\u001b[0m     \\\\u001b[38;5;28;01mif\\\\u001b[39;00m \\\\u001b[38;5;129;01mnot\\\\u001b[39;00m pool_changed:\\\\n\\\\u001b[0;32m    727\\\\u001b[0m         \\\\u001b[38;5;66;03m# we get the result in case there\\'s an error to raise\\\\u001b[39;00m\\\\n\\\\u001b[1;32m--> 728\\\\u001b[0m         [async_result\\\\u001b[38;5;241m.\\\\u001b[39mget(timeout\\\\u001b[38;5;241m=\\\\u001b[39m\\\\u001b[38;5;241m0.05\\\\u001b[39m) \\\\u001b[38;5;28;01mfor\\\\u001b[39;00m async_result \\\\u001b[38;5;129;01min\\\\u001b[39;00m async_results]\\\\n\",\\n      \"File \\\\u001b[1;32m~\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\gpu\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\datasets\\\\\\\\utils\\\\\\\\py_utils.py:728\\\\u001b[0m, in \\\\u001b[0;36m<listcomp>\\\\u001b[1;34m(.0)\\\\u001b[0m\\\\n\\\\u001b[0;32m    725\\\\u001b[0m \\\\u001b[38;5;28;01mfinally\\\\u001b[39;00m:\\\\n\\\\u001b[0;32m    726\\\\u001b[0m     \\\\u001b[38;5;28;01mif\\\\u001b[39;00m \\\\u001b[38;5;129;01mnot\\\\u001b[39;00m pool_changed:\\\\n\\\\u001b[0;32m    727\\\\u001b[0m         \\\\u001b[38;5;66;03m# we get the result in case there\\'s an error to raise\\\\u001b[39;00m\\\\n\\\\u001b[1;32m--> 728\\\\u001b[0m         [\\\\u001b[43masync_result\\\\u001b[49m\\\\u001b[38;5;241;43m.\\\\u001b[39;49m\\\\u001b[43mget\\\\u001b[49m\\\\u001b[43m(\\\\u001b[49m\\\\u001b[43mtimeout\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[38;5;241;43m0.05\\\\u001b[39;49m\\\\u001b[43m)\\\\u001b[49m \\\\u001b[38;5;28;01mfor\\\\u001b[39;00m async_result \\\\u001b[38;5;129;01min\\\\u001b[39;00m async_results]\\\\n\",\\n      \"File \\\\u001b[1;32m~\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\gpu\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\multiprocess\\\\\\\\pool.py:771\\\\u001b[0m, in \\\\u001b[0;36mApplyResult.get\\\\u001b[1;34m(self, timeout)\\\\u001b[0m\\\\n\\\\u001b[0;32m    769\\\\u001b[0m     \\\\u001b[38;5;28;01mreturn\\\\u001b[39;00m \\\\u001b[38;5;28mself\\\\u001b[39m\\\\u001b[38;5;241m.\\\\u001b[39m_value\\\\n\\\\u001b[0;32m    770\\\\u001b[0m \\\\u001b[38;5;28;01melse\\\\u001b[39;00m:\\\\n\\\\u001b[1;32m--> 771\\\\u001b[0m     \\\\u001b[38;5;28;01mraise\\\\u001b[39;00m \\\\u001b[38;5;28mself\\\\u001b[39m\\\\u001b[38;5;241m.\\\\u001b[39m_value\\\\n\",\\n      \"\\\\u001b[1;31mAttributeError\\\\u001b[0m: \\'numpy.ndarray\\' object has no attribute \\'input_features\\'\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"ds_fleur = ds_fleur.map(\\\\n\",\\n    \"    prepare_dataset,\\\\n\",\\n    \"    remove_columns=ds_fleur.column_names[\\\\\"train\\\\\"],\\\\n\",\\n    \"    num_proc=4,  # Process 16 samples at a time \\\\n\",\\n    \"    fn_kwargs={\\\\\"feature_extractor\\\\\": feature_extractor, \\\\\"tokenizer\\\\\": tokenizer}\\\\n\",\\n    \")\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from transformers import WhisperForConditionalGeneration\\\\n\",\\n    \"\\\\n\",\\n    \"# gen text conditioned on an input(audio data)\\\\n\",\\n    \"model = WhisperForConditionalGeneration.from_pretrained(\\\\\"openai/whisper-small\\\\\")\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"model.generation_config.language = \\\\\"spanish\\\\\"\\\\n\",\\n    \"model.generation_config.task = \\\\\"transcribe\\\\\"\\\\n\",\\n    \"\\\\n\",\\n    \"model.config.pad_token_id = model.config.eos_token_id\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"import torch\\\\n\",\\n    \"\\\\n\",\\n    \"from dataclasses import dataclass\\\\n\",\\n    \"from typing import Any, Dict, List, Union\\\\n\",\\n    \"\\\\n\",\\n    \"@dataclass \\\\n\",\\n    \"class DataCollatorSpeechSeq2SeqWithPadding:\\\\n\",\\n    \"    processor: Any\\\\n\",\\n    \"    decoder_start_token_id: int\\\\n\",\\n    \"    # called when collator is used to batch samples together\\\\n\",\\n    \"    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\\\\n\",\\n    \"        \\\\n\",\\n    \"        # Extract audio features from each sample and pad them to same length\\\\n\",\\n    \"        input_features = [{\\\\\"input_features\\\\\": feature[\\\\\"input_features\\\\\"]} for feature in features]\\\\n\",\\n    \"        #uses processor  built earlier that can call extractor and tokenizer\\\\n\",\\n    \"        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\\\\\"pt\\\\\")\\\\n\",\\n    \"\\\\n\",\\n    \"        # Extract text labels from each sample and pad them to same length\\\\n\",\\n    \"        label_features = [{\\\\\"input_ids\\\\\": feature[\\\\\"labels\\\\\"]} for feature in features]\\\\n\",\\n    \"        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\\\\\"pt\\\\\")\\\\n\",\\n    \"\\\\n\",\\n    \"        # Replace padding tokens with -100 (which is ignored in loss calculation)\\\\n\",\\n    \"        #when labels attention mask does NotEqual (ne) 1, then returns True. When this is true, meaning padding token,\\\\n\",\\n    \"        #then it will replace it with a -100\\\\n\",\\n    \"        labels = labels_batch[\\\\\"input_ids\\\\\"].masked_fill(labels_batch.attention_mask.ne(1), -100) \\\\n\",\\n    \"\\\\n\",\\n    \"        # Remove the start token if it was added during tokenization\\\\n\",\\n    \"        # (it will be added again during generation)\\\\n\",\\n    \"        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\\\\n\",\\n    \"            labels = labels[:, 1:]\\\\n\",\\n    \"\\\\n\",\\n    \"        batch[\\\\\"labels\\\\\"] = labels\\\\n\",\\n    \"\\\\n\",\\n    \"        #batch with audio feats and labels\\\\n\",\\n    \"        return batch\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"#initialize collator\\\\n\",\\n    \"data_collator = DataCollatorSpeechSeq2SeqWithPadding(\\\\n\",\\n    \"    processor=processor,\\\\n\",\\n    \"    decoder_start_token_id=model.config.decoder_start_token_id,\\\\n\",\\n    \")\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"import evaluate\\\\n\",\\n    \"\\\\n\",\\n    \"metric = evaluate.load(\\\\\"wer\\\\\")\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"import numpy as np\\\\n\",\\n    \"\\'\\'\\'\\\\n\",\\n    \"- replaces -100 with `pad_token_id` in `label_ids`\\\\n\",\\n    \"    - then decodes predicted and label ids to string\\\\n\",\\n    \"    - Computes WER\\\\n\",\\n    \"\\'\\'\\'\\\\n\",\\n    \"\\\\n\",\\n    \"def compute_metrics(pred):\\\\n\",\\n    \"    pred_ids = pred.predictions\\\\n\",\\n    \"    label_ids = np.array(pred.label_ids)\\\\n\",\\n    \"\\\\n\",\\n    \"    # replace -100 with the pad_token_id\\\\n\",\\n    \"    label_ids[label_ids == -100] = tokenizer.pad_token_id\\\\n\",\\n    \"\\\\n\",\\n    \"    # we do not want to group tokens when computing the metrics\\\\n\",\\n    \"    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\\\\n\",\\n    \"    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\\\\n\",\\n    \"\\\\n\",\\n    \"    #two different lines because compute returns dict[Unknown, Unknown] or None. So, first need to get wer val to * by 100\\\\n\",\\n    \"    #but before run won\\'t let multiply by None, so need to check if None or a real value\\\\n\",\\n    \"    wer = metric.compute(predictions=pred_str, references=label_str)\\\\n\",\\n    \"\\\\n\",\\n    \"    return {\\\\\"wer\\\\\": 100 * wer}\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from transformers.training_args_seq2seq import Seq2SeqTrainingArguments\\\\n\",\\n    \"\\\\n\",\\n    \"#define arguments\\\\n\",\\n    \"training_args = Seq2SeqTrainingArguments(\\\\n\",\\n    \"    output_dir=\\\\\"./whisper-small-es\\\\\",  # change to a repo name of your choice\\\\n\",\\n    \"    run_name=\\\\\"whisper-spanish-training\\\\\",\\\\n\",\\n    \"    per_device_train_batch_size=16,\\\\n\",\\n    \"    gradient_accumulation_steps=2,  # increase by 2x for every 2x decrease in batch size\\\\n\",\\n    \"    learning_rate=1e-5,\\\\n\",\\n    \"    warmup_steps=200,\\\\n\",\\n    \"    max_steps=2500,\\\\n\",\\n    \"    gradient_checkpointing=True,\\\\n\",\\n    \"    fp16=True,\\\\n\",\\n    \"    per_device_eval_batch_size=8,\\\\n\",\\n    \"    predict_with_generate=True,\\\\n\",\\n    \"    generation_max_length=225,\\\\n\",\\n    \"    save_steps=500,\\\\n\",\\n    \"    eval_steps=500,\\\\n\",\\n    \"    logging_steps=50,\\\\n\",\\n    \"    report_to=[\\\\\"tensorboard\\\\\",\\\\\"wandb\\\\\"],\\\\n\",\\n    \"    load_best_model_at_end=True,\\\\n\",\\n    \"    metric_for_best_model=\\\\\"wer\\\\\",\\\\n\",\\n    \"    greater_is_better=False,\\\\n\",\\n    \"    push_to_hub=True,\\\\n\",\\n    \"    eval_strategy=\\\\\"steps\\\\\",\\\\n\",\\n    \"    save_strategy=\\\\\"steps\\\\\"\\\\n\",\\n    \")\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"# Clear memory before training\\\\n\",\\n    \"clear_gpu_memory()\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"import torch\\\\n\",\\n    \"import gc\\\\n\",\\n    \"\\'\\'\\'\\\\n\",\\n    \"# Clear GPU memory\\\\n\",\\n    \"gc.collect()\\\\n\",\\n    \"torch.cuda.empty_cache()\\\\n\",\\n    \"torch.cuda.reset_peak_memory_stats()\\\\n\",\\n    \"\\'\\'\\'\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from transformers.trainer_seq2seq import Seq2SeqTrainer\\\\n\",\\n    \"from torchdata.stateful_dataloader import StatefulDataLoader\\\\n\",\\n    \"\\\\n\",\\n    \"trainer = Seq2SeqTrainer(\\\\n\",\\n    \"    args=training_args,\\\\n\",\\n    \"    model=model,\\\\n\",\\n    \"    train_dataset=ds_fleur[\\\\\"train\\\\\"],\\\\n\",\\n    \"    eval_dataset=ds_fleur[\\\\\"test\\\\\"],\\\\n\",\\n    \"    data_collator=data_collator,\\\\n\",\\n    \"    compute_metrics=compute_metrics\\\\n\",\\n    \")\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from dotenv import load_dotenv\\\\n\",\\n    \"import os\\\\n\",\\n    \"\\\\n\",\\n    \"load_dotenv(\\\\\"HF.config\\\\\")\\\\n\",\\n    \"wandb_key = os.getenv(\\\\\"WANDB_API_KEY\\\\\")\\\\n\",\\n    \"wandb_nb_name = os.getenv(\\\\\"WANDB_NOTEBOOK_NAME\\\\\")\\\\n\",\\n    \"\\\\n\",\\n    \"trainer.train()\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": []\\n  }\\n ],\\n \"metadata\": {\\n  \"kernelspec\": {\\n   \"display_name\": \"Python [conda env:gpu] *\",\\n   \"language\": \"python\",\\n   \"name\": \"conda-env-gpu-py\"\\n  },\\n  \"language_info\": {\\n   \"codemirror_mode\": {\\n    \"name\": \"ipython\",\\n    \"version\": 3\\n   },\\n   \"file_extension\": \".py\",\\n   \"mimetype\": \"text/x-python\",\\n   \"name\": \"python\",\\n   \"nbconvert_exporter\": \"python\",\\n   \"pygments_lexer\": \"ipython3\",\\n   \"version\": \"3.9.18\"\\n  }\\n },\\n \"nbformat\": 4,\\n \"nbformat_minor\": 4\\n}\\n', 'side_proj/src/spanish_chat_bot/audio/recorder.py': '\"\"\"\\nAudio recorder.\\n\"\"\"\\nimport sounddevice as sd\\nimport numpy as np\\nfrom scipy.signal import butter, filtfilt\\n\\n\\ndef record_audio(duration=5, sample_rate=16000, cutoff_freq=100, filter_order=4):\\n    \"\"\"\\n    Record audio for set duration and process it\\n    \\n    Args:\\n        duration (int): Amount of time recording user audio in seconds\\n        sample rate (int): sample rate to record audio in Hz\\n        cutoff_freq (int): Cutoff frequency for high-pass filter in Hz\\n        filter_order (int): Order of the Butterworth filter\\n    Returns:\\n        numpy.ndarray: Processed audio signal normalized to [-1, 1]\\n        \\n    \"\"\"\\n    try:\\n        myrecording = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1)\\n        sd.wait()\\n        \\n        #convert to single array, then normalize audio\\n        audio = myrecording.flatten()\\n        audio = audio / np.max(np.abs(audio))\\n        \\n        # apply filter to reduce noise\\n        nyquist = sample_rate / 2 #nyquist rate is 1/2 of the frequency\\n        \\n        #filter_order: higher order = sharper cutoff\\n        #cutoff_freq/nyquist normalizes the cutoff frequency\\n        #btype = high pass filter\\n        b, a = butter(filter_order, cutoff_freq/nyquist, btype=\\'high\\')\\n        audio = filtfilt(b, a, audio)\\n        \\n        #normalize again\\n        audio = audio / np.max(np.abs(audio))\\n        return audio\\n        \\n    except Exception as e:\\n        raise RuntimeError(f\"Failed to record audio: {str(e)}\")', 'side_proj/src/spanish_chat_bot/audio/tts.py': '\\'\\'\\' \\nConverts text response to audio\\n\\'\\'\\'\\nimport sounddevice as sd\\nfrom scipy.io import wavfile\\nfrom TTS.api import TTS\\nfrom sympy import false\\n\\n\\ndef text_to_speech(text, output_path=\"output.wav\"):\\n    \"\"\"\\n    Take text input and convert it to wav file\\n    \\n    Args:\\n        text (string): Generated text from response generator\\n        output_path (string): path for wav file output\\n    Returns:\\n        string: path to wav file of response\\n    \"\"\"\\n    \\n    try: \\n    # Initialize TTS with Spanish xtts model\\n        tts = TTS(model_name=\"tts_models/es/css10/vits\", progress_bar=False)\\n        \\n        # Generate speech\\n        tts.tts_to_file(text=text, file_path=output_path)\\n        \\n        # Play the audio\\n        sample_rate, audio = wavfile.read(output_path)\\n        sd.play(audio, sample_rate)\\n        sd.wait()\\n        \\n        return output_path\\n    \\n    except Exception as e:\\n        raise RuntimeError(f\"Failed to convert text to audio: {str(e)}\")\\n        ', 'side_proj/src/spanish_chat_bot/models/response_generator.py': '\\'\\'\\'\\nGenerate text response\\n\\'\\'\\'\\nfrom dotenv import load_dotenv\\nload_dotenv(\"tokens.env\")\\nimport os\\nfrom typing import Optional, List\\nimport google.generativeai as gem\\nfrom google.api_core import exceptions as google_exceptions\\n\\nclass ResponseGenerator:\\n    def __init__(self, model_name: str = \"gemini-2.0-flash\", api_key: Optional[str] = None):\\n        \\'\\'\\'\\n        Initialize generator with model.\\n        \\n        Args:\\n            model_name (string): name of generative model to use\\n            api_key (str, optional): API key for model use if necessary. If not provided, will look for in env\\n        \\'\\'\\'\\n        self.api_key = api_key or os.getenv(\"GEMINI_API_KEY\")\\n        if not self.api_key:\\n            raise ValueError(\"Gemini API key not found. Please provide it or set GEMINI_API_KEY environment variable.\")\\n            \\n        # Configure the API\\n        gem.configure(api_key=self.api_key)\\n        \\n        # Initialize the model\\n        self.model = gem.GenerativeModel(model_name)\\n        \\n        # Optional: Store conversation history\\n        self.conversation_history: List[dict] = []\\n\\n    def generate(self, text: str) -> str:\\n        \"\"\"\\n        Generate a response to the user\\'s speech\\n        \\n        Args:\\n            text (str): user\\'s speech input converted to text\\n        Returns:\\n            str: Generated text\\n        Raises:\\n            RuntimeError: If there\\'s an error generating the response\\n        \"\"\"\\n        try:\\n            response = self.model.generate_content(\\n                    f\"Eres un asistente conversacional en espaÃ±ol. Adaptarse al contexto de la conversaciÃ³n. \"\\n                    f\"(Por ejemplo: si la persona te diga algo como si fuera tu amigo o un familiar, respondele con ese rol.)\"\\n                    f\"Tu no eres un chatbot, eres la persona.\"\\n                    f\"No usa los emojis\"\\n                    f\"Mantenga la duraciÃ³n de tu respuesta corta. Responde de manera natural y conversacional a: {text}\"\\n                )\\n            \\n            if not response or not response.text:\\n                raise RuntimeError(\"Received empty response from model\")\\n                \\n            return response.text\\n            \\n        except google_exceptions.ResourceExhausted:\\n            raise RuntimeError(\"API rate limit exceeded. Please try again later.\")\\n        except google_exceptions.ServiceUnavailable:\\n            raise RuntimeError(\"Gemini API service is currently unavailable. Please try again later.\")\\n        except google_exceptions.InvalidArgument as e:\\n            raise RuntimeError(f\"Invalid input to model: {str(e)}\")\\n        except Exception as e:\\n            raise RuntimeError(f\"Failed to generate response: {str(e)}\")\\n\\n\\n# Create a singleton instance\\n_generator: Optional[ResponseGenerator] = None\\n\\ndef generate_response(text: str, model_name: str = \"gemini-2.0-flash\", api_key: Optional[str] = None) -> str:\\n    \"\"\"\\n    Global function to generate responses using the singleton generator.\\n    \\n    Args:\\n        text (str): Text to generate response for\\n        model_name (str): Name of the model to use\\n        api_key (str, optional): API key for the model\\n        \\n    Returns:\\n        str: Generated response\\n    \"\"\"\\n    global _generator\\n    if _generator is None:\\n        _generator = ResponseGenerator(model_name=model_name, api_key=api_key)\\n    return _generator.generate(text)', 'side_proj/src/spanish_chat_bot/models/transcriber.py': '\"\"\"\\nAudio transcription functionality using Whisper model.\\n\"\"\"\\n\\nimport torch\\nfrom transformers import (\\n    WhisperProcessor,\\n    WhisperForConditionalGeneration,\\n    WhisperFeatureExtractor,\\n    WhisperTokenizer\\n)\\nfrom typing import Optional, Union\\nimport numpy as np\\n\\n\\nclass Transcriber:\\n    def __init__(self, model_name: str = \"openai/whisper-small\", device: Optional[str] = None):\\n        \"\"\"\\n        Initialize the transcriber with Whisper model.\\n        \\n        Args:\\n            model_name (str): Name of the Whisper model to use\\n            device (str, optional): Device to run the model on (\\'cuda\\' or \\'cpu\\')\\n        \"\"\"\\n        # looks for cuda device to run model on\\n        if device is None:\\n            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\\n            \\n        self.device = device\\n        self.feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name)\\n        self.tokenizer = WhisperTokenizer.from_pretrained(model_name)\\n        self.model = WhisperForConditionalGeneration.from_pretrained(model_name).to(self.device)\\n        \\n        # Configure model for Spanish + transcription\\n        self.model.config.forced_decoder_ids = self.tokenizer.get_decoder_prompt_ids(\\n            language=\"spanish\",\\n            task=\"transcribe\"\\n        )\\n\\n    def transcribe(self, audio: np.ndarray, sample_rate: int = 16000) -> str:\\n        \"\"\"\\n        Transcribe audio to text.\\n        \\n        Args:\\n            audio (numpy.ndarray): Audio array\\n            sample_rate (int): Audio sample rate\\n            \\n        Returns:\\n            str: Transcribed text\\n        \"\"\"\\n        #process audio to tensors\\n        inputs = self.feature_extractor(\\n            audio,\\n            sampling_rate=sample_rate,\\n            return_tensors=\"pt\"\\n        )\\n        input_features = inputs.input_features.to(self.device)\\n        \\n        # Generate transcription\\n        predicted_ids = self.model.generate(\\n            input_features,\\n            max_length=448,\\n            num_beams=5, \\n            temperature=0.7\\n        )\\n        transcription = self.tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)[0]\\n        \\n        return transcription\\n\\n\\n# Create a singleton instance, more than one transcriber != good\\n_transcriber: Optional[Transcriber] = None\\n\\ndef transcribe_audio(audio: np.ndarray, sample_rate: int = 16000, model_name: str = \"openai/whisper-small\") -> str:\\n    \"\"\"\\n    Global function to transcribe audio using the singleton transcriber.\\n    \\n    Args:\\n        audio (numpy.ndarray): Audio array\\n        sample_rate (int): Audio sample rate\\n        model_name (str): Name of the Whisper model to use\\n        \\n    Returns:\\n        str: Transcribed text\\n    \"\"\"\\n    global _transcriber\\n    if _transcriber is None:\\n        _transcriber = Transcriber(model_name=model_name)\\n    return _transcriber.transcribe(audio, sample_rate) ', 'side_proj/frontend/package.json': '{\\n  \"name\": \"frontend\",\\n  \"private\": true,\\n  \"version\": \"0.0.0\",\\n  \"type\": \"module\",\\n  \"scripts\": {\\n    \"dev\": \"vite\",\\n    \"build\": \"vite build\",\\n    \"lint\": \"eslint .\",\\n    \"preview\": \"vite preview\"\\n  },\\n  \"dependencies\": {\\n    \"@heroicons/react\": \"^2.2.0\",\\n    \"axios\": \"^1.9.0\",\\n    \"react\": \"^19.1.0\",\\n    \"react-dom\": \"^19.1.0\"\\n  },\\n  \"devDependencies\": {\\n    \"@eslint/js\": \"^9.25.0\",\\n    \"@tailwindcss/postcss\": \"^4.1.8\",\\n    \"@types/react\": \"^19.1.2\",\\n    \"@types/react-dom\": \"^19.1.2\",\\n    \"@vitejs/plugin-react\": \"^4.4.1\",\\n    \"autoprefixer\": \"^10.4.21\",\\n    \"eslint\": \"^9.25.0\",\\n    \"eslint-plugin-react-hooks\": \"^5.2.0\",\\n    \"eslint-plugin-react-refresh\": \"^0.4.19\",\\n    \"globals\": \"^16.0.0\",\\n    \"postcss\": \"^8.5.4\",\\n    \"tailwindcss\": \"^4.1.8\",\\n    \"vite\": \"^6.3.5\"\\n  }\\n}\\n', 'side_proj/frontend/vite.config.js': \"import { defineConfig } from 'vite'\\nimport react from '@vitejs/plugin-react'\\n\\n// https://vite.dev/config/\\nexport default defineConfig({\\n  plugins: [react()],\\n  css: {\\n    postcss: './postcss.config.js',\\n  },\\n  server: {\\n    proxy: {\\n      '/api': 'http://localhost:8000'\\n    }\\n  }\\n})\\n\"}}\n",
      "  ðŸ’¬ Agent Text Response: I have fetched the content of all the selected files. I will now proceed with the analysis....\n",
      "ðŸ Final response from agent received.\n",
      "  ðŸ’¬ Agent Text Response: ```markdown\n",
      "# Project Analysis Report: Contigo - Spanish Conversational Voice Bot\n",
      "\n",
      "## Executive Summary\n",
      "\n",
      "Contigo is a proof-of-concept Spanish convers...\n",
      "  ðŸ’¾ State Change: {'analysis_results': '```markdown\\n# Project Analysis Report: Contigo - Spanish Conversational Voice Bot\\n\\n## Executive Summary\\n\\nContigo is a proof-of-concept Spanish conversational voice bot designed to facilitate language learning. It leverages a robust pipeline involving speech-to-text (Whisper), natural language understanding and response generation (likely a Google Gemini model), and text-to-speech (TTS) synthesis. The project currently operates locally with a basic web-based frontend for user interaction and conversation history. The core voice-to-voice interaction is functional, allowing users to speak in Spanish, receive transcribed text, get a generated response, and hear that response spoken back. Future development plans include deployment, enhanced conversation memory, improved UI/UX, and advanced fine-tuning of the Whisper model to better handle diverse Spanish dialects and accents.\\n\\n## Technology Stack\\n\\nThe project utilizes a diverse set of modern technologies for its core functionality and development:\\n\\n*   **Programming Languages:** Python (backend), JavaScript (frontend)\\n*   **Backend Framework:** FastAPI (for creating the API endpoints)\\n*   **Machine Learning Libraries:**\\n    *   `torch`: For deep learning operations, particularly with transformer models.\\n    *   `transformers`: Hugging Face\\'s library for accessing and utilizing pre-trained models like Whisper.\\n    *   `sounddevice`: For real-time audio input/output.\\n    *   `numpy`: For numerical operations, especially with audio data.\\n    *   `scipy`: For scientific computing, including signal processing for audio.\\n    *   `google-generativeai`: For interacting with Google\\'s Gemini language models.\\n    *   `TTS`: For text-to-speech synthesis.\\n    *   `datasets`: For loading and processing datasets, likely for model fine-tuning or evaluation.\\n    *   `evaluate`: For evaluating model performance (e.g., Word Error Rate).\\n*   **Frontend Framework:** React (via Vite)\\n*   **Frontend Styling:** Tailwind CSS\\n*   **Environment Management:** `python-dotenv` (for managing environment variables like API keys)\\n*   **Development Tools:**\\n    *   Vite: For fast frontend development and bundling.\\n    *   Uvicorn: ASGI server for running the FastAPI application.\\n    *   ESLint: For code quality and consistency in the frontend.\\n*   **Cloud/MLOps:**\\n    *   Hugging Face Hub: Used for accessing pre-trained models (Whisper) and potentially for model sharing.\\n    *   Weights & Biases (`wandb`): For experiment tracking and visualization during model training/fine-tuning.\\n\\n## Project Structure and Key Components\\n\\nThe project is organized into a `side_proj` directory, containing the main application logic, frontend, and notebooks.\\n\\n### Backend (`side_proj/src/spanish_chat_bot/`)\\n\\n*   **`__init__.py`**: Standard Python package initialization.\\n*   **`conversation.py`**: Orchestrates the main conversation flow, handling user input (audio), transcription, response generation, and speech synthesis. It includes a `start` method for a continuous conversational loop and a `process_audio` method for individual turns.\\n*   **`endpoint_handler.py`**: Implements the FastAPI application, defining API endpoints for starting recording, transcription, and response generation. It also includes a WebSocket endpoint (currently empty) for potential real-time updates.\\n*   **`audio/`**:\\n    *   `recorder.py`: Contains the `record_audio` function, responsible for capturing audio from the microphone, processing it (flattening, normalization, high-pass filtering), and returning a NumPy array.\\n    *   `tts.py`: Implements the `text_to_speech` function, using the `TTS` library to convert text responses into speech and play it back.\\n*   **`models/`**:\\n    *   `transcriber.py`: Encapsulates the Whisper model for speech-to-text. It includes a `Transcriber` class with a `transcribe` method and a global singleton `_transcriber` for efficient model loading. It\\'s configured for Spanish transcription.\\n    *   `response_generator.py`: Handles text generation using Google\\'s Gemini API. It features a `ResponseGenerator` class and a global singleton `_generator` for managing the language model instance. It\\'s prompted to act as a conversational partner in Spanish.\\n\\n### Frontend (`side_proj/frontend/`)\\n\\n*   **`src/`**: Contains the React application code.\\n    *   `App.jsx`: The main application component, managing the overall layout, message display, and interaction with the `RecordingInterface`. It passes new messages to the `App` component via the `handleNewMessage` prop.\\n    *   `components/`: Contains reusable UI components:\\n        *   `Message.jsx` (inferred): Likely renders individual chat messages.\\n        *   `RecordingInterface.jsx`: Handles user audio input (recording and sending to the backend).\\n    *   `main.jsx`: Entry point for the React application.\\n*   **`vite.config.js`**: Configures Vite, including proxy settings to route API requests to the backend (e.g., `/api` to `http://localhost:8000`).\\n*   **`package.json`**: Defines frontend dependencies (React, Tailwind CSS, Axios) and scripts for development and building.\\n\\n### Notebooks (`side_proj/notebooks/`)\\n\\n*   **`inference_pipeline.ipynb`**: Demonstrates the end-to-end pipeline, including audio recording, transcription using both a base Whisper model (`openai/whisper-small`) and a potentially fine-tuned Spanish model (`mevitts/whisper-small-es`), response generation using Gemini, and text-to-speech. It also includes setup for device (GPU/CPU) and Hugging Face authentication.\\n*   **`spanish_conversation_bot.ipynb`**: Appears to be related to model fine-tuning or experimentation. It covers setting up the environment, GPU memory management, Hugging Face login, dataset loading (Google Fleurs for Spanish), Whisper processor and tokenizer setup, data preparation (feature extraction, tokenization), model loading (`openai/whisper-small`), data collator, metrics computation (WER), and setting up training arguments for `Seq2SeqTrainer`.\\n\\n## Core Functionality & Pipeline\\n\\nThe core of Contigo is the voice interaction pipeline:\\n\\n1.  **User Input:** The user interacts with the frontend (`App.jsx`, `RecordingInterface.jsx`). When the record button is pressed, `RecordingInterface.jsx` likely uses the browser\\'s MediaDevices API to capture audio.\\n2.  **Audio Recording (Backend):** The captured audio data is sent to the backend API (`endpoint_handler.py`). The `start_recording` endpoint (or a similar mechanism) triggers `side_proj/src/spanish_chat_bot/audio/recorder.py`\\'s `record_audio` function to capture and process the audio.\\n3.  **Speech-to-Text (STT):** The processed audio array is sent to the `/api/transcribe` endpoint, which calls `side_proj/src/spanish_chat_bot/models/transcriber.py`\\'s `transcribe_audio` function. This function utilizes the Whisper model to convert the audio into Spanish text.\\n4.  **Response Generation:** The transcribed text is passed to the `/api/generate` endpoint, which invokes `side_proj/src/spanish_chat_bot/models/response_generator.py`\\'s `generate_response` function. This function queries the Google Gemini model to produce a relevant and conversational Spanish text response.\\n5.  **Text-to-Speech (TTS):** The generated text response is then passed to `side_proj/src/spanish_chat_bot/audio/tts.py`\\'s `text_to_speech` function. This function uses the `TTS` library (specifically a Spanish VITS model) to synthesize speech from the text and play it back to the user.\\n6.  **Frontend Update:** The frontend (`App.jsx`) receives the transcribed text and the spoken response, updating the chat history to display the conversation.\\n\\n## Key Features & Capabilities\\n\\n*   **Voice-to-Voice Interaction:** Enables natural, spoken conversations in Spanish.\\n*   **Spanish Language Support:** Utilizes models specifically configured or fine-tuned for the Spanish language.\\n*   **Modular Design:** Components for audio recording, transcription, response generation, and TTS are separated into distinct modules, promoting maintainability and extensibility.\\n*   **Real-time Processing:** Aims for real-time interaction, though performance may vary based on hardware and model size.\\n*   **Basic Frontend:** Provides a simple web interface for user interaction and visual feedback.\\n*   **Model Management:** Uses singletons for critical ML components (Transcriber, ResponseGenerator) to ensure efficient resource utilization (e.g., avoiding reloading models).\\n*   **GPU Acceleration:** Explicitly checks for and utilizes CUDA-enabled GPUs for faster model inference.\\n\\n## Development & Future Goals\\n\\nThe project is in a \"Proof of Concept Operational\" state, with the core pipeline working. Future development focuses on enhancing its capabilities and usability:\\n\\n*   **Deployment:** Making the application accessible via cloud hosting.\\n*   **Conversation Memory:** Implementing a mechanism for the bot to retain context across multiple turns for more coherent dialogues.\\n*   **UX/UI Enhancements:** Improving the user interface to be more intuitive and visually appealing.\\n*   **Model Fine-Tuning:**\\n    *   **Whisper:** Addressing the current challenge where fine-tuning led to worse performance than the base model. The goal is to successfully fine-tune Whisper on diverse Spanish dialects and accents to improve transcription accuracy for learners with less natural speech patterns. `spanish_conversation_bot.ipynb` outlines some initial steps towards this.\\n    *   **LLM:** Potentially fine-tuning or prompt-engineering the language model further for more specific conversational styles or tasks.\\n\\n## Dependencies\\n\\nThe project has a comprehensive list of dependencies, including core machine learning libraries (`torch`, `transformers`, `TTS`), cloud/AI SDKs (`google-generativeai`), audio processing tools (`sounddevice`, `scipy`), web frameworks (`fastapi`, `uvicorn`), and frontend build tools and libraries (`vite`, `react`, `tailwindcss`, `axios`).\\n\\n## Conclusion\\n\\nContigo represents a promising initiative in leveraging advanced AI technologies for language education. Its current operational status as a voice-to-voice conversational bot is a strong foundation. The planned future enhancements, particularly in model fine-tuning for diverse Spanish accents and improved conversational memory, are crucial for realizing its full potential as an accessible and effective language learning tool.\\n```'}\n",
      "ðŸ Final response from agent received.\n",
      "\n",
      "âœ… Pipeline execution complete.\n",
      "\n",
      "--- Final Analysis Results ---\n",
      "# Project Analysis Report: Contigo - Spanish Conversational Voice Bot\n",
      "\n",
      "## Executive Summary\n",
      "\n",
      "Contigo is a proof-of-concept Spanish conversational voice bot designed to facilitate language learning. It leverages a robust pipeline involving speech-to-text (Whisper), natural language understanding and response generation (likely a Google Gemini model), and text-to-speech (TTS) synthesis. The project currently operates locally with a basic web-based frontend for user interaction and conversation history. The core voice-to-voice interaction is functional, allowing users to speak in Spanish, receive transcribed text, get a generated response, and hear that response spoken back. Future development plans include deployment, enhanced conversation memory, improved UI/UX, and advanced fine-tuning of the Whisper model to better handle diverse Spanish dialects and accents.\n",
      "\n",
      "## Technology Stack\n",
      "\n",
      "The project utilizes a diverse set of modern technologies for its core functionality and development:\n",
      "\n",
      "*   **Programming Languages:** Python (backend), JavaScript (frontend)\n",
      "*   **Backend Framework:** FastAPI (for creating the API endpoints)\n",
      "*   **Machine Learning Libraries:**\n",
      "    *   `torch`: For deep learning operations, particularly with transformer models.\n",
      "    *   `transformers`: Hugging Face's library for accessing and utilizing pre-trained models like Whisper.\n",
      "    *   `sounddevice`: For real-time audio input/output.\n",
      "    *   `numpy`: For numerical operations, especially with audio data.\n",
      "    *   `scipy`: For scientific computing, including signal processing for audio.\n",
      "    *   `google-generativeai`: For interacting with Google's Gemini language models.\n",
      "    *   `TTS`: For text-to-speech synthesis.\n",
      "    *   `datasets`: For loading and processing datasets, likely for model fine-tuning or evaluation.\n",
      "    *   `evaluate`: For evaluating model performance (e.g., Word Error Rate).\n",
      "*   **Frontend Framework:** React (via Vite)\n",
      "*   **Frontend Styling:** Tailwind CSS\n",
      "*   **Environment Management:** `python-dotenv` (for managing environment variables like API keys)\n",
      "*   **Development Tools:**\n",
      "    *   Vite: For fast frontend development and bundling.\n",
      "    *   Uvicorn: ASGI server for running the FastAPI application.\n",
      "    *   ESLint: For code quality and consistency in the frontend.\n",
      "*   **Cloud/MLOps:**\n",
      "    *   Hugging Face Hub: Used for accessing pre-trained models (Whisper) and potentially for model sharing.\n",
      "    *   Weights & Biases (`wandb`): For experiment tracking and visualization during model training/fine-tuning.\n",
      "\n",
      "## Project Structure and Key Components\n",
      "\n",
      "The project is organized into a `side_proj` directory, containing the main application logic, frontend, and notebooks.\n",
      "\n",
      "### Backend (`side_proj/src/spanish_chat_bot/`)\n",
      "\n",
      "*   **`__init__.py`**: Standard Python package initialization.\n",
      "*   **`conversation.py`**: Orchestrates the main conversation flow, handling user input (audio), transcription, response generation, and speech synthesis. It includes a `start` method for a continuous conversational loop and a `process_audio` method for individual turns.\n",
      "*   **`endpoint_handler.py`**: Implements the FastAPI application, defining API endpoints for starting recording, transcription, and response generation. It also includes a WebSocket endpoint (currently empty) for potential real-time updates.\n",
      "*   **`audio/`**:\n",
      "    *   `recorder.py`: Contains the `record_audio` function, responsible for capturing audio from the microphone, processing it (flattening, normalization, high-pass filtering), and returning a NumPy array.\n",
      "    *   `tts.py`: Implements the `text_to_speech` function, using the `TTS` library to convert text responses into speech and play it back.\n",
      "*   **`models/`**:\n",
      "    *   `transcriber.py`: Encapsulates the Whisper model for speech-to-text. It includes a `Transcriber` class with a `transcribe` method and a global singleton `_transcriber` for efficient model loading. It's configured for Spanish transcription.\n",
      "    *   `response_generator.py`: Handles text generation using Google's Gemini API. It features a `ResponseGenerator` class and a global singleton `_generator` for managing the language model instance. It's prompted to act as a conversational partner in Spanish.\n",
      "\n",
      "### Frontend (`side_proj/frontend/`)\n",
      "\n",
      "*   **`src/`**: Contains the React application code.\n",
      "    *   `App.jsx`: The main application component, managing the overall layout, message display, and interaction with the `RecordingInterface`. It passes new messages to the `App` component via the `handleNewMessage` prop.\n",
      "    *   `components/`: Contains reusable UI components:\n",
      "        *   `Message.jsx` (inferred): Likely renders individual chat messages.\n",
      "        *   `RecordingInterface.jsx`: Handles user audio input (recording and sending to the backend).\n",
      "    *   `main.jsx`: Entry point for the React application.\n",
      "*   **`vite.config.js`**: Configures Vite, including proxy settings to route API requests to the backend (e.g., `/api` to `http://localhost:8000`).\n",
      "*   **`package.json`**: Defines frontend dependencies (React, Tailwind CSS, Axios) and scripts for development and building.\n",
      "\n",
      "### Notebooks (`side_proj/notebooks/`)\n",
      "\n",
      "*   **`inference_pipeline.ipynb`**: Demonstrates the end-to-end pipeline, including audio recording, transcription using both a base Whisper model (`openai/whisper-small`) and a potentially fine-tuned Spanish model (`mevitts/whisper-small-es`), response generation using Gemini, and text-to-speech. It also includes setup for device (GPU/CPU) and Hugging Face authentication.\n",
      "*   **`spanish_conversation_bot.ipynb`**: Appears to be related to model fine-tuning or experimentation. It covers setting up the environment, GPU memory management, Hugging Face login, dataset loading (Google Fleurs for Spanish), Whisper processor and tokenizer setup, data preparation (feature extraction, tokenization), model loading (`openai/whisper-small`), data collator, metrics computation (WER), and setting up training arguments for `Seq2SeqTrainer`.\n",
      "\n",
      "## Core Functionality & Pipeline\n",
      "\n",
      "The core of Contigo is the voice interaction pipeline:\n",
      "\n",
      "1.  **User Input:** The user interacts with the frontend (`App.jsx`, `RecordingInterface.jsx`). When the record button is pressed, `RecordingInterface.jsx` likely uses the browser's MediaDevices API to capture audio.\n",
      "2.  **Audio Recording (Backend):** The captured audio data is sent to the backend API (`endpoint_handler.py`). The `start_recording` endpoint (or a similar mechanism) triggers `side_proj/src/spanish_chat_bot/audio/recorder.py`'s `record_audio` function to capture and process the audio.\n",
      "3.  **Speech-to-Text (STT):** The processed audio array is sent to the `/api/transcribe` endpoint, which calls `side_proj/src/spanish_chat_bot/models/transcriber.py`'s `transcribe_audio` function. This function utilizes the Whisper model to convert the audio into Spanish text.\n",
      "4.  **Response Generation:** The transcribed text is passed to the `/api/generate` endpoint, which invokes `side_proj/src/spanish_chat_bot/models/response_generator.py`'s `generate_response` function. This function queries the Google Gemini model to produce a relevant and conversational Spanish text response.\n",
      "5.  **Text-to-Speech (TTS):** The generated text response is then passed to `side_proj/src/spanish_chat_bot/audio/tts.py`'s `text_to_speech` function. This function uses the `TTS` library (specifically a Spanish VITS model) to synthesize speech from the text and play it back to the user.\n",
      "6.  **Frontend Update:** The frontend (`App.jsx`) receives the transcribed text and the spoken response, updating the chat history to display the conversation.\n",
      "\n",
      "## Key Features & Capabilities\n",
      "\n",
      "*   **Voice-to-Voice Interaction:** Enables natural, spoken conversations in Spanish.\n",
      "*   **Spanish Language Support:** Utilizes models specifically configured or fine-tuned for the Spanish language.\n",
      "*   **Modular Design:** Components for audio recording, transcription, response generation, and TTS are separated into distinct modules, promoting maintainability and extensibility.\n",
      "*   **Real-time Processing:** Aims for real-time interaction, though performance may vary based on hardware and model size.\n",
      "*   **Basic Frontend:** Provides a simple web interface for user interaction and visual feedback.\n",
      "*   **Model Management:** Uses singletons for critical ML components (Transcriber, ResponseGenerator) to ensure efficient resource utilization (e.g., avoiding reloading models).\n",
      "*   **GPU Acceleration:** Explicitly checks for and utilizes CUDA-enabled GPUs for faster model inference.\n",
      "\n",
      "## Development & Future Goals\n",
      "\n",
      "The project is in a \"Proof of Concept Operational\" state, with the core pipeline working. Future development focuses on enhancing its capabilities and usability:\n",
      "\n",
      "*   **Deployment:** Making the application accessible via cloud hosting.\n",
      "*   **Conversation Memory:** Implementing a mechanism for the bot to retain context across multiple turns for more coherent dialogues.\n",
      "*   **UX/UI Enhancements:** Improving the user interface to be more intuitive and visually appealing.\n",
      "*   **Model Fine-Tuning:**\n",
      "    *   **Whisper:** Addressing the current challenge where fine-tuning led to worse performance than the base model. The goal is to successfully fine-tune Whisper on diverse Spanish dialects and accents to improve transcription accuracy for learners with less natural speech patterns. `spanish_conversation_bot.ipynb` outlines some initial steps towards this.\n",
      "    *   **LLM:** Potentially fine-tuning or prompt-engineering the language model further for more specific conversational styles or tasks.\n",
      "\n",
      "## Dependencies\n",
      "\n",
      "The project has a comprehensive list of dependencies, including core machine learning libraries (`torch`, `transformers`, `TTS`), cloud/AI SDKs (`google-generativeai`), audio processing tools (`sounddevice`, `scipy`), web frameworks (`fastapi`, `uvicorn`), and frontend build tools and libraries (`vite`, `react`, `tailwindcss`, `axios`).\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "Contigo represents a promising initiative in leveraging advanced AI technologies for language education. Its current operational status as a voice-to-voice conversational bot is a strong foundation. The planned future enhancements, particularly in model fine-tuning for diverse Spanish accents and improved conversational memory, are crucial for realizing its full potential as an accessible and effective language learning tool.\n"
     ]
    }
   ],
   "source": [
    "await run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1f8f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea02ca94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
